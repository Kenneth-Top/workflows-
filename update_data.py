import requests
import re
import json
import pandas as pd
from datetime import datetime
import os
import time

# === é…ç½® ===
DATA_FILE = "history_database.csv"  # æˆ‘ä»¬çš„â€œæ•°æ®åº“â€æ–‡ä»¶
MODELS = [
    "moonshotai/kimi-k2-thinking",
    "moonshotai/kimi-k2.5",
    "deepseek/deepseek-v3.2",
    "minimax/minimax-m2.1",
    "x-ai/grok-4.1-fast",
    "openai/gpt-5.1",
    # åœ¨è¿™é‡Œæ·»åŠ æ›´å¤šæ¨¡å‹...
]

def fetch_data(model_id):
    """æŠ“å–å•ä¸ªæ¨¡å‹æ•°æ® (åŒ…å«ä»£ç†ä¿®å¤)"""
    url = f"https://openrouter.ai/{model_id}"
    print(f"ğŸš€ æ­£åœ¨æŠ“å–: {model_id} ...")
    
    try:
        session = requests.Session()
        session.trust_env = False  # ç»•è¿‡ç³»ç»Ÿä»£ç†ï¼Œé˜²æ­¢æŠ¥é”™
        headers = {"User-Agent": "Mozilla/5.0"}
        
        response = session.get(url, headers=headers, timeout=20)
        if response.status_code != 200: return None
        
        match = re.search(r'\\?"analytics\\?":\s*(\[\{.*?\}\])', response.text)
        if match:
            raw = match.group(1).replace('\\"', '"').replace('\\\\', '\\')
            return json.loads(raw)
    except Exception as e:
        print(f"âŒ é”™è¯¯ {model_id}: {e}")
    return None

def update_database():
    # 1. è¯»å–ç°æœ‰æ•°æ®åº“ (å¦‚æœå­˜åœ¨)
    if os.path.exists(DATA_FILE):
        print("ğŸ“‚ è¯»å–ç°æœ‰å†å²æ•°æ®...")
        df_old = pd.read_csv(DATA_FILE)
        df_old['Date'] = pd.to_datetime(df_old['Date'])
    else:
        print("ğŸ“‚ åˆå§‹åŒ–æ–°æ•°æ®åº“...")
        df_old = pd.DataFrame(columns=['Date', 'Model', 'Prompt', 'Completion', 'Reasoning', 'Total_Tokens'])

    new_records = []

    # 2. çˆ¬å–æœ€æ–°æ•°æ®
    for model in MODELS:
        data = fetch_data(model)
        if not data: continue
        
        for record in data:
            # æ•°æ®æ¸…æ´—ä¸å•ä½è½¬æ¢ (Billion)
            p = record.get('total_prompt_tokens', 0) / 1e9
            c = record.get('total_completion_tokens', 0) / 1e9
            r = record.get('total_native_tokens_reasoning', 0) / 1e9
            t = p + c
            
            new_records.append({
                'Date': datetime.strptime(record['date'], "%Y-%m-%d %H:%M:%S"),
                'Model': model,
                'Prompt': round(p, 6),
                'Completion': round(c, 6),
                'Reasoning': round(r, 6),
                'Total_Tokens': round(t, 6)
            })
        time.sleep(1)

    if not new_records:
        print("âš ï¸ æœ¬æ¬¡æœªæŠ“å–åˆ°ä»»ä½•æ•°æ®")
        return

    df_new = pd.DataFrame(new_records)
    
    # 3. æ ¸å¿ƒé€»è¾‘ï¼šå¢é‡åˆå¹¶ (Upsert)
    # æˆ‘ä»¬å°†æ—§æ•°æ®å’Œæ–°æ•°æ®åˆå¹¶
    df_combined = pd.concat([df_old, df_new])
    
    # å…³é”®ï¼šå¦‚æœæœ‰é‡å¤çš„ (Date, Model)ï¼Œä¿ç•™æœ€æ–°çš„é‚£ä¸€æ¡ï¼ˆdf_new çš„ï¼‰
    # è¿™æ ·æ—¢èƒ½ä¿ç•™å†å²ï¼Œåˆèƒ½æ›´æ–°â€œæ˜¨å¤©â€ä¸å®Œæ•´çš„æ•°æ®
    df_combined = df_combined.sort_values('Date').drop_duplicates(subset=['Date', 'Model'], keep='last')
    
    # 4. ä¿å­˜å› CSV
    df_combined.to_csv(DATA_FILE, index=False)
    print(f"âœ… æ•°æ®åº“æ›´æ–°å®Œæˆï¼å½“å‰æ€»è®°å½•æ•°: {len(df_combined)}")

if __name__ == "__main__":
    # æ¸…ç†ç¯å¢ƒå˜é‡é˜²æ­¢ä»£ç†å¹²æ‰°
    if "HTTP_PROXY" in os.environ: del os.environ["HTTP_PROXY"]
    update_database()