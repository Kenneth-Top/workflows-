import requests
import re
import json
import pandas as pd
from datetime import datetime
import os
import time

# === é…ç½® ===
DATA_FILE = "history_database.csv"
MODELS_API = "https://openrouter.ai/api/v1/models"

# å…¬å…± Sessionï¼ˆå¤ç”¨è¿æ¥æ± ï¼Œé¿å…æ¯æ¬¡è¯·æ±‚éƒ½åˆ›å»ºæ–° Sessionï¼‰
SESSION = requests.Session()
SESSION.trust_env = False
SESSION.headers.update({
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
})


def fetch_all_model_ids():
    """ä» OpenRouter API è‡ªåŠ¨è·å–æ‰€æœ‰å¯ç”¨æ¨¡å‹çš„ id åˆ—è¡¨"""
    print("ğŸ” æ­£åœ¨ä» OpenRouter API è·å–æ¨¡å‹åˆ—è¡¨...")
    try:
        resp = SESSION.get(MODELS_API, timeout=30)
        resp.raise_for_status()
        models = resp.json().get("data", [])
        # æŒ‰ created å€’åºæ’åˆ—ï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
        models.sort(key=lambda m: m.get("created", 0), reverse=True)
        ids = [m["id"] for m in models]
        print(f"âœ… å‘ç° {len(ids)} ä¸ªæ¨¡å‹")
        return ids
    except Exception as e:
        print(f"âŒ è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥: {e}")
        return []


def fetch_analytics(model_id):
    """ä»æ¨¡å‹é¡µé¢ HTML ä¸­æå– analytics æ•°æ®"""
    url = f"https://openrouter.ai/{model_id}"
    try:
        resp = SESSION.get(url, timeout=20)
        if resp.status_code != 200:
            return None

        match = re.search(r'\\?"analytics\\?":\s*(\[\{.*?\}\])', resp.text)
        if match:
            raw = match.group(1).replace('\\"', '"').replace('\\\\', '\\')
            return json.loads(raw)
    except Exception as e:
        print(f"  âŒ é”™è¯¯ {model_id}: {e}")
    return None


def update_database():
    # 1. è¯»å–æ—§æ•°æ®
    columns = ['Date', 'Model', 'Prompt', 'Completion', 'Reasoning', 'Total_Tokens']
    if os.path.exists(DATA_FILE):
        try:
            df_old = pd.read_csv(DATA_FILE)
            df_old['Date'] = pd.to_datetime(df_old['Date'])
        except Exception:
            df_old = pd.DataFrame(columns=columns)
    else:
        df_old = pd.DataFrame(columns=columns)

    # 2. è‡ªåŠ¨è·å–æ¨¡å‹åˆ—è¡¨
    all_models = fetch_all_model_ids()
    if not all_models:
        print("âš ï¸ æ— æ³•è·å–æ¨¡å‹åˆ—è¡¨ï¼Œç»ˆæ­¢")
        return

    # è¯†åˆ«æ–°æ¨¡å‹ï¼ˆCSV ä¸­å°šæœªå‡ºç°çš„æ¨¡å‹ï¼‰
    existing_models = set(df_old['Model'].unique()) if not df_old.empty else set()
    new_models = [m for m in all_models if m not in existing_models]
    if new_models:
        print(f"ğŸ†• å‘ç° {len(new_models)} ä¸ªæ–°æ¨¡å‹:")
        for m in new_models[:10]:  # æœ€å¤šæ˜¾ç¤º10ä¸ª
            print(f"   + {m}")
        if len(new_models) > 10:
            print(f"   ... åŠå¦å¤– {len(new_models) - 10} ä¸ª")

    # 3. æ‰¹é‡çˆ¬å– analytics æ•°æ®
    new_records = []
    today_str = datetime.utcnow().strftime("%Y-%m-%d")  # OpenRouter ä½¿ç”¨ UTC æ—¥æœŸ

    for i, model in enumerate(all_models):
        print(f"ğŸš€ [{i+1}/{len(all_models)}] æ­£åœ¨æŠ“å–: {model}")
        data = fetch_analytics(model)
        if not data:
            continue

        for record in data:
            # è¿‡æ»¤å½“å¤©æœªç»“ç®—æ•°æ®ï¼ˆå½“å¤©ç»Ÿè®¡ä¸å®Œæ•´ï¼Œä¼šå¯¼è‡´æ•°å€¼åä½ï¼‰
            record_date_str = record['date'][:10]  # "2026-02-13 00:00:00" -> "2026-02-13"
            if record_date_str == today_str:
                continue

            p = (record.get('total_prompt_tokens') or 0) / 1e9
            c = (record.get('total_completion_tokens') or 0) / 1e9
            r = (record.get('total_native_tokens_reasoning') or 0) / 1e9
            t = p + c  # Total = Prompt + Completion (OpenAI æ ‡å‡†)

            new_records.append({
                'Date': datetime.strptime(record['date'], "%Y-%m-%d %H:%M:%S"),
                'Model': model,
                'Prompt': round(p, 6),
                'Completion': round(c, 6),
                'Reasoning': round(r, 6),
                'Total_Tokens': round(t, 6)
            })
        time.sleep(1)

    if not new_records:
        print("âš ï¸ æœ¬æ¬¡æœªæŠ“å–åˆ°ä»»ä½•æ•°æ®")
        return

    df_new = pd.DataFrame(new_records)

    # 4. å¢é‡åˆå¹¶ (Upsert)ï¼šDate + Model ä¸ºå”¯ä¸€é”®
    df_combined = pd.concat([df_old, df_new])
    df_combined = df_combined.sort_values('Date').drop_duplicates(
        subset=['Date', 'Model'], keep='last'
    )

    # 5. ä¿å­˜
    df_combined.to_csv(DATA_FILE, index=False)
    print(f"âœ… æ•°æ®åº“æ›´æ–°å®Œæˆï¼å½“å‰æ€»è®°å½•æ•°: {len(df_combined)}")


if __name__ == "__main__":
    if "HTTP_PROXY" in os.environ:
        del os.environ["HTTP_PROXY"]
    update_database()
