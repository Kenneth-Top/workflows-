import requests
import re
import json
import pandas as pd
from datetime import datetime
import os
import time

# === é…ç½® ===
DATA_FILE = "history_database.csv"
# ================= æ‰¹é‡é…ç½®åŒºåŸŸ =================
MODELS = [
    "deepseek/deepseek-r1",
    "deepseek/deepseek-v3.2",
    "moonshotai/kimi-k2-thinking",
    "moonshotai/kimi-k2.5",
    "z-ai/glm-4.7",
    "z-ai/glm-4.7-flash",
    "z-ai/glm-4.6v",
    "minimax/minimax-m2.1",
    "minimax/minimax-m2-her",
    "qwen/qwen3-coder-next",
    "qwen/qwen3-embedding-8b",
    "qwen/qwen3-embedding-4b",
    "anthropic/claude-opus-4.6",
    "anthropic/claude-opus-4.5",
    "google/gemini-3-pro-preview",
    "google/gemini-3-flash-preview",
    "google/gemini-3-pro-image-preview"
    "openai/gpt-5.2-codex",
    "openai/gpt-5.2",
    "x-ai/grok-4.1-fast"
]
# ===========================================

def fetch_data(model_id):
    url = f"https://openrouter.ai/{model_id}"
    print(f"ğŸš€ æ­£åœ¨æŠ“å–: {model_id} ...")
    
    try:
        session = requests.Session()
        session.trust_env = False
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        }
        
        response = session.get(url, headers=headers, timeout=20)
        if response.status_code != 200: return None
        
        # æå– JSON
        match = re.search(r'\\?"analytics\\?":\s*(\[\{.*?\}\])', response.text)
        if match:
            raw = match.group(1).replace('\\"', '"').replace('\\\\', '\\')
            return json.loads(raw)
    except Exception as e:
        print(f"âŒ é”™è¯¯ {model_id}: {e}")
    return None

def update_database():
    # 1. è¯»å–æ—§æ•°æ®
    if os.path.exists(DATA_FILE):
        try:
            df_old = pd.read_csv(DATA_FILE)
            df_old['Date'] = pd.to_datetime(df_old['Date'])
        except:
            df_old = pd.DataFrame(columns=['Date', 'Model', 'Prompt', 'Completion', 'Reasoning', 'Total_Tokens'])
    else:
        df_old = pd.DataFrame(columns=['Date', 'Model', 'Prompt', 'Completion', 'Reasoning', 'Total_Tokens'])

    new_records = []

    # 2. çˆ¬å–æ–°æ•°æ®
    for model in MODELS:
        data = fetch_data(model)
        if not data: continue
        
        for record in data:
            # === æ•°æ®æ¸…æ´—ä¸å•ä½è½¬æ¢ (Billion) ===
            # ä½¿ç”¨ or 0 é˜²æ­¢ None å€¼æŠ¥é”™
            p = (record.get('total_prompt_tokens') or 0) / 1e9
            c = (record.get('total_completion_tokens') or 0) / 1e9
            r = (record.get('total_native_tokens_reasoning') or 0) / 1e9
            
            # Total = Prompt + Completion (OpenAI æ ‡å‡†)
            t = p + c
            
            new_records.append({
                'Date': datetime.strptime(record['date'], "%Y-%m-%d %H:%M:%S"),
                'Model': model,
                'Prompt': round(p, 6),
                'Completion': round(c, 6),
                'Reasoning': round(r, 6),
                'Total_Tokens': round(t, 6)
            })
        time.sleep(1)

    if not new_records:
        print("âš ï¸ æœ¬æ¬¡æœªæŠ“å–åˆ°ä»»ä½•æ•°æ®")
        return

    df_new = pd.DataFrame(new_records)
    
    # 3. å¢é‡åˆå¹¶ (Upsert)
    # åˆå¹¶æ–°æ—§æ•°æ®
    df_combined = pd.concat([df_old, df_new])
    
    # å»é‡é€»è¾‘ï¼šDate + Model æ˜¯å”¯ä¸€é”®
    # keep='last' ç¡®ä¿ä¿ç•™æœ€æ–°æŠ“å–çš„æ•°æ®ï¼ˆå¦‚æœ OpenRouter æ›´æ–°äº†å½“å¤©çš„ç»Ÿè®¡ï¼‰
    df_combined = df_combined.sort_values('Date').drop_duplicates(subset=['Date', 'Model'], keep='last')
    
    # 4. ä¿å­˜
    df_combined.to_csv(DATA_FILE, index=False)
    print(f"âœ… æ•°æ®åº“æ›´æ–°å®Œæˆï¼å½“å‰æ€»è®°å½•æ•°: {len(df_combined)}")

if __name__ == "__main__":
    if "HTTP_PROXY" in os.environ: del os.environ["HTTP_PROXY"]
    update_database()

