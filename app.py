import streamlit as st
import pandas as pd
import altair as alt
import os
import io
import json

# === 1. åŸºç¡€é…ç½® ===
st.set_page_config(page_title="LLM æ•°æ®çœ‹æ¿", layout="wide")
DATA_FILE = "history_database.csv"
PRICING_FILE = "openrouter_pricing_provider_records.csv"
BENCHMARK_FILE = "openrouter_benchmark_records.csv"
LMARENA_FILE = "lmarena_leaderboard_records.csv"

# AI ç§˜é’¥é…ç½® (ä¼˜å…ˆä» Streamlit Secrets å®‰å…¨è¯»å–ï¼Œæ— é…ç½®æ–‡ä»¶åˆ™é€€å›ç¯å¢ƒå˜é‡)
def get_api_key(key_name):
    try:
        if key_name in st.secrets:
            return st.secrets[key_name]
    except Exception:
        pass
    return os.environ.get(key_name, "")

OPENROUTER_KEY = get_api_key("OPENROUTER_API_KEY")
GOOGLE_KEY = get_api_key("GOOGLE_API_KEY")
MODELSCOPE_KEY = get_api_key("MODELSCOPE_API_KEY")

# å®šä¹‰ AI æä¾›å•†é…ç½®
AI_PROVIDERS = {
    "OpenRouter": {
        "base_url": "https://openrouter.ai/api/v1",
        "key": OPENROUTER_KEY,
        "models": {
            "GLM-4.5-Air (å…è´¹)": "z-ai/glm-4.5-air:free",
            "Gemini 3 Flash (OpenRouterç‰ˆ)": "google/gemini-3-flash-preview",
            "Claude Haiku 4.5": "anthropic/claude-haiku-4.5",
        }
    },
    "Google AI Studio": {
        "base_url": "https://generativelanguage.googleapis.com/v1beta/openai",
        "key": GOOGLE_KEY,
        "models": {
            "Gemini 2.5 Flash": "gemini-2.5-flash",
            "Gemini 2.5 Flash Lite":"gemini-2.5-flash-lite",
            "Gemini 2.5 Pro": "gemini-2.5-pro",
            "Gemini 3.0 flash preview": "gemini-3-flash-preview",
            
        }
    },
    "é­”å¡”ç¤¾åŒº (ModelScope)": {
        "base_url": "https://api-inference.modelscope.cn/v1",
        "key": MODELSCOPE_KEY,
        "models": {
            "GLM-5": "ZhipuAI/GLM-5",
            "Minimax-M2.5": "MiniMax/MiniMax-M2.5",
            "Kimi-K2.5": "moonshotai/Kimi-K2.5",
            "Qwen-3.5": "Qwen/Qwen3.5-397B-A17B",
        }
    }
}

# é¡µé¢æ ‡é¢˜
st.title("LLM æ•°æ®çœ‹æ¿")

# å®šä¹‰é¡µé¢åç§°å¸¸é‡
NAV_AI_QUERY = "AI æŸ¥è¯¢"
NAV_DAILY_BRIEF = "æ¯æ—¥ç®€æŠ¥"
NAV_TN_DAILY = "T+N æ—¥ç”¨é‡å¯¹æ¯”"
NAV_CUMULATIVE_COMPARE = "ç´¯è®¡ç”¨é‡å¯¹æ¯”"
NAV_DETAIL_DAILY = "å•æ¨¡å‹ç”¨é‡"
NAV_RAW_DATA = "æ•°æ®å¯¼å‡º"
NAV_PRICING = "ä¾›åº”å•†å®šä»·"
NAV_BENCHMARK = "åŸºå‡†æµ‹è¯•"
NAV_SINGLE_MODEL = "å•æ¨¡å‹æ·±åº¦åˆ†æ"

# === 2. å·¥å…·å‡½æ•° ===

def is_reasoning_model(model_name: str) -> bool:
    """åŸºäºæ¨¡å‹å‘½åè§„åˆ™è¿›è¡Œç²—ç•¥åˆ¤æ–­æ˜¯å¦ä¸ºæ·±åº¦æ¨ç†æ¨¡å‹"""
    if not isinstance(model_name, str): return False
    name_lower = model_name.lower()
    reasoning_keywords = ['reasoning', 'o1', 'o3', 'r1', 'qwq']
    for kw in reasoning_keywords:
        if kw in name_lower:
            return True
    return False

import re as _re_global

def _tokenize_model_name(name: str) -> set:
    """å°†æ¨¡å‹åæ‹†ä¸º token é›†åˆï¼Œç”¨äºæ¨¡ç³ŠåŒ¹é…"""
    n = name.lower()
    # å»æ‰å‚å•†å‰ç¼€
    if '/' in n:
        n = n.split('/')[-1]
    # å»æ‰æ‹¬å·å†…ä¿®é¥°è¯ï¼Œå¦‚ (Reasoning), (Oct '24), (Non-reasoning)
    n = _re_global.sub(r'\s*\(.*?\)', '', n)
    # æŒ‰ ç©ºæ ¼ã€æ¨ªçº¿ã€ä¸‹åˆ’çº¿ åˆ†å‰²
    tokens = set(_re_global.split(r'[\s\-_]+', n.strip()))
    tokens.discard('')
    return tokens

def _jaccard_similarity(set_a: set, set_b: set) -> float:
    """è®¡ç®—ä¸¤ä¸ªé›†åˆçš„ Jaccard ç›¸ä¼¼åº¦"""
    if not set_a or not set_b:
        return 0.0
    intersection = set_a & set_b
    union = set_a | set_b
    return len(intersection) / len(union)

def normalize_model_name(name: str) -> str:
    """ç»Ÿä¸€æ¶ˆé™¤å‚å•†å‰ç¼€å’Œæ— ç”¨çš„å¤§å°å†™ï¼Œä½¿ä¸åŒæ•°æ®æºä¸­çš„åŒæ¬¾æ¨¡å‹èƒ½åˆå¹¶"""
    if not isinstance(name, str): return str(name)
    n = name.lower()
    # ç§»é™¤è¯¸å¦‚ 'anthropic/', 'google/' ç­‰å‰ç¼€
    if '/' in n:
        n = n.split('/')[-1]
    
    # ä»…ä¿ç•™æç«¯ç‰¹ä¾‹çš„ç¡¬æ˜ å°„ï¼ˆå®Œå…¨ä¸åŒå‘½åçš„æƒ…å†µï¼‰
    mapping = {
        'deepseek v3': 'deepseek-chat',
        'deepseek-v3': 'deepseek-chat',
    }
    
    for key, val in mapping.items():
        if key in n:
            return val
            
    # å»é™¤å¤šä½™æ‹¬å·å¦‚ (Reasoning) ç­‰å¹²æ‰°è¯ï¼Œä¿ç•™æ ¸å¿ƒ slug
    n = _re_global.sub(r'\s*\(.*?\)', '', n).strip()
    n = n.replace(' ', '-')
    return n

def fuzzy_match_model(target_norm: str, candidate_names: list, threshold: float = 0.55) -> list:
    """åœ¨å€™é€‰æ¨¡å‹ååˆ—è¡¨ä¸­ï¼Œç”¨ Token åŒ– Jaccard åŒ¹é…æ‰¾å‡ºä¸ target_norm ç›¸ä¼¼çš„åå­—"""
    target_tokens = _tokenize_model_name(target_norm)
    matched_with_scores = []
    for cand in candidate_names:
        cand_tokens = _tokenize_model_name(cand)
        sim = _jaccard_similarity(target_tokens, cand_tokens)
        if sim >= threshold:
            matched_with_scores.append((cand, sim))
    
    # æŒ‰å¾—åˆ†ä»é«˜åˆ°ä½æ’åºï¼Œç¡®ä¿ matched[0] æ˜¯æœ€åƒçš„
    matched_with_scores.sort(key=lambda x: x[1], reverse=True)
    return [m[0] for m in matched_with_scores]

@st.cache_data(ttl=600)
def load_data():
    if not os.path.exists(DATA_FILE):
        return None, f"âŒ æ‰¾ä¸åˆ°æ–‡ä»¶ `{DATA_FILE}`ï¼Œè¯·ç­‰å¾…çˆ¬è™«è¿è¡Œã€‚"
    try:
        # Load Token Data
        df = pd.read_csv(DATA_FILE)
        if df.empty: return None, "CSV æ–‡ä»¶ä¸ºç©º"
        df['Date'] = pd.to_datetime(df['Date'])
        
        # åç§°æ¸…æ´—ï¼šå»æ‰ '/' å‰é¢çš„å‚å•†å
        df['Display_Name'] = df['Model'].apply(lambda x: x.split('/')[-1] if '/' in x else x)
        
        return df, None
    except Exception as e:
        return None, str(e)

@st.cache_data(ttl=600)
def load_pricing_data():
    if not os.path.exists(PRICING_FILE):
        return None
    try:
        df_price = pd.read_csv(PRICING_FILE)
        df_price['Date'] = pd.to_datetime(df_price['Date'])
        return df_price
    except Exception:
        return None

@st.cache_data(ttl=600)
def load_benchmark_data():
    if not os.path.exists(BENCHMARK_FILE):
        return None
    try:
        df_bench = pd.read_csv(BENCHMARK_FILE)
        df_bench['Date'] = pd.to_datetime(df_bench['Date'])
        return df_bench
    except Exception:
        return None

@st.cache_data(ttl=600)
def load_lmarena_data():
    if not os.path.exists(LMARENA_FILE): return None
    try:
        df = pd.read_csv(LMARENA_FILE)
        df['Date'] = pd.to_datetime(df['Date'])
        return df
    except Exception:
        return None

# Excel/CSV æ™ºèƒ½å¯¼å‡ºå‡½æ•°
def get_dataset_download(df, filename_prefix):
    try:
        output = io.BytesIO()
        with pd.ExcelWriter(output, engine='openpyxl') as writer:
            df.to_excel(writer, index=False, sheet_name='Sheet1')
        data = output.getvalue()
        file_name = f"{filename_prefix}.xlsx"
        mime = "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
        label = "ğŸ“¥ ä¸‹è½½ Excel æ–‡ä»¶ (.xlsx)"
    except ImportError:
        data = df.to_csv(index=False).encode('utf-8-sig')
        file_name = f"{filename_prefix}.csv"
        mime = "text/csv"
        label = "ğŸ“¥ ä¸‹è½½ CSV æ–‡ä»¶ (Excelå…¼å®¹)"
    
    return data, file_name, mime, label

df, error = load_data()
df_price = load_pricing_data()
df_bench = load_benchmark_data()
df_lmarena = load_lmarena_data()

if error and not (df_price is not None or df_bench is not None):
    st.error(error)
    st.stop()

# === 3. ä¾§è¾¹æ å¯¼èˆª ===
st.sidebar.title("å¯¼èˆª")
page = st.sidebar.radio("é€‰æ‹©åˆ†æè§†å›¾", [
    NAV_AI_QUERY,
    NAV_DAILY_BRIEF,
    NAV_SINGLE_MODEL,
    NAV_TN_DAILY,
    NAV_CUMULATIVE_COMPARE,
    NAV_DETAIL_DAILY,
    NAV_PRICING,
    NAV_BENCHMARK,
    NAV_RAW_DATA
])

all_model_names = df['Display_Name'].unique() if df is not None else []
all_pricing_models = df_price['Model'].unique() if df_price is not None else []
all_benchmark_models = [c for c in df_bench.columns if c not in ['Date', 'Metric']] if df_bench is not None else []

# æ•°æ®æ¦‚è§ˆé¢æ¿
st.sidebar.divider()
st.sidebar.markdown("#### æ•°æ®æ¦‚è§ˆ")
if df is not None:
    st.sidebar.metric("è¿½è¸ªæ¨¡å‹æ•°", len(all_model_names))
    st.sidebar.caption(f"æ•°æ®åŒºé—´: {df['Date'].min().strftime('%Y-%m-%d')} ~ {df['Date'].max().strftime('%Y-%m-%d')}")
if df_price is not None:
    st.sidebar.metric("å®šä»·æ¨¡å‹æ•°", len(all_pricing_models))
    st.sidebar.caption(f"å®šä»·æ›´æ–°è‡³: {df_price['Date'].max().strftime('%Y-%m-%d')}")
if df_bench is not None:
    st.sidebar.metric("è·‘åˆ†æ¨¡å‹æ•°", len(all_benchmark_models))
    st.sidebar.caption(f"è·‘åˆ†æ›´æ–°è‡³: {df_bench['Date'].max().strftime('%Y-%m-%d')}")
if df_lmarena is not None:
    st.sidebar.metric("LMARENA æ¨¡å‹æ•°", df_lmarena['Model'].nunique())
    st.sidebar.caption(f"LMARENA æ›´æ–°è‡³: {df_lmarena['Date'].max().strftime('%Y-%m-%d')}")

# ========================================================
# é¡µé¢ 0: AI æ™ºèƒ½æŸ¥è¯¢
# ========================================================
if page == NAV_AI_QUERY:
    st.subheader("AI æ•°æ®åˆ†æåŠ©æ‰‹")
    
    # é¡¶éƒ¨æ§åˆ¶åŒºï¼šæä¾›å•†ä¸æ¨¡å‹é€‰æ‹©
    col_p1, col_p2, col_p3 = st.columns([1, 1, 1])
    with col_p1:
        provider_name = st.selectbox("é€‰æ‹© AI æœåŠ¡å•†:", list(AI_PROVIDERS.keys()), index=2) # é»˜è®¤é€‰ é­”å¡”ç¤¾åŒº
        provider_cfg = AI_PROVIDERS[provider_name]
    with col_p2:
        selected_model_label = st.selectbox("é€‰æ‹©æ¨¡å‹:", list(provider_cfg["models"].keys()), index=0)
        AI_MODEL = provider_cfg["models"][selected_model_label]
    with col_p3:
        st.write("") # å ä½
        st.write("")
        enable_web_search = st.toggle("ğŸŒ å¯ç”¨è”ç½‘æœç´¢", value=False)

    api_key = provider_cfg["key"]
    if not api_key:
        api_key = st.text_input(f"è¯·è¾“å…¥ {provider_name} API Key:", type="password")
    else:
        # æ„å»ºæ•°æ®åº“ä¸Šä¸‹æ–‡æ‘˜è¦
        @st.cache_data(ttl=600)
        def build_db_context(_df, _df_price, _df_bench, _df_lmarena):
            context_parts = []
            
            if _df is not None and not _df.empty:
                display_names = sorted(_df['Display_Name'].dropna().unique().tolist())
                context_parts.append(f"""### Token æ¶ˆè€—æ•°æ® (å˜é‡å: df)
- åˆ—: Date, Model, Prompt, Completion, Reasoning, Total_Tokens, Display_Name
- è®°å½•æ•°: {len(_df)}, æ—¥æœŸèŒƒå›´: {_df['Date'].min().strftime('%Y-%m-%d')} ~ {_df['Date'].max().strftime('%Y-%m-%d')}
- Token å•ä½: Billion (10äº¿)
- **å…¨éƒ¨å¯ç”¨æ¨¡å‹åˆ—è¡¨ (Display_Nameåˆ—)**: {', '.join(display_names)}""")

            if _df_price is not None and not _df_price.empty:
                price_models = sorted(_df_price['Model'].dropna().unique().tolist())
                context_parts.append(f"""### å®šä»·æ•°æ® (å˜é‡å: df_price)
- åˆ—: Date, Model, Provider, Input_Price_1M, Output_Price_1M, Cache_Hit_Rate
- ä»·æ ¼å•ä½: $/1M Tokens
- **å…¨éƒ¨å®šä»·æ¨¡å‹åˆ—è¡¨**: {', '.join(price_models)}""")

            if _df_bench is not None and not _df_bench.empty:
                context_parts.append(f"""### Benchmark è·‘åˆ† (å˜é‡å: df_bench)
- ç»“æ„: å®½è¡¨ï¼Œæ¯è¡Œæ˜¯ä¸€ä¸ª Metricï¼Œæ¯åˆ—æ˜¯ä¸€ä¸ªæ¨¡å‹å
- Metric: {', '.join(_df_bench['Metric'].unique()[:8])}
- æ¨¡å‹æ•°: {len([c for c in _df_bench.columns if c not in ['Date','Metric']])}""")

            if _df_lmarena is not None and not _df_lmarena.empty:
                score_cols = [c for c in _df_lmarena.columns if c.startswith('Score_')]
                context_parts.append(f"""### Arena ç«æŠ€æ’è¡Œ (å˜é‡å: df_lmarena)
- æ•°æ®æº: arena.ai (åŸ LMARENA)
- 8 ä¸ª ELO æ’è¡Œæ¦œ: {', '.join(c.replace('Score_','') for c in score_cols)}
- æ¨¡å‹ç¤ºä¾‹: {', '.join(_df_lmarena['Model'].unique().tolist()[:30])}""")
            
            return '\n\n'.join(context_parts)
        
        db_context = build_db_context(df, df_price, df_bench, df_lmarena)
        
        # åŠ¨æ€æ³¨å…¥è”ç½‘å¼ºåŠ›è§„åˆ™
        web_search_rules = ""
        if enable_web_search:
            web_search_rules = """
## ğŸŒ è”ç½‘æœç´¢è§„èŒƒï¼ˆæœ€é«˜è­¦æˆ’ï¼‰
ä½ å½“å‰å·²å¯ç”¨è”ç½‘åŠŸèƒ½ã€‚ä½ **ä»…èƒ½ä½¿ç”¨ç½‘ç»œä¿¡æ¯æ¥è§£é‡Šæ•°æ®è¶‹åŠ¿èƒŒåçš„â€œå¤–éƒ¨åŸå› â€**ï¼ˆå¦‚ï¼šæŸ¥é˜…æŸå¤©æ¨¡å‹ç”¨é‡æš´å¢æ˜¯å¦å› ä¸ºé™ä»·/æŸå¹³å°å…è´¹ã€å‘æ–°ç‰ˆæˆ–çªå‘æ–°é—»ï¼‰ã€‚
**ä¸¥ç¦**ç”¨ç½‘ç»œä¸Šçš„å…¬å¼€æ•°æ®æ¥ä¿®æ”¹ã€æ›¿ä»£æˆ–ä¼ªé€ æœ¬åœ°æ•°æ®åº“ï¼ˆdf, df_priceç­‰ï¼‰ä¸­çš„æ•°å€¼ã€‚ä»£ç ç»˜åˆ¶çš„å›¾è¡¨å’Œè¾“å‡ºçš„å…·ä½“ Token æ•°æ®ï¼Œå¿…é¡» **100% ä¸¥æ ¼æ¥æºäºæœ¬åœ°æ•°æ®åº“**ï¼
"""
        
        SYSTEM_PROMPT = f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ LLM è¡Œä¸šæŠ•ç ”åˆ†æå¸ˆï¼ŒæœåŠ¡äºæœºæ„æŠ•èµ„è€…ã€‚ä½ çš„æ ¸å¿ƒä»»åŠ¡æ˜¯ç”¨**æ•°æ®é©±åŠ¨çš„å¯è§†åŒ–å›¾è¡¨**å›ç­”é—®é¢˜ã€‚ç»å¯¹ç¦æ­¢åœ¨å›å¤ä¸­è¾“å‡ºä»»ä½• <tool_call>, <function> ç­‰ XML æˆ–å†…éƒ¨å·¥å…·è°ƒç”¨æ ‡ç­¾ã€‚å¦‚æœç¼ºå°‘ä¿¡æ¯ï¼Œè¯·ç›´æ¥å›ç­”â€œæˆ‘ä¸çŸ¥é“â€æˆ–åŸºäºç°æœ‰æ•°æ®è¿›è¡Œæ¨æµ‹ã€‚

## æ•°æ®åº“
{db_context}

{web_search_rules}

## æ•°æ®è¿‡æ»¤æŒ‡å—ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰
1. **æœç»ç›²ç›®æ¨¡ç³ŠåŒ¹é…**ï¼šå½“ç”¨æˆ·è¯¢é—®ç‰¹å®šæ¨¡å‹ï¼ˆå¦‚ "M2.5", "minimax", "gemini"ï¼‰æ—¶ï¼Œ**å¿…é¡»**ä»”ç»†æŸ¥é˜…æˆ‘æä¾›çš„â€œå…¨éƒ¨å¯ç”¨æ¨¡å‹åˆ—è¡¨â€ï¼Œæ‰¾å‡ºç¡®åˆ‡çš„ç›®æ ‡åç§°ã€‚
2. **ä½¿ç”¨ç²¾å‡†è¿‡æ»¤**ï¼šä¸¥ç¦ä½¿ç”¨å®½æ³›çš„ `str.contains('2.5')`ï¼ä½ å¿…é¡»ä½¿ç”¨ `isin` æˆ– `==` è¿›è¡Œç²¾å‡†åŒ¹é…ï¼Œä¾‹å¦‚ï¼š`df[df['Display_Name'].isin(['minimax-m2.5'])]`ã€‚
3. **åŒºåˆ†åŒå‚å•†æ–°è€æ¨¡å‹**ï¼šå¦‚æœç”¨æˆ·æŸ¥ç‰¹å®šæ–°æ¨¡å‹ï¼Œç»ä¸è¦æŠŠè¯¥å‚å•†å‡ ä¸ªæœˆå‰çš„è€æ¨¡å‹æ•°æ®ä¸€å¹¶ç®—å…¥ï¼Œä»¥å…æ±¡æŸ“æ—¶é—´è½´ã€‚

## è¾“å‡ºæ ¼å¼ï¼ˆä¸¥æ ¼éµå®ˆï¼‰
1. **æ–‡å­—éƒ¨åˆ†**ï¼šç”¨ 3-5 å¥è¯ç»™å‡ºæ ¸å¿ƒç»“è®ºï¼ŒåƒæŠ•ç ”æŠ¥å‘Šæ‘˜è¦ä¸€æ ·ç®€æ´ã€‚æä¾›å…³é”®æ•°æ®+æ´å¯Ÿã€‚
2. **ä»£ç éƒ¨åˆ†**ï¼šå¿…é¡»ç”Ÿæˆä¸€ä¸ª ```python``` ä»£ç å—ï¼ŒåŒ…å«**è‡³å°‘ 1 ä¸ªå›¾è¡¨**ã€‚

## å¯è§†åŒ–æŒ‡å—
- ä½¿ç”¨ `st.altair_chart(chart, use_container_width=True)` å±•ç¤ºå›¾è¡¨ã€‚
- ç”¨ `st.dataframe()` å±•ç¤ºè¾…åŠ©æ•°æ®è¡¨æ ¼ï¼ˆå¯é€‰ï¼‰ã€‚
- æ ‡é¢˜ç”¨ä¸­æ–‡ï¼Œå­—å·è®¾ä¸º 16ã€‚
- å¯¹æ‰€æœ‰æ•°å€¼åˆ—æ“ä½œå‰**å…ˆç¡®ä¿ç±»å‹æ­£ç¡®** (å¦‚ `pd.to_numeric()`)ã€‚
- åªå†™ä¸€ä¸ªä»£ç å—ï¼ŒåŒ…å«æ‰€æœ‰å›¾è¡¨å’Œè¡¨æ ¼ä»£ç ã€‚
"""

        # åˆå§‹åŒ–èŠå¤©å†å²
        if "ai_messages" not in st.session_state:
            st.session_state.ai_messages = []
        
        # ç”¨äº exec çš„å‘½åç©ºé—´
        import numpy as np
        exec_namespace = {
            "df": df, "df_price": df_price, "df_bench": df_bench, "df_lmarena": df_lmarena,
            "st": st, "alt": alt, "pd": pd, "np": np, "os": os,
        }
        
        # è¾…åŠ©å‡½æ•°ï¼šä» AI å›å¤ä¸­åˆ†ç¦»æ–‡å­—å’Œä»£ç 
        def split_reply(reply):
            import re as _re
            code_blocks = _re.findall(r'```python\s*\n(.*?)```', reply, _re.DOTALL)
            text_only = _re.sub(r'```python\s*\n.*?```', '', reply, flags=_re.DOTALL).strip()
            return text_only, code_blocks[0] if code_blocks else None
        
        def safe_exec(code, ns):
            for key in ['df', 'df_price', 'df_lmarena']:
                frame = ns.get(key)
                if frame is not None and 'Model' in frame.columns:
                    frame = frame.copy()
                    frame['Model'] = frame['Model'].astype(str)
                    ns[key] = frame
            exec(code, ns)
        
        # æ˜¾ç¤ºå†å²å¯¹è¯
        for msg in st.session_state.ai_messages:
            with st.chat_message(msg["role"]):
                if msg["role"] == "assistant":
                    text_part, code = split_reply(msg["content"])
                    st.markdown(text_part)
                    if code:
                        try:
                            safe_exec(code, exec_namespace)
                        except Exception:
                            pass
                else:
                    st.markdown(msg["content"])
        
        # ç”¨æˆ·è¾“å…¥
        user_query = st.chat_input("è¾“å…¥ä½ çš„é—®é¢˜ï¼Œä¾‹å¦‚: 'åˆ†æ M2.5 æœ¬æœˆçš„ç”¨é‡è¶‹åŠ¿ï¼Œå¹¶æŸ¥æ‰¾å®ƒè¿‘æœŸæš´æ¶¨çš„åŸå› '")
        
        if user_query:
            st.session_state.ai_messages.append({"role": "user", "content": user_query})
            with st.chat_message("user"):
                st.markdown(user_query)
            
            messages = [{"role": "system", "content": SYSTEM_PROMPT}]
            for msg in st.session_state.ai_messages[-12:]:
                messages.append({"role": msg["role"], "content": msg["content"]})
            
            # æ„å»º API è¯·æ±‚ Payload (åŠ¨æ€æ³¨å…¥è”ç½‘æ’ä»¶)
            api_payload = {
                "model": AI_MODEL,
                "messages": messages,
                "max_tokens": 4000,
                "temperature": 0.3
            }
            if enable_web_search:
                if provider_name == "OpenRouter":
                    api_payload["plugins"] = [{"id": "web", "max_results": 4}]
                else:
                    # ä½¿ç”¨çº¯æœ¬åœ°å…è´¹æ–¹æ¡ˆç»™é OpenRouter æ¨¡å‹æ·»åŠ è”ç½‘èƒ½åŠ›
                    # æç¤ºè¯é¢„å¤„ç†ï¼šå»é™¤å¼•å·ã€å¤šä½™ç©ºæ ¼ï¼Œå¹¶åªå–å‰ 60 ä¸ªå­—ç¬¦ï¼Œé˜²æ­¢æœç´¢è¯è¿‡é•¿å¯¼è‡´ DDGS è¿”å›ç©º
                    clean_query = user_query.replace('"', '').replace("'", "").strip()[:60]
                    
                    with st.spinner(f"æ­£åœ¨å…¨ç½‘æœç´¢çº¿ç´¢: '{clean_query}'..."):
                        try:
                            from duckduckgo_search import DDGS
                            with DDGS() as ddgs:
                                search_results = list(ddgs.text(clean_query, max_results=5))
                            
                            if search_results:
                                context_str = "ã€å®æ—¶ç½‘ç»œæœç´¢å‚è€ƒèµ„æ–™ã€‘\n"
                                for r in search_results:
                                    context_str += f"- æ ‡é¢˜: {r.get('title', '')}\n  æ‘˜è¦: {r.get('body', '')}\n"
                                
                                # æ³¨å…¥ä¸Šä¸‹æ–‡å’Œå¼ºåˆ¶è¾“å‡ºæ ¼å¼è¦æ±‚
                                api_payload["messages"][-1]["content"] += f"\n\nè¯·å‚è€ƒä»¥ä¸‹æœ€æ–°çš„ç½‘ç»œæœç´¢ç»“æœæ¥è¾…åŠ©å›ç­”ä¸Šè¿°é—®é¢˜ï¼š\n{context_str}\n\nã€æœ€é«˜ä¼˜å…ˆçº§æŒ‡ä»¤ã€‘ï¼šæ— è®ºä½ å‚è€ƒäº†ä»€ä¹ˆå¤–éƒ¨èµ„æ–™ï¼Œä½ çš„ä¸»è¦ä»»åŠ¡ä»ç„¶æ˜¯æ‰§è¡Œæ•°æ®åˆ†æã€‚å¦‚æœä½ éœ€è¦ç”Ÿæˆå›¾è¡¨ï¼Œè¯·åŠ¡å¿…è¿”å›å®Œå…¨ç‹¬ç«‹ã€æ— ä¾èµ–æŠ¥é”™çš„ Python st/alt æ¸²æŸ“ä»£ç ï¼Œå¹¶ä½¿ç”¨ ```python ... ``` åŒ…è£¹ä»£ç å—ï¼"
                                st.toast(f"âœ… å·²æŠ“å–ä¸ '{clean_query}' ç›¸å…³çš„å®æ—¶æ•°æ®ï¼")
                            else:
                                st.toast(f"âš ï¸ æœç´¢ '{clean_query}' æœªå‘ç°ç›´æ¥ç»“æœï¼Œå°†å‡­æ¨¡å‹çŸ¥è¯†åº“å›ç­”ã€‚")
                        except Exception as e:
                            st.toast(f"âš ï¸ æœ¬åœ°è”ç½‘æœç´¢å—é˜»: {e}ï¼Œå°†æ­£å¸¸å‘é€æ–‡æœ¬ã€‚")
            
            with st.chat_message("assistant"):
                with st.spinner(f"AI ({provider_name}) æ­£åœ¨åˆ†ææ•°æ®..." + (" (æ­£åœ¨å…¨ç½‘æœç´¢çº¿ç´¢ ğŸŒ)" if enable_web_search else "")):
                    try:
                        import requests as _req
                        headers = {
                            "Authorization": f"Bearer {api_key}",
                            "Content-Type": "application/json"
                        }
                        if provider_name == "OpenRouter":
                            headers.update({"HTTP-Referer": "http://localhost", "X-Title": "LLM-Dashboard"})
                        
                        resp = _req.post(
                            f"{provider_cfg['base_url']}/chat/completions",
                            headers=headers,
                            json=api_payload,
                            timeout=75
                        )
                        if resp.status_code != 200:
                            raise Exception(f"API Error {resp.status_code}: {resp.text}")
                        result = resp.json()
                        ai_reply = result['choices'][0]['message']['content']
                    except Exception as e:
                        ai_reply = f"æŸ¥è¯¢å¤±è´¥: {str(e)}"
                    
                    # æ¸²æŸ“æ–‡å­—å’Œä»£ç 
                    text_part, code = split_reply(ai_reply)
                    st.markdown(text_part)
                    if code:
                        try:
                            safe_exec(code, exec_namespace)
                        except Exception as code_e:
                            st.warning("âš ï¸ å›¾è¡¨æ¸²æŸ“å‡ºé”™ï¼Œå¤§æ¨¡å‹ç”Ÿæˆçš„ä»£ç å¯èƒ½å­˜åœ¨è¯­æ³•é”™è¯¯æˆ–å¼•ç”¨äº†ä¸å­˜åœ¨çš„åˆ—åã€‚")
                            with st.expander("æŸ¥çœ‹é”™è¯¯è¯¦æƒ…"):
                                st.code(str(code_e) + "\n\n" + code)
                
                st.session_state.ai_messages.append({
                    "role": "assistant", 
                    "content": ai_reply,
                })
        
        # æ¸…ç©ºå¯¹è¯æŒ‰é’®
        if st.session_state.ai_messages:
            if st.button("æ¸…ç©ºå¯¹è¯å†å²"):
                st.session_state.ai_messages = []
                st.rerun()
                
# ========================================================
# é¡µé¢ 1: T+N æ¨ªå‘å¯¹æ¯” (æ¯æ—¥æ¶ˆè€—)
# ========================================================
elif page == NAV_TN_DAILY:
    st.subheader("æ¨¡å‹å¢é•¿æ›²çº¿å¯¹æ¯” (T+N æ¯æ—¥æ¶ˆè€—)")
    st.info("æ¨ªè½´ï¼šä¸Šçº¿å¤©æ•° | çºµè½´ï¼šå½“æ—¥ Token æ¶ˆè€—é‡")

    selected_names = st.multiselect(
        "é€‰æ‹©è¦å¯¹æ¯”çš„æ¨¡å‹:", 
        all_model_names, 
        default=all_model_names[:1] 
    )
    
    if selected_names:
        tn_data = []
        standard_ticks = [0, 1, 2, 3, 4, 5, 6, 7, 10, 14, 30, 60]
        final_tick_values = set(standard_ticks)
        
        max_days_global = 0

        for name in selected_names:
            m_df = df[df['Display_Name'] == name].sort_values('Date')
            if m_df.empty: continue
            
            start_date = m_df.iloc[0]['Date']
            st.caption(f"ğŸ“… **{name}** æ”¶å½•èµ·å§‹æ—¥: {start_date.strftime('%Y-%m-%d')}")
            
            if len(m_df) > 1: m_df = m_df.iloc[:-1]

            latest_date = m_df.iloc[-1]['Date']
            latest_day_diff = (latest_date - start_date).days
            final_tick_values.add(latest_day_diff)
            
            if latest_day_diff > max_days_global:
                max_days_global = latest_day_diff

            for _, row in m_df.iterrows():
                day_diff = (row['Date'] - start_date).days
                if day_diff in standard_ticks or day_diff == latest_day_diff:
                    tn_data.append({
                        'Model': name,
                        'Days_Since_Start': day_diff,
                        'Total_Tokens': row['Total_Tokens'],
                        'Label': f"T+{day_diff}" if day_diff != latest_day_diff else f"Latest (T+{day_diff})",
                        'Real_Date': row['Date'].strftime('%Y-%m-%d')
                    })
        
        if tn_data:
            df_tn = pd.DataFrame(tn_data)
            
            # ã€é…è‰²ä¼˜åŒ–ã€‘ä½¿ç”¨ tableau10 é«˜å¯¹æ¯”é…è‰²
            chart = alt.Chart(df_tn).mark_line(
                point=alt.OverlayMarkDef(size=100, filled=True, color="white", strokeWidth=2)
            ).encode(
                x=alt.X(
                    'Days_Since_Start', 
                    title='ä¸Šçº¿å¤©æ•° (Days)',
                    axis=alt.Axis(values=list(final_tick_values), labelFontSize=20, titleFontSize=24, grid=True),
                    scale=alt.Scale(domain=[0, max_days_global + 1], clamp=True)
                ),
                y=alt.Y(
                    'Total_Tokens', 
                    title='Total Tokens (Billion)',
                    axis=alt.Axis(labelFontSize=20, titleFontSize=24)
                ),
                # å…³é”®ä¿®æ”¹ï¼šscale=alt.Scale(scheme='tableau10')
                color=alt.Color('Model', 
                                scale=alt.Scale(scheme='tableau10'), 
                                legend=alt.Legend(title="æ¨¡å‹åç§°", orient='bottom')),
                tooltip=['Model', 'Label', 'Total_Tokens', 'Real_Date']
            ).properties(height=500)
            
            st.altair_chart(chart, use_container_width=True)
            
            st.markdown("#### ğŸ“‹ æ•°æ®æ˜ç»†")
            df_pivot = df_tn.pivot_table(index='Model', columns='Days_Since_Start', values='Total_Tokens')
            df_pivot.columns = [f"T+{c}" for c in df_pivot.columns]
            st.dataframe(df_pivot.style.format("{:.4f} B"), use_container_width=True)
            
            data, name, mime, label = get_dataset_download(df_pivot.reset_index(), "tn_daily_comparison")
            st.download_button(label=label, data=data, file_name=name, mime=mime)

# ========================================================
# é¡µé¢ 2: å¤šæ¨¡å‹ç´¯è®¡å¢é•¿ (è¶‹åŠ¿å¯¹æ¯”)
# ========================================================
elif page == NAV_CUMULATIVE_COMPARE:
    st.subheader("å¤šæ¨¡å‹ç´¯è®¡å¢é•¿å¯¹æ¯”")
    
    selected_names = st.multiselect(
        "é€‰æ‹©è¦å¯¹æ¯”çš„æ¨¡å‹:", 
        all_model_names, 
        default=all_model_names[:3] if len(all_model_names) >=3 else all_model_names
    )

    if selected_names:
        plot_data = []
        max_day_plot = 0
        
        cols = st.columns(len(selected_names))
        for idx, name in enumerate(selected_names):
            m_df_temp = df[df['Display_Name'] == name].sort_values('Date')
            if not m_df_temp.empty:
                s_date = m_df_temp.iloc[0]['Date'].strftime('%Y-%m-%d')
                cols[idx].caption(f"ğŸ“… **{name}**: {s_date}")

        for name in selected_names:
            m_df = df[df['Display_Name'] == name].sort_values('Date')
            m_df['Cum_Tokens'] = m_df['Total_Tokens'].cumsum()
            if len(m_df) > 1: m_df = m_df.iloc[:-1]
            if m_df.empty: continue

            start_date = m_df.iloc[0]['Date']
            current_max_day = (m_df.iloc[-1]['Date'] - start_date).days
            if current_max_day > max_day_plot:
                max_day_plot = current_max_day

            for _, row in m_df.iterrows():
                day_num = (row['Date'] - start_date).days
                plot_data.append({
                    'Model': name, 'Day': day_num,
                    'Date': row['Date'].strftime('%Y-%m-%d'),
                    'Cumulative_Tokens': row['Cum_Tokens']
                })

        if plot_data:
            df_plot = pd.DataFrame(plot_data)

            # ã€é…è‰²ä¼˜åŒ–ã€‘ä½¿ç”¨ tableau10 é«˜å¯¹æ¯”é…è‰²
            base = alt.Chart(df_plot).encode(
                x=alt.X('Day', title="ä¸Šçº¿å¤©æ•° (Daily)", 
                        scale=alt.Scale(domain=[0, max_day_plot + 2], clamp=True),
                        axis=alt.Axis(labelFontSize=16, titleFontSize=18, grid=True)),
                y=alt.Y('Cumulative_Tokens', title='ç´¯è®¡ Token (Billion)', 
                        axis=alt.Axis(labelFontSize=16, titleFontSize=18)),
                # å…³é”®ä¿®æ”¹ï¼šscale=alt.Scale(scheme='tableau10')
                color=alt.Color('Model', 
                                title='æ¨¡å‹åç§°', 
                                scale=alt.Scale(scheme='tableau10'),
                                legend=alt.Legend(orient='bottom')),
                tooltip=['Model', 'Day', 'Date', 'Cumulative_Tokens']
            )
            chart = (base.mark_line(strokeWidth=3) + base.mark_circle(size=60)).properties(height=600)
            
            st.altair_chart(chart, use_container_width=True)

            st.markdown("### ğŸ“… ç´¯è®¡æ•°å€¼æ˜ç»†")
            df_pivot = df_plot.pivot_table(index='Day', columns='Model', values='Cumulative_Tokens')
            st.dataframe(df_pivot.style.format("{:.4f} B"), use_container_width=True)

            data, name, mime, label = get_dataset_download(df_pivot.reset_index(), "cumulative_growth")
            st.download_button(label=label, data=data, file_name=name, mime=mime)

# ========================================================
# é¡µé¢ 3: å•æ¨¡å‹æ¯æ—¥è¯¦æƒ… (è¶‹åŠ¿åˆ†æ + æ—¥æœŸç­›é€‰)
# ========================================================
elif page == NAV_DETAIL_DAILY:
    st.subheader("å•æ¨¡å‹æ¯æ—¥è¯¦æƒ…è¶‹åŠ¿")
    
    selected_name = st.selectbox("é€‰æ‹©æ¨¡å‹", all_model_names)
    m_df_full = df[df['Display_Name'] == selected_name].sort_values('Date')
    
    if not m_df_full.empty:
        min_date = m_df_full['Date'].min().date()
        max_date = m_df_full['Date'].max().date()
        st.success(f"ğŸ“… **{selected_name}** æ•°æ®æ”¶å½•åŒºé—´: {min_date} è‡³ {max_date}")

        col_filter1, col_filter2 = st.columns([1, 3])
        with col_filter1:
            date_range = st.date_input(
                "ğŸ” ç­›é€‰æ—¶é—´æ®µ",
                value=(min_date, max_date),
                min_value=min_date,
                max_value=max_date
            )
        
        if len(date_range) == 2:
            start_filter, end_filter = date_range
            mask = (m_df_full['Date'].dt.date >= start_filter) & (m_df_full['Date'].dt.date <= end_filter)
            m_df = m_df_full.loc[mask]
        else:
            m_df = m_df_full

        if not m_df.empty:
            latest = m_df.iloc[-1]
            c1, c2, c3 = st.columns(3)
            c1.metric("é€‰æ®µæœ€æ–°æ—¥æœŸ", latest['Date'].strftime('%Y-%m-%d'))
            c2.metric("å½“æ—¥æ¶ˆè€—", f"{latest['Total_Tokens']:.4f} B")
            if latest.get('Reasoning', 0) > 0 and latest.get('Completion', 0) > 0:
                ratio = (latest['Reasoning'] / latest['Completion']) * 100
                c3.metric("Reasoning å æ¯”", f"{ratio:.1f}%")
            else:
                c3.metric("Prompt Tokens", f"{latest['Prompt']:.4f} B")

            chart = alt.Chart(m_df).mark_line(point=True).encode(
                x=alt.X('Date', title='æ—¥æœŸ', axis=alt.Axis(format='%m-%d')),
                y=alt.Y('Total_Tokens', title='Token (Billion)'),
                tooltip=['Date', 'Total_Tokens', 'Prompt', 'Completion']
            )
            
            st.altair_chart(chart, use_container_width=True)
            
            display_cols = ['Date', 'Total_Tokens', 'Prompt', 'Completion', 'Reasoning']
            valid_cols = [c for c in display_cols if c in m_df.columns]
            st.dataframe(m_df[valid_cols].sort_values('Date', ascending=False).style.format({'Total_Tokens':'{:.4f}'}), use_container_width=True)

            data, name, mime, label = get_dataset_download(m_df[valid_cols], f"{selected_name}_daily")
            st.download_button(label=label, data=data, file_name=name, mime=mime)
        else:
            st.warning("âš ï¸ æ‰€é€‰æ—¶é—´æ®µå†…æ— æ•°æ®ã€‚")

# ========================================================
# é¡µé¢ 4: åŸå§‹æ•°æ®æ£€æŸ¥
# ========================================================
elif page == NAV_RAW_DATA:
    st.subheader("æ•°æ®åº“åŸå§‹æ•°æ®")
    
    st.markdown("#### ğŸ’¾ å…¨é‡æ•°æ®ä¸‹è½½")
    data, name, mime, label = get_dataset_download(df, "full_history_database")
    st.download_button(label=label, data=data, file_name=name, mime=mime)
    
    st.divider()
    
    check_name = st.selectbox("é€‰æ‹©è¦æ£€æŸ¥çš„æ¨¡å‹:", all_model_names)
    filtered_df = df[df['Display_Name'] == check_name].sort_values('Date', ascending=False)
    
    st.dataframe(
        filtered_df.style.format({
            'Prompt': '{:.6f} B', 'Completion': '{:.6f} B', 
            'Reasoning': '{:.6f} B', 'Total_Tokens': '{:.6f} B'
        }), use_container_width=True
    )

# ========================================================
# é¡µé¢ 5: æ¯æ—¥é€Ÿè§ˆä¸åˆ†æ
# ========================================================
elif page == NAV_DAILY_BRIEF:
    st.subheader("æ¨¡å‹è¡¨ç°é€Ÿè§ˆä¸åˆ†ææŠ¥å‘Š")
    st.caption("åŸºäºå†å²æ•°æ®çš„å¤šç»´åº¦é‡åŒ–åˆ†æï¼Œæ‰€æœ‰æŒ‡æ ‡å‡ç”±æ•°æ®è‡ªåŠ¨è®¡ç®—ç”Ÿæˆã€‚")

    # --- é¢„è®¡ç®—æ‰€æœ‰æ¨¡å‹çš„æŒ‡æ ‡ ---
    latest_date = df['Date'].max()
    two_weeks_ago = latest_date - pd.Timedelta(days=14)
    seven_days_ago = latest_date - pd.Timedelta(days=7)

    metrics_list = []
    for name in all_model_names:
        m_df = df[df['Display_Name'] == name].sort_values('Date')
        if m_df.empty:
            continue
        # å»æ‰æœ€åä¸€å¤©ï¼ˆå½“å¤©æœªç»“ç®—æ•°æ®ï¼Œå’Œå…¶ä»–é¡µé¢é€»è¾‘ä¿æŒä¸€è‡´ï¼‰
        if len(m_df) > 1:
            m_df = m_df.iloc[:-1]
        if m_df.empty:
            continue

        first_date = m_df.iloc[0]['Date']
        last_date = m_df.iloc[-1]['Date']
        days_online = max((last_date - first_date).days, 1)
        cumulative = m_df['Total_Tokens'].sum()
        daily_avg = cumulative / days_online
        peak = m_df['Total_Tokens'].max()

        # è¿‘ 7 æ—¥å¢é€Ÿ
        recent_df = m_df[m_df['Date'] >= seven_days_ago]
        recent_days = max(len(recent_df), 1)
        recent_avg = recent_df['Total_Tokens'].sum() / recent_days if not recent_df.empty else 0

        # å¢é•¿åŠ¨é‡
        momentum = (recent_avg / daily_avg) if daily_avg > 0 else 0

        metrics_list.append({
            'Model': name,
            'First_Date': first_date,
            'Last_Date': last_date,
            'Days_Online': days_online,
            'Cumulative': round(cumulative, 4),
            'Daily_Avg': round(daily_avg, 4),
            'Recent_7d_Avg': round(recent_avg, 4),
            'Momentum': round(momentum, 2),
            'Peak': round(peak, 4),
        })

    df_metrics = pd.DataFrame(metrics_list)

    if df_metrics.empty:
        st.warning("æš‚æ— å¯åˆ†æçš„æ¨¡å‹æ•°æ®ã€‚")
        st.stop()

    # è®¡ç®—ç™¾åˆ†ä½æ’åï¼ˆä¾›åç»­æ¨¡å—ä½¿ç”¨ï¼‰
    df_metrics['Pct_Rank_DailyAvg'] = df_metrics['Daily_Avg'].rank(pct=True)

    # æå‰è®¡ç®—æ–°æ¨¡å‹æ•°æ®ä¾› AI ç®€æŠ¥ä½¿ç”¨
    new_models_df = df_metrics[df_metrics['First_Date'] >= two_weeks_ago].sort_values('First_Date', ascending=False)
    display_new = pd.DataFrame()
    if not new_models_df.empty:
        enhanced_new_models = []
        for row in new_models_df.itertuples():
            model_name = row.Model
            norm_name = normalize_model_name(model_name)
            
            # --- æŸ¥ä»·æ ¼ ---
            input_price, output_price = None, None
            if df_price is not None and not df_price.empty:
                latest_price_date = df_price['Date'].max()
                price_rows = df_price[(df_price['Date'] == latest_price_date) & 
                                      (df_price['Provider'] == 'Weighted Average')]
                
                # ç²¾ç¡®åŒ¹é…ï¼ˆé˜²æ­¢éƒ¨åˆ†åç§°è¢«è¯¯æ€ï¼‰
                matched_price_model = fuzzy_match_model(norm_name, price_rows['Model'].unique().tolist(), threshold=0.55)
                if matched_price_model:
                    match_row = price_rows[price_rows['Model'] == matched_price_model[0]].iloc[0]
                    input_price = match_row.get('Input_Price_1M')
                    output_price = match_row.get('Output_Price_1M')

            # --- æŸ¥ LMARENA æ’å ---
            arena_rank = None
            if df_lmarena is not None and not df_lmarena.empty:
                latest_lm_date = df_lmarena['Date'].max()
                lm_rows = df_lmarena[df_lmarena['Date'] == latest_lm_date]
                matched_lm_model = fuzzy_match_model(norm_name, lm_rows['Model'].unique().tolist(), threshold=0.55)
                if matched_lm_model:
                    match_row = lm_rows[lm_rows['Model'] == matched_lm_model[0]].iloc[0]
                    arena_rank = match_row.get('Rank_Overall')
            
            enhanced_new_models.append({
                'æ¨¡å‹åç§°': model_name,
                'ä¸Šçº¿æ—¥æœŸ': row.First_Date.strftime('%Y-%m-%d'),
                'ä¸Šçº¿å¤©æ•°': row.Days_Online,
                'ç´¯è®¡æ¶ˆè€— (B)': row.Cumulative,
                'æ—¥å‡æ¶ˆè€— (B)': row.Daily_Avg,
                'è¾“å…¥ä»·æ ¼ ($/1M)': f"${input_price:.4f}" if pd.notna(input_price) else "-",
                'è¾“å‡ºä»·æ ¼ ($/1M)': f"${output_price:.4f}" if pd.notna(output_price) else "-",
                'Arena æ’å': f"{int(arena_rank)}" if pd.notna(arena_rank) else "-"
            })
        display_new = pd.DataFrame(enhanced_new_models)

# ============================
    # æ¨¡å— A (ç½®é¡¶): AI æ™ºèƒ½ç®€æŠ¥åˆ†æ
    # ============================
    st.markdown("---")
    st.markdown("### ğŸ¤– æ™ºèƒ½è¶‹åŠ¿ç®€æŠ¥")
    st.caption("åŸºäºä»Šæ—¥æ•°æ®çš„è‡ªåŠ¨æ·±åº¦åˆ†æ (æ•°æ®æ¯æ—¥è‡ªåŠ¨ç¼“å­˜ï¼Œé¿å…é‡å¤è¯·æ±‚)")
    
    # æ„é€ ç»™ AI çš„å½“æ—¥å…³é”®æ•°æ®
    if not df_metrics.empty:
        # è·å–è¡¨ç°æœ€ä¼˜å’Œæœ€å·®çš„æ¨¡å‹
        top_momentum = df_metrics.nlargest(5, 'Momentum')
        low_momentum = df_metrics.nsmallest(5, 'Momentum')
        
        # æå–æ–°æ¨¡å‹ç®€æŠ¥æ•°æ®
        new_models_context = ""
        if not new_models_df.empty:
            new_models_context = display_new.to_string(index=False)
            
        ai_brief_prompt = f"""
ä½ æ˜¯ä¸€ä½èµ„æ·± TMT è¡Œä¸šæŠ•ç ”åˆ†æå¸ˆã€‚è¯·åŸºäºä»¥ä¸‹æœ€æ–°æ•°æ®ï¼Œç›´æ¥æ’°å†™ä¸€ä»½ã€å¤§æ¨¡å‹è¶‹åŠ¿è¿½è¸ªç®€æŠ¥ã€‘ã€‚
å½“å‰æ—¥æœŸ: {latest_date.strftime('%Y-%m-%d')}

## 1. è¿‘ä¸¤å‘¨æ–°ä¸Šçº¿æ¨¡å‹ï¼ˆé™„å¸¦ä»·æ ¼å’Œå¤–éƒ¨èƒ½åŠ›æ’åï¼‰
{new_models_context if new_models_context else "è¿‘ä¸¤å‘¨æ— æ–°æ¨¡å‹ã€‚"}

## 2. å¢é•¿åŠ¨é‡ (Momentum) æé€Ÿä¸Šå‡çš„æ¨¡å‹ TOP 5 (å¯èƒ½å­˜åœ¨ç ´åœˆæˆ–å…è´¹æ´»åŠ¨)
{top_momentum[['Model', 'Momentum', 'Daily_Avg', 'Recent_7d_Avg']].to_string(index=False)}

## 3. å¢é€Ÿæ”¾ç¼“æˆ–èç¼©çš„æ¨¡å‹ TOP 5
{low_momentum[['Model', 'Momentum', 'Daily_Avg', 'Recent_7d_Avg']].to_string(index=False)}

## åˆ†æè¦æ±‚ï¼ˆåŠ¡å¿…ä½¿ç”¨ç½‘ç»œæ’ä»¶æ£€ç´¢æ”¿ç­–/æ´»åŠ¨åŸå› ï¼‰ï¼š
è¯·ç›´æ¥è¾“å‡ºä¸€æ®µç®€ç»ƒçš„ã€è¨€ä¹‹æœ‰ç‰©ã€æœ‰æ•°æ®æœ‰é€»è¾‘çš„æŠ•ç ”åˆ†æï¼Œä¸è¦ä»»ä½•å¯’æš„ã€æ€»ç»“æ€§å¥—è¯æˆ–è¿‡åº¦æ‹”é«˜çš„å®šæ€§åˆ†æã€‚
è¯·åŠ¡å¿…å‚è€ƒä»¥ä¸‹é£æ ¼å’Œç»“æ„æ’°å†™ï¼ˆå°†å…¶ä½œä¸ºä½ çš„æ ¼å¼æ¨¡æ¿ï¼‰ï¼š

â€œæœ¬å‘¨å— [æŸå¤–éƒ¨äº‹ä»¶/èŠ‚ç‚¹] å½±å“ï¼Œå¤§æ¨¡å‹æ¨æ–° [åŠ å¿«/æ”¾ç¼“]ï¼Œå…±æ¨å‡º [X] æ¬¾æ¨¡å‹ã€‚

ä¸€ã€é‡ç‚¹æ–°æ¨¡å‹è¿½è¸ª
è¿‘æœŸæ–°ä¸Šçº¿çš„ä¸€æ‰¹æ¨¡å‹ä¸­ï¼Œåªè¦æ˜¯æ¥è‡ªå¤´éƒ¨å¤§å‚ï¼ˆå¦‚ OpenAIã€Anthropicã€Googleã€Moonshot (Kimi)ã€DeepSeekã€é˜¿é‡Œã€è…¾è®¯ã€ç™¾åº¦ç­‰ï¼‰çš„æ¨¡å‹å¿…é¡»å…¨éƒ¨ç‚¹åæ’­æŠ¥ã€‚å…¶ä¸­æœ€å€¼å¾—å…³æ³¨çš„æ˜¯ [æ¨¡å‹A] å’Œ [æ¨¡å‹B]ï¼Œå…¶æ ¸å¿ƒæ€§èƒ½æŒ‡æ ‡ [æ’å/åŸºå‡†] è¾¾åˆ° [X]ï¼Œè€Œä»·æ ¼ä¸º [è¾“å…¥X/è¾“å‡ºX]ï¼Œå¯¹ç°æœ‰å¸‚åœºæ ¼å±€å¸¦æ¥ä¸€å®šå½±å“ã€‚

äºŒã€æ¨¡å‹åŠ¨é‡å˜åŠ¨åˆ†æ
åœ¨è°ƒç”¨é‡å¢é€Ÿæ–¹é¢ï¼Œå‰”é™¤é•¿å°¾éæ——èˆ°æ¨¡å‹åï¼Œ[æ——èˆ°/æ–°æ¨¡å‹C] ä¸Šé‡çŒ›å¢ï¼Œè¿‘7æ—¥å‡å€¼è¾¾åˆ° [X] Bï¼ŒåŠ¨é‡é«˜è¾¾ [X]ã€‚ç»æŸ¥è¯ï¼Œè¿™æ˜¯å› ä¸ºå…¶æ¨å‡ºäº† [å…·ä½“çš„ä¼˜æƒ æ”¿ç­–/èƒ½åŠ›æ›´æ–°]ï¼›å¦å¤–ï¼Œ[æ——èˆ°æ¨¡å‹D] å¢é€Ÿåœæ»æˆ–èç¼©ï¼Œæ¨æµ‹ä¸ [å…·ä½“é€€å¸‚/å•†ä¸šåŒ–ç­–ç•¥] æœ‰å…³ã€‚

ä¸‰ã€å¤šæ¨¡æ€èµ›é“è§‚å¯Ÿ
åœ¨å›¾åƒã€éŸ³é¢‘ã€è§†é¢‘ç­‰å¤šæ¨¡æ€é¢†åŸŸï¼Œæœ¬å‘¨ [æ¨¡å‹E/F] è¡¨ç°äº®çœ¼ / [æˆ–æš‚æ— æ˜æ˜¾åŠ¨å‘]ã€‚[åŸºäºæ•°æ®çš„å¤šæ¨¡æ€èƒ½åŠ›ä¸å®šä»·åˆ†æ]â€

ä»»åŠ¡è§„èŒƒï¼š
1. å¤§å‚è¿½è¸ªï¼šæ–°æ¨¡å‹åˆ—è¡¨ä¸­ï¼Œå‡¡æ˜¯å±äºå¤´éƒ¨ä¸»æµå‚å•†çš„ï¼ˆæ— è®ºæ˜¯å›½å†…å¤–ï¼‰ï¼Œå¿…é¡»å…¨éƒ¨æåŠï¼Œä¸é—æ¼ã€‚
2. åŠ¨é‡è¿‡æ»¤ï¼šå¯¹äºâ€œæé€Ÿä¸Šå‡çš„ TOP 5â€ï¼Œå¦‚æœé‡åˆ°ä¸çŸ¥åã€éæ——èˆ°ã€éæ–°ä¸Šçº¿çš„é•¿å°¾æ¨¡å‹ï¼Œè¯·ç›´æ¥å¿½ç•¥å®ƒï¼ŒåªæŒ‘å…¶ä¸­å…·æœ‰è¡Œä¸šä»£è¡¨æ€§çš„æ——èˆ°æˆ–æ–°æ¨¡å‹è¿›è¡Œåˆ†æã€‚
3. äº‹å®ä¸ºå‡†ï¼šå¿…é¡»ä½¿ç”¨å‡†ç¡®çš„ä¸Šæ–¹æ•°æ®ï¼Œå¹¶åœ¨é™ˆè¿°åŸå› æ—¶å¿…é¡»é€šè¿‡ç½‘ç»œæœç´¢å‡ºç¡®åˆ‡çš„æ”¿ç­–æˆ–ç‰ˆæœ¬äº‹ä»¶ï¼ˆå¦‚æŸå¤§å‚åœ¨å‡ å·å®£å¸ƒäº†ä»€ä¹ˆAPIå…è´¹è®¡åˆ’ï¼Œæˆ–è€…å‘å¸ƒæ–°ç‰ˆæœ¬ï¼‰ã€‚åšå†³æ‹’ç»â€œå¼•çˆ†å…¨çƒè°ƒç”¨â€ç­‰å‡å¤§ç©ºçš„æŠ’æƒ…ä¸»è§‚è¯æ±‡ã€‚
        """

        @st.cache_data(ttl=86400, show_spinner=False)
        def fetch_daily_ai_brief(prompt, provider="Google AI Studio"):
            import requests as _req
            cfg = AI_PROVIDERS.get(provider, AI_PROVIDERS["Google AI Studio"])
            key = cfg["key"]
            if not key: raise Exception(f"ç¼ºå¤± {provider} API Key")
            
            try:
                headers = {"Authorization": f"Bearer {key}", "Content-Type": "application/json"}
                
                # åŠ¨æ€è·å–è¯¥æä¾›å•†é…ç½®çš„ç¬¬ä¸€ä¸ªæ¨¡å‹ï¼Œé˜²æ­¢ç¡¬ç¼–ç å¯¼è‡´æ¨¡å‹ä¸å­˜åœ¨ (400) é—®é¢˜
                if cfg.get("models"):
                    model = list(cfg["models"].values())[0]
                else:
                    model = "z-ai/glm-4.5-air:free" # Fallback
                
                payload = {
                    "model": model,
                    "messages": [{"role": "user", "content": prompt}],
                    "max_tokens": 2500,
                }
                
                # åªæœ‰ OpenRouter æ”¯æŒè¿™é‡Œçš„ web æ’ä»¶è¯­æ³•
                if provider == "OpenRouter":
                    payload["plugins"] = [{"id": "web", "max_results": 4}]
                else:
                    # å¯¹äºç”Ÿç®€æŠ¥çš„é OpenRouter æ¨¡å‹ï¼Œä¹Ÿæ‰§è¡Œä¸€è½®é¢„æœç´¢å¸®åŠ©å®ƒå¯»æ‰¾æœ€æ–°çš„èµ„è®¯
                    try:
                        from duckduckgo_search import DDGS
                        with DDGS() as ddgs:
                            news_res = list(ddgs.text("AI å¤§æ¨¡å‹ è¿‘æœŸåŠ¨æ€", max_results=5, timelimit='w'))
                        if news_res:
                            context_str = "\n\nã€è¡¥å……èµ„æ–™ï¼šè¿‘æœŸå¤§æ¨¡å‹ç›¸å…³æ–°é—»ã€‘ï¼š\n"
                            for r in news_res:
                                context_str += f"- {r.get('title', '')}: {r.get('body', '')}\n"
                            payload["messages"][0]["content"] += context_str
                    except Exception as e:
                        pass
                
                resp = _req.post(
                    f"{cfg['base_url']}/chat/completions",
                    headers=headers,
                    json=payload,
                    timeout=300
                )
                resp.raise_for_status()
                result = resp.json()
                return result['choices'][0]['message']['content']
            except Exception as e:
                raise Exception(f"ç®€æŠ¥ç”Ÿæˆå¤±è´¥: {str(e)}")
                
        # å…è®¸åˆ‡æ¢ç®€æŠ¥æä¾›å•†å¹¶å±•ç¤ºå½“å‰å…·ä½“æ¨¡å‹
        st.sidebar.divider()
        brief_provider = st.sidebar.selectbox("ç®€æŠ¥ç”Ÿæˆå•†:", list(AI_PROVIDERS.keys()), index=1, help="é»˜è®¤ä½¿ç”¨ Google (Gemini 2.5 Flash)")
        
        # å®æ—¶æ˜¾ç¤ºè¯¥å•†è°ƒç”¨çš„å…·ä½“æ¨¡å‹ï¼ˆå–é…ç½®ä¸­ç¬¬ä¸€ä¸ªï¼‰
        current_brief_model = list(AI_PROVIDERS[brief_provider]["models"].values())[0] if AI_PROVIDERS[brief_provider].get("models") else "Unknown"
        st.sidebar.caption(f"å½“å‰ç®€æŠ¥é€šé“æ¨¡å‹ï¼š`{current_brief_model}`")

        with st.spinner(f"ğŸ¤– æ­£åœ¨è°ƒç”¨ {brief_provider} ({current_brief_model}) ç”Ÿæˆå½“æ—¥ç®€æŠ¥..."):
            try:
                brief_report = fetch_daily_ai_brief(ai_brief_prompt, provider=brief_provider)
                st.markdown(brief_report)
            except Exception as call_err:
                st.error(f"ğŸ¤– åˆ†ææŠ¥å‘Šç”Ÿæˆå¤±è´¥: {call_err}")
                if st.button("ğŸ”„ é‡è¯•"):
                    fetch_daily_ai_brief.clear()
                    st.rerun()
    else:
        st.info("æ•°æ®ä¸è¶³ï¼Œæ— æ³•ç”Ÿæˆæ€»ç»“æŠ¥å‘Šã€‚")

    # ============================
    # æ¨¡å— B: è¿‘ä¸¤å‘¨æ–°å¢æ¨¡å‹ä¸€è§ˆ
    # ============================
    st.markdown("---")
    st.markdown("### è¿‘ä¸¤å‘¨æ–°å¢æ¨¡å‹ä¸€è§ˆ")
    st.caption(f"ç»Ÿè®¡åŒºé—´: {two_weeks_ago.strftime('%Y-%m-%d')} ~ {latest_date.strftime('%Y-%m-%d')}")

    if new_models_df.empty:
        st.info("è¿‡å»ä¸¤å‘¨å†…æ²¡æœ‰æ–°ä¸Šçº¿çš„æ¨¡å‹ã€‚")
    else:
        st.markdown(f"è¿‡å»ä¸¤å‘¨å…±ä¸Šçº¿ **{len(new_models_df)}** ä¸ªæ–°æ¨¡å‹ã€‚")
        st.dataframe(
            display_new.style.format({'ç´¯è®¡æ¶ˆè€— (B)': '{:.4f}', 'æ—¥å‡æ¶ˆè€— (B)': '{:.4f}'}),
            use_container_width=True, hide_index=True
        )

    # ============================
    # æ¨¡å— C: æ–°æ¨¡å‹ç´¯è®¡å¢é•¿å¯¹æ¯”
    # ============================
    if not new_models_df.empty:
        st.markdown("---")
        st.markdown("### æ–°æ¨¡å‹ç´¯è®¡å¢é•¿å¯¹æ¯”")

        new_model_names = new_models_df['Model'].tolist()
        plot_new = []
        max_day_new = 0

        for name in new_model_names:
            m_df = df[df['Display_Name'] == name].sort_values('Date')
            m_df['Cum_Tokens'] = m_df['Total_Tokens'].cumsum()
            if len(m_df) > 1:
                m_df = m_df.iloc[:-1]
            if m_df.empty:
                continue
            start_date = m_df.iloc[0]['Date']
            current_max = (m_df.iloc[-1]['Date'] - start_date).days
            if current_max > max_day_new:
                max_day_new = current_max
            for _, row in m_df.iterrows():
                day_n = (row['Date'] - start_date).days
                plot_new.append({
                    'Model': name, 'Day': day_n,
                    'Date': row['Date'].strftime('%Y-%m-%d'),
                    'Cumulative_Tokens': row['Cum_Tokens']
                })

        if plot_new:
            df_plot_new = pd.DataFrame(plot_new)
            base_new = alt.Chart(df_plot_new).encode(
                x=alt.X('Day', title='ä¸Šçº¿å¤©æ•°',
                        scale=alt.Scale(domain=[0, max_day_new + 2], clamp=True),
                        # å¼ºåˆ¶ tickMinStep=1 é¿å…æ˜¾ç¤ºå°æ•°åˆ»åº¦
                        axis=alt.Axis(tickMinStep=1, format='d', labelFontSize=14, titleFontSize=16, grid=True)),
                y=alt.Y('Cumulative_Tokens', title='ç´¯è®¡ Token (Billion)',
                        axis=alt.Axis(labelFontSize=14, titleFontSize=16)),
                color=alt.Color('Model', title='æ¨¡å‹',
                                scale=alt.Scale(scheme='tableau10'),
                                legend=alt.Legend(orient='bottom')),
                tooltip=['Model', 'Day', 'Date', 'Cumulative_Tokens']
            )
            chart_new = (base_new.mark_line(strokeWidth=3) + base_new.mark_circle(size=60)).properties(height=500)
            st.altair_chart(chart_new, use_container_width=True)
        else:
            st.info("æ–°æ¨¡å‹æš‚æ— è¶³å¤Ÿæ•°æ®ç»˜åˆ¶è¶‹åŠ¿å›¾ã€‚")

    # ============================
    # æ¨¡å— C: åˆ†ææ‘˜è¦ (è¡¨æ ¼åŒ–)
    # ============================
    st.markdown("---")
    st.markdown("### ç»¼åˆåˆ†ææ‘˜è¦")
    st.caption(f"åˆ†æåŸºå‡†æ—¥: {latest_date.strftime('%Y-%m-%d')}")

    # Top 3 ç´¯è®¡æ¶ˆè€—
    with st.expander("ç´¯è®¡æ¶ˆè€— Top 3", expanded=True):
        top3_cum = df_metrics.nlargest(3, 'Cumulative').copy()
        top3_cum['Rank'] = range(1, len(top3_cum) + 1)
        display_top3 = top3_cum[['Rank', 'Model', 'Cumulative', 'Days_Online', 'Daily_Avg']].copy()
        display_top3.columns = ['æ’å', 'æ¨¡å‹', 'ç´¯è®¡æ¶ˆè€— (B)', 'ä¸Šçº¿å¤©æ•°', 'æ—¥å‡æ¶ˆè€— (B)']
        st.dataframe(
            display_top3.style.format({'ç´¯è®¡æ¶ˆè€— (B)': '{:.4f}', 'æ—¥å‡æ¶ˆè€— (B)': '{:.4f}'}),
            use_container_width=True, hide_index=True
        )

    # è¿‘ 7 æ—¥å¢é€Ÿæœ€å¿«
    with st.expander("è¿‘7æ—¥å¢é€Ÿé¢†å…ˆ (Top 3)", expanded=True):
        top3_recent = df_metrics.nlargest(3, 'Recent_7d_Avg').copy()
        top3_recent['Rank'] = range(1, len(top3_recent) + 1)
        display_recent = top3_recent[['Rank', 'Model', 'Recent_7d_Avg']].copy()
        display_recent.columns = ['æ’å', 'æ¨¡å‹', 'è¿‘7æ—¥æ—¥å‡æ¶ˆè€— (B)']
        st.dataframe(
            display_recent.style.format({'è¿‘7æ—¥æ—¥å‡æ¶ˆè€— (B)': '{:.4f}'}),
            use_container_width=True, hide_index=True
        )

    # åŠ é€Ÿå¢é•¿ä¸­çš„æ¨¡å‹
    with st.expander("æ­£åœ¨åŠ é€Ÿå¢é•¿ (åŠ¨é‡ > 1.2)", expanded=True):
        accel = df_metrics[df_metrics['Momentum'] >= 1.2].sort_values('Momentum', ascending=False)
        if not accel.empty:
            accel['Growth_Pct'] = (accel['Momentum'] - 1) * 100
            display_accel = accel[['Model', 'Momentum', 'Growth_Pct']].head(5).copy()
            display_accel.columns = ['æ¨¡å‹', 'åŠ¨é‡å€¼', 'è¿‘æœŸå¢é€Ÿè¶…å‡ºå‡å€¼ (%)']
            st.dataframe(
                display_accel.style.format({'åŠ¨é‡å€¼': '{:.2f}', 'è¿‘æœŸå¢é€Ÿè¶…å‡ºå‡å€¼ (%)': '+{:.0f}%'}),
                use_container_width=True, hide_index=True
            )
        else:
            st.info("æš‚æ— æ˜æ˜¾åŠ é€Ÿå¢é•¿çš„æ¨¡å‹ã€‚")

    # å¢é€Ÿæ”¾ç¼“çš„æ¨¡å‹
    with st.expander("å¢é€Ÿæ”¾ç¼“å…³æ³¨ (åŠ¨é‡ < 0.8)", expanded=True):
        decel = df_metrics[(df_metrics['Momentum'] <= 0.8) & (df_metrics['Days_Online'] >= 7)].sort_values('Momentum')
        if not decel.empty:
            decel['Slowdown_Pct'] = (1 - decel['Momentum']) * 100
            display_decel = decel[['Model', 'Momentum', 'Slowdown_Pct']].head(5).copy()
            display_decel.columns = ['æ¨¡å‹', 'åŠ¨é‡å€¼', 'è¿‘æœŸå¢é€Ÿä½äºå‡å€¼ (%)']
            st.dataframe(
                display_decel.style.format({'åŠ¨é‡å€¼': '{:.2f}', 'è¿‘æœŸå¢é€Ÿä½äºå‡å€¼ (%)': '-{:.0f}%'}),
                use_container_width=True, hide_index=True
            )
        else:
            st.info("æš‚æ— æ˜æ˜¾å¢é€Ÿæ”¾ç¼“çš„æ¨¡å‹ã€‚")

    # æ–°æ¨¡å‹é€Ÿè¯„ï¼ˆä½¿ç”¨ç™¾åˆ†ä½æ’åäº”çº§åˆ¶ï¼‰
    if not new_models_df.empty:
        with st.expander("æ–°æ¨¡å‹åˆæœŸè¡¨ç°è¯„çº§", expanded=True):
            rating_data = []
            for row in new_models_df.itertuples():
                pct_rank = row.Pct_Rank_DailyAvg
                if pct_rank >= 0.90:
                    tier, desc = "S Â· å¤´éƒ¨æ°´å¹³", f"è¶…è¿‡ {pct_rank*100:.0f}% æ¨¡å‹"
                elif pct_rank >= 0.75:
                    tier, desc = "A Â· è¡¨ç°ä¼˜å¼‚", f"è¶…è¿‡ {pct_rank*100:.0f}% æ¨¡å‹"
                elif pct_rank >= 0.50:
                    tier, desc = "B Â· ä¸­ç­‰æ°´å¹³", "æ—¥å‡ > ä¸­ä½æ•°"
                elif pct_rank >= 0.25:
                    tier, desc = "C Â· ä½äºé¢„æœŸ", f"ä»…è¶… {pct_rank*100:.0f}% æ¨¡å‹"
                else:
                    tier, desc = "D Â· èµ·æ­¥ç¼“æ…¢", f"å {(1-pct_rank)*100:.0f}% åˆ†ä½"
                
                rating_data.append({
                    'æ¨¡å‹': row.Model,
                    'ä¸Šçº¿æ—¥æœŸ': row.First_Date.strftime('%m-%d'),
                    'æ—¥å‡æ¶ˆè€— (B)': row.Daily_Avg,
                    'è¯„çº§': tier,
                    'è¯´æ˜': desc
                })
            
            df_rating = pd.DataFrame(rating_data)
            st.dataframe(
                df_rating.style.format({'æ—¥å‡æ¶ˆè€— (B)': '{:.4f}'}),
                use_container_width=True, hide_index=True
            )

    # ============================
    # æ¨¡å— D : å…¨æ¨¡å‹è¡¨ç°æ’å 
    # ============================
    st.markdown("---")
    st.markdown("### å…¨æ¨¡å‹è¡¨ç°æ’å (Top 15)")

    RANK_OPTIONS = {
        'ç´¯è®¡æ€»é‡': 'Cumulative',
        'æ—¥å‡æ¶ˆè€—': 'Daily_Avg',
        'è¿‘7æ—¥å¢é€Ÿ': 'Recent_7d_Avg',
        'å¢é•¿åŠ¨é‡': 'Momentum',
        'å³°å€¼æ¶ˆè€—': 'Peak',
        'ä¸Šçº¿å¤©æ•°': 'Days_Online'
    }
    col_rank1, col_rank2 = st.columns([1, 3])
    with col_rank1:
        rank_label = st.selectbox("é€‰æ‹©æ’åç»´åº¦", list(RANK_OPTIONS.keys()))
    rank_col = RANK_OPTIONS[rank_label]

    df_ranked = df_metrics.sort_values(rank_col, ascending=False).head(15).reset_index(drop=True)
    df_ranked.index = df_ranked.index + 1

    chart_rank = alt.Chart(df_ranked).mark_bar(
        cornerRadiusTopLeft=4, cornerRadiusTopRight=4
    ).encode(
        x=alt.X('Model', sort='-y', title='æ¨¡å‹',
                axis=alt.Axis(labelAngle=-45, labelFontSize=11)),
        y=alt.Y(rank_col, title=rank_label,
                axis=alt.Axis(labelFontSize=14, titleFontSize=16)),
        color=alt.Color('Model', legend=None, scale=alt.Scale(scheme='tableau10')),
        tooltip=['Model', alt.Tooltip(rank_col, title=rank_label, format='.4f')]
    ).properties(height=400)
    st.altair_chart(chart_rank, use_container_width=True)

    display_ranked = df_ranked[['Model', 'Days_Online', 'Cumulative', 'Daily_Avg', 'Recent_7d_Avg', 'Momentum', 'Peak']].copy()
    display_ranked.columns = ['æ¨¡å‹', 'ä¸Šçº¿å¤©æ•°', 'ç´¯è®¡ (B)', 'æ—¥å‡ (B)', 'è¿‘7æ—¥å‡ (B)', 'åŠ¨é‡', 'å³°å€¼ (B)']

    def highlight_momentum(val):
        if isinstance(val, (int, float)):
            if val >= 1.2:
                return 'background-color: #d4edda; color: #155724'
            elif val <= 0.8:
                return 'background-color: #f8d7da; color: #721c24'
        return ''

    st.dataframe(
        display_ranked.style
            .format({'ç´¯è®¡ (B)': '{:.4f}', 'æ—¥å‡ (B)': '{:.4f}', 'è¿‘7æ—¥å‡ (B)': '{:.4f}', 'åŠ¨é‡': '{:.2f}', 'å³°å€¼ (B)': '{:.4f}'})
            .map(highlight_momentum, subset=['åŠ¨é‡']),
        use_container_width=True, hide_index=False
    )
    st.caption("åŠ¨é‡ > 1.2 (ç»¿è‰²èƒŒæ™¯) = åŠ é€Ÿå¢é•¿ Â· åŠ¨é‡ < 0.8 (çº¢è‰²èƒŒæ™¯) = å¢é€Ÿæ”¾ç¼“")





    # ============================
    # æ¨¡å— E: æŒ‡æ ‡å®šä¹‰ä¸å…¬å¼è¯´æ˜
    # ============================
    st.markdown("---")
    st.markdown("### é™„å½•: æŒ‡æ ‡å®šä¹‰ä¸å…¬å¼è¯´æ˜")
    with st.expander("æŸ¥çœ‹å®Œæ•´æŒ‡æ ‡è¯´æ˜", expanded=False):
        st.markdown("""
| æŒ‡æ ‡ | å®šä¹‰ | è®¡ç®—å…¬å¼ |
|------|------|----------|
| **æ—¥å‡æ¶ˆè€—** | æ¨¡å‹å…¨ç”Ÿå‘½å‘¨æœŸå†…å¹³å‡æ¯å¤©çš„ Token æ¶ˆè€—é‡ | `ç´¯è®¡æ€»é‡ Ã· ä¸Šçº¿å¤©æ•°` |
| **è¿‘7æ—¥å¢é€Ÿ** | æœ€è¿‘ 7 ä¸ªè‡ªç„¶æ—¥å†…çš„æ—¥å¹³å‡ Token æ¶ˆè€—é‡ | `Î£(è¿‘7æ—¥ Total_Tokens) Ã· è¿‘7æ—¥æ•°æ®æ¡æ•°` |
| **å¢é•¿åŠ¨é‡** | è¿‘æœŸæ´»è·ƒåº¦ç›¸å¯¹äºå…¨ç”Ÿå‘½å‘¨æœŸå‡å€¼çš„æ¯”ç‡ | `è¿‘7æ—¥å¢é€Ÿ Ã· æ—¥å‡æ¶ˆè€—` |
| **å³°å€¼æ¶ˆè€—** | å†å²å•æ—¥æœ€é«˜ Token æ¶ˆè€—é‡ | `max(æ¯æ—¥ Total_Tokens)` |
| **ç´¯è®¡æ€»é‡** | æ¨¡å‹ä¸Šçº¿ä»¥æ¥æ‰€æœ‰æ—¥æœŸ Token æ¶ˆè€—ä¹‹å’Œ | `Î£(Total_Tokens)` |
| **ä¸Šçº¿å¤©æ•°** | æ¨¡å‹é¦–æ¬¡å‡ºç°åœ¨æ•°æ®åº“åˆ°æœ€æ–°æ•°æ®çš„å¤©æ•° | `æœ€æ–°æ•°æ®æ—¥æœŸ - é¦–æ¬¡å‡ºç°æ—¥æœŸ` |

**åŠ¨é‡è§£è¯»:**
- åŠ¨é‡ = 1.0 â†’ è¿‘æœŸå¢é€Ÿä¸å…¨æœŸå‡å€¼æŒå¹³
- åŠ¨é‡ > 1.2 â†’ è¿‘æœŸå¤„äºåŠ é€Ÿå¢é•¿é˜¶æ®µ
- åŠ¨é‡ < 0.8 â†’ è¿‘æœŸå¢é€Ÿæ”¾ç¼“ï¼Œå¯èƒ½è¿›å…¥è¡°é€€æœŸ

**æ–°æ¨¡å‹è¯„çº§è¯´æ˜:**

è¯„çº§é‡‡ç”¨**ç™¾åˆ†ä½æ’åæ³• (Percentile Rank)**ï¼Œå°†æ–°æ¨¡å‹çš„æ—¥å‡æ¶ˆè€—æ”¾å…¥å…¨éƒ¨æ¨¡å‹çš„æ—¥å‡æ¶ˆè€—åˆ†å¸ƒä¸­è®¡ç®—æ’åç™¾åˆ†ä½:
- `ç™¾åˆ†ä½ = æ—¥å‡æ¶ˆè€— < è¯¥æ¨¡å‹çš„æ¨¡å‹æ•°é‡ Ã· æ€»æ¨¡å‹æ•°`

| è¯„çº§ | ç™¾åˆ†ä½åŒºé—´ | å«ä¹‰ |
|------|-----------|------|
| **S Â· å¤´éƒ¨æ°´å¹³** | â‰¥ P90 | æ—¥å‡æ¶ˆè€—è¶…è¿‡ 90% çš„æ¨¡å‹ï¼Œå±äºé¡¶çº§è¡¨ç° |
| **A Â· è¡¨ç°ä¼˜å¼‚** | P75 ~ P90 | æ—¥å‡æ¶ˆè€—å¤„äºå‰ 25%ï¼Œå¢é•¿åŠ¿å¤´å¼ºåŠ² |
| **B Â· ä¸­ç­‰æ°´å¹³** | P50 ~ P75 | æ—¥å‡æ¶ˆè€—é«˜äºä¸­ä½æ•°ï¼Œè¡¨ç°ä¸­è§„ä¸­çŸ© |
| **C Â· ä½äºé¢„æœŸ** | P25 ~ P50 | æ—¥å‡æ¶ˆè€—å¤„äºä¸­ä½æ•°ä»¥ä¸‹ï¼Œå…³æ³¨åç»­èµ°åŠ¿ |
| **D Â· èµ·æ­¥ç¼“æ…¢** | < P25     | æ—¥å‡æ¶ˆè€—å¤„äºå€’æ•° 25%ï¼Œå¸‚åœºæ¥å—åº¦è¾ƒä½ |
""")

# ========================================================
# é¡µé¢ 6: ä¾›åº”å•†ä»·æ ¼ä¸æœ‰æ•ˆå®šä»·åˆ†æ
# ========================================================
elif page == NAV_PRICING:
    st.subheader("æ¨¡å‹å®šä»·")
    st.caption("åŸºäº OpenRouter å‰ç«¯ API æŠ“å–çš„æœ€æ–°å®é™…æœ‰æ•ˆä»·æ ¼ã€‚")
    
    if df_price is None or df_price.empty:
        st.warning("æš‚æœªå‘ç°å¯ç”¨çš„å®šä»·æ•°æ®ã€‚")
    else:
        all_models = sorted(df_price['Model'].unique())
        selected_price_model = st.selectbox("é€‰æ‹©æ¨¡å‹:", all_models, index=0)

        # è·å–è¯¥æ¨¡å‹æ‰€æœ‰å†å²æ—¶é—´ç‚¹çš„æ•°æ®
        m_price_df = df_price[df_price['Model'] == selected_price_model].copy()
        
        # å°†æœ€æ–°çš„ç»¼åˆæŠ¥ä»·æ‹†å‡ºæ¥å±•ç¤º KPI
        latest_pricing_date = m_price_df['Date'].max()
        df_latest_prices = m_price_df[m_price_df['Date'] == latest_pricing_date]
        
        weighted_avg = df_latest_prices[df_latest_prices['Provider'] == 'Weighted Average']
        provider_latest = df_latest_prices[df_latest_prices['Provider'] != 'Weighted Average'].sort_values('Input_Price_1M')
        
        if not weighted_avg.empty:
            wa_row = weighted_avg.iloc[0]
            st.markdown("### æœ€æ–°æœ‰æ•ˆä»·æ ¼ (Weighted Average)")
            col1, col2 = st.columns(2)
            col1.metric("Input Price ($/1M)", f"${wa_row['Input_Price_1M']:.4f}")
            col2.metric("Output Price ($/1M)", f"${wa_row['Output_Price_1M']:.4f}")
        
        st.markdown("---")
        
        # === å›¾1: æœ‰æ•ˆä»·æ ¼è¶‹åŠ¿ (Weighted Average çš„ Input+Output å†å²) ===
        st.markdown("### æœ‰æ•ˆä»·æ ¼è¶‹åŠ¿")
        wa_history = m_price_df[m_price_df['Provider'] == 'Weighted Average'].copy()
        if not wa_history.empty:
            wa_history['Date'] = pd.to_datetime(wa_history['Date'])
            wa_long = wa_history.melt(
                id_vars=['Date'],
                value_vars=['Input_Price_1M', 'Output_Price_1M'],
                var_name='Type', value_name='Price'
            ).dropna(subset=['Price'])
            wa_long['Type'] = wa_long['Type'].map({'Input_Price_1M': 'Input', 'Output_Price_1M': 'Output'})
            
            chart_wa = alt.Chart(wa_long).mark_line(point=True).encode(
                x=alt.X('Date:T', title='æ—¶é—´', axis=alt.Axis(format='%m/%d')),
                y=alt.Y('Price:Q', title='ä»·æ ¼ ($/1M Tokens)'),
                color=alt.Color('Type:N', title='ç±»å‹'),
                tooltip=['Date:T', 'Type', alt.Tooltip('Price:Q', format='$.4f')]
            ).properties(height=300)
            st.altair_chart(chart_wa, use_container_width=True)
        else:
            st.info("æš‚æ— æœ‰æ•ˆä»·æ ¼å†å²æ•°æ®ã€‚")
        
        st.markdown("---")
        
        # === å›¾2: å„ä¾›åº”å•† Input ä»·æ ¼è¶‹åŠ¿æŠ˜çº¿å›¾ ===
        st.markdown("### å„ä¾›åº”å•† Input ä»·æ ¼è¶‹åŠ¿")
        provider_history = m_price_df[m_price_df['Provider'] != 'Weighted Average'].copy()
        if not provider_history.empty:
            provider_history['Date'] = pd.to_datetime(provider_history['Date'])
            
            chart_input = alt.Chart(provider_history).mark_line(point=True).encode(
                x=alt.X('Date:T', title='æ—¶é—´', axis=alt.Axis(format='%m/%d')),
                y=alt.Y('Input_Price_1M:Q', title='Input ä»·æ ¼ ($/1M Tokens)'),
                color=alt.Color('Provider:N', title='ä¾›åº”å•†', scale=alt.Scale(scheme='tableau20')),
                tooltip=['Date:T', 'Provider', alt.Tooltip('Input_Price_1M:Q', format='$.4f')]
            ).properties(height=350)
            st.altair_chart(chart_input, use_container_width=True)
        else:
            st.info("æš‚æ— ä¾›åº”å•† Input ä»·æ ¼æ•°æ®ã€‚")
        
        st.markdown("---")
        
        # === å›¾3: å„ä¾›åº”å•† Output ä»·æ ¼è¶‹åŠ¿æŠ˜çº¿å›¾ ===
        st.markdown("### å„ä¾›åº”å•† Output ä»·æ ¼è¶‹åŠ¿")
        if not provider_history.empty:
            chart_output = alt.Chart(provider_history).mark_line(point=True).encode(
                x=alt.X('Date:T', title='æ—¶é—´', axis=alt.Axis(format='%m/%d')),
                y=alt.Y('Output_Price_1M:Q', title='Output ä»·æ ¼ ($/1M Tokens)'),
                color=alt.Color('Provider:N', title='ä¾›åº”å•†', scale=alt.Scale(scheme='tableau20')),
                tooltip=['Date:T', 'Provider', alt.Tooltip('Output_Price_1M:Q', format='$.4f')]
            ).properties(height=350)
            st.altair_chart(chart_output, use_container_width=True)
        else:
            st.info("æš‚æ— ä¾›åº”å•† Output ä»·æ ¼æ•°æ®ã€‚")
        
        st.markdown("---")
        
        # ä¾›åº”å•†è¯¦æƒ…è¡¨æ ¼ï¼ˆæœ€æ–°ä¸€å¤©ï¼‰
        st.markdown("### ä¾›åº”å•†è¯¦æƒ… (æœ€æ–°)")
        if not provider_latest.empty:
            st.dataframe(
                provider_latest[['Provider', 'Input_Price_1M', 'Output_Price_1M', 'Cache_Hit_Rate']].style.format({
                    'Input_Price_1M': '${:.4f}',
                    'Output_Price_1M': '${:.4f}',
                    'Cache_Hit_Rate': '{:.1%}'
                }),
                use_container_width=True,
                hide_index=True
            )
            
        data, name, mime, label = get_dataset_download(df_price, "openrouter_pricing_full")
        st.download_button(label=label, data=data, file_name=name, mime=mime)

# ========================================================
# é¡µé¢ 7: Benchmark è·‘åˆ†æ•°æ®çŸ©é˜µ
# ========================================================
elif page == NAV_BENCHMARK:
    st.subheader("åŸºå‡†æµ‹è¯•ä¸æ’è¡Œæ¦œ")
    st.caption("æ•°æ®æºï¼šArtificial Analysis åŸºå‡†è·‘åˆ† + LMARENA (Chatbot Arena) ç«æŠ€æ’åã€‚")
    
    tab1, tab2, tab3 = st.tabs([
        "å•æŒ‡æ ‡æ’è¡Œ (AA Benchmark)",
        "å¤šæŒ‡æ ‡çŸ©é˜µ (AA Benchmark)",
        "LMARENA ç«æŠ€æ’å"
    ])
    
    # --- Tab 1 & 2: åŸæœ‰ Artificial Analysis Benchmark ---
    if df_bench is None or df_bench.empty:
        with tab1:
            st.warning("æš‚æœªå‘ç°å¯ç”¨çš„ Benchmark æ•°æ®ï¼Œè¯·ç¡®è®¤æ˜¯å¦æˆåŠŸè¿è¡Œ `openrouter_benchmark_scraper.py`ã€‚")
        with tab2:
            st.warning("æš‚æœªå‘ç°å¯ç”¨çš„ Benchmark æ•°æ®ã€‚")
    else:
        latest_bench_date = df_bench['Date'].max()
        df_latest_bench = df_bench[df_bench['Date'] == latest_bench_date].drop(columns=['Date'])
        
        # çŸ©é˜µè½¬ç½®ï¼šè®© Model å˜æˆ indexï¼ŒMetrics å˜æˆ columns
        bench_melted = df_latest_bench.melt(id_vars=['Metric'], var_name='Model', value_name='Score')
        bench_pivot = bench_melted.pivot_table(index='Model', columns='Metric', values='Score')
        
        metrics_available = bench_pivot.columns.tolist()
        
        with tab1:
            st.markdown("### æ ¸å¿ƒåŸºå‡†æµ‹è¯•æ’è¡Œæ¦œ")
            st.info(f"æ•°æ®æ›´æ–°äº: **{latest_bench_date.strftime('%Y-%m-%d')}**")
            primary_metric = st.selectbox("é€‰æ‹©æ’åºæŒ‡æ ‡:", metrics_available, index=0, key="tab1_metric")
            
            if primary_metric:
                bench_sorted = bench_pivot.sort_values(by=primary_metric, ascending=False).reset_index()
                bench_sorted = bench_sorted.dropna(subset=[primary_metric])
                
                top_10_models = bench_sorted['Model'].head(10).tolist()
                
                selected_b_models = st.multiselect(
                    "é€‰æ‹©å¯¹æ¯”æ¨¡å‹ (é»˜è®¤å‰10):", 
                    bench_sorted['Model'].tolist(), 
                    default=top_10_models,
                    key="tab1_models"
                )
                
                if selected_b_models:
                    plot_df = bench_sorted[bench_sorted['Model'].isin(selected_b_models)]
                    
                    chart_vertical = alt.Chart(plot_df).mark_bar(
                        cornerRadiusTopLeft=3, cornerRadiusTopRight=3
                    ).encode(
                        x=alt.X('Model:N', sort='-y', title='æ¨¡å‹åç§°', axis=alt.Axis(labelAngle=-45, labelOverlap=False)),
                        y=alt.Y(f'{primary_metric}:Q', title='å¾—åˆ†æ•°å€¼'),
                        color=alt.Color('Model:N', legend=None, scale=alt.Scale(scheme='tableau20')),
                        tooltip=['Model', alt.Tooltip(f'{primary_metric}:Q', format='.3f')]
                    ).properties(height=500)
                    
                    st.altair_chart(chart_vertical, use_container_width=True)
                else:
                    st.info("è¯·è‡³å°‘é€‰æ‹©ä¸€ä¸ªæ¨¡å‹è¿›è¡Œå¯¹æ¯”ç»˜åˆ¶ã€‚")
                    
        with tab2:
            st.markdown("### å¤šç»´åº¦æŒ‡æ ‡äº¤å‰å¯¹æ¯”")
            st.info(f"æ•°æ®æ›´æ–°äº: **{latest_bench_date.strftime('%Y-%m-%d')}**")
            col_t1, col_t2 = st.columns([1, 2])
            with col_t1:
                t2_metric = st.selectbox("æ’åºæŒ‡æ ‡ä¼˜å…ˆæƒ:", metrics_available, index=0, key="tab2_main_metric")
            with col_t2:
                t2_metrics = st.multiselect("éœ€è¦ä¸€å¹¶åˆ—å‡ºçš„å…¶ä»–æŒ‡æ ‡:", metrics_available, default=metrics_available[:4] if len(metrics_available) >= 4 else metrics_available, key="tab2_metrics")
            
            t2_sorted = bench_pivot.sort_values(by=t2_metric, ascending=False).reset_index()
            t2_models_selected = st.multiselect(
                "éœ€è¦æ”¾å…¥è¡¨æ ¼å¯¹æ¯”çš„æ¨¡å‹ (ç•™ç©ºä»£è¡¨æ˜¾ç¤ºæ‰€æœ‰):",
                t2_sorted['Model'].tolist(),
                default=[]
            )
            
            display_cols = [t2_metric] + [m for m in t2_metrics if m != t2_metric]
            
            if t2_models_selected:
                display_df = bench_pivot.loc[t2_models_selected, display_cols].sort_values(by=t2_metric, ascending=False)
            else:
                display_df = bench_pivot.loc[:, display_cols].sort_values(by=t2_metric, ascending=False)
                
            st.dataframe(display_df.style.format("{:.3f}", na_rep='-'), use_container_width=True)
    
    # --- Tab 3: Arena ç«æŠ€æ’å ---
    with tab3:
        st.markdown("### Arena æ’è¡Œæ¦œ")
        st.caption("æ•°æ®æº: arena.ai Â· ç”±çœŸäººç›²æµ‹å¯¹æˆ˜çš„ ELO åˆ†æ•°")
        
        if df_lmarena is None or df_lmarena.empty:
            st.warning("æš‚æœªå‘ç° Arena æ’è¡Œæ¦œæ•°æ®ã€‚")
        else:
            latest_lm_date = df_lmarena['Date'].max()
            st.info(f"æ•°æ®æ›´æ–°äº: **{latest_lm_date.strftime('%Y-%m-%d')}**")
            
            df_latest_lm = df_lmarena[df_lmarena['Date'] == latest_lm_date].copy()
            
            # ä¸¤å¤§ç±»æ•°æ®ï¼šELO æ’è¡Œæ¦œ + Arena Overview
            arena_sub1, arena_sub2 = st.tabs(["ğŸ† ELO æ’è¡Œæ¦œ", "ğŸ“Š Arena Overview"])
            
            # ---- ELO æ’è¡Œæ¦œå­æ ‡ç­¾ ----
            with arena_sub1:
                ELO_LABELS = {
                    'Score_text': 'æ–‡æœ¬ (Text)',
                    'Score_code': 'ä»£ç  (Code)',
                    'Score_vision': 'è§†è§‰ (Vision)',
                    'Score_text_to_image': 'æ–‡ç”Ÿå›¾ (Text-to-Image)',
                    'Score_image_edit': 'å›¾åƒç¼–è¾‘ (Image Edit)',
                    'Score_search': 'æœç´¢ (Search)',
                    'Score_text_to_video': 'æ–‡ç”Ÿè§†é¢‘ (Text-to-Video)',
                    'Score_image_to_video': 'å›¾ç”Ÿè§†é¢‘ (Image-to-Video)',
                }
                
                elo_options = {}
                for col_key, label in ELO_LABELS.items():
                    if col_key in df_latest_lm.columns and df_latest_lm[col_key].notna().sum() > 0:
                        elo_options[label] = col_key
                
                if not elo_options:
                    st.info("æš‚æ—  ELO æ’è¡Œæ•°æ®ã€‚")
                else:
                    selected_elo_label = st.selectbox("é€‰æ‹©æ’è¡Œæ¦œ:", list(elo_options.keys()), index=0, key="arena_elo_cat")
                    selected_elo_col = elo_options[selected_elo_label]
                    
                    ranked_df = df_latest_lm.dropna(subset=[selected_elo_col]).copy()
                    ranked_df = ranked_df.sort_values(selected_elo_col, ascending=False).reset_index(drop=True)
                    
                    # Votes åˆ—
                    votes_col = selected_elo_col.replace('Score_', 'Votes_')
                    
                    if not ranked_df.empty:
                        top_n = min(25, len(ranked_df))
                        top_df = ranked_df.head(top_n).copy()
                        top_df['ELO'] = top_df[selected_elo_col].astype(int)
                        
                        tooltip_fields = ['Model', alt.Tooltip('ELO:Q', title='ELO åˆ†æ•°')]
                        if votes_col in top_df.columns:
                            top_df['Votes'] = top_df[votes_col].fillna(0).astype(int)
                            tooltip_fields.append(alt.Tooltip('Votes:Q', title='æŠ•ç¥¨æ•°', format=','))
                        
                        chart_elo = alt.Chart(top_df).mark_bar(
                            cornerRadiusTopRight=3, cornerRadiusBottomRight=3
                        ).encode(
                            y=alt.Y('Model:N', 
                                    sort=alt.EncodingSortField(field='ELO', order='descending'),
                                    title=None, 
                                    axis=alt.Axis(labelOverlap=False)),
                            x=alt.X('ELO:Q', title='ELO åˆ†æ•°', scale=alt.Scale(zero=False)),
                            color=alt.value('#4C78A8'),
                            tooltip=tooltip_fields
                        ).properties(height=max(300, top_n * 28))
                        st.altair_chart(chart_elo, use_container_width=True)
                        
                        # è¡¨æ ¼
                        disp_cols = ['Model', selected_elo_col]
                        disp_names = {'Model': 'æ¨¡å‹', selected_elo_col: 'ELO åˆ†æ•°'}
                        if votes_col in ranked_df.columns:
                            disp_cols.append(votes_col)
                            disp_names[votes_col] = 'æŠ•ç¥¨æ•°'
                        display_lm = ranked_df[disp_cols].copy()
                        display_lm.rename(columns=disp_names, inplace=True)
                        st.dataframe(display_lm, use_container_width=True, hide_index=True, height=400)
                    else:
                        st.info("è¯¥æ’è¡Œæ¦œæš‚æ— æ•°æ®ã€‚")
            
            # ---- Arena Overview å­æ ‡ç­¾ ----
            with arena_sub2:
                RANK_LABELS = {
                    'Rank_Overall': 'ç»¼åˆ (Overall)',
                    'Rank_Expert': 'ä¸“å®¶ (Expert)',
                    'Rank_Hard_Prompts': 'å›°éš¾æç¤ºè¯',
                    'Rank_Coding': 'ä»£ç ',
                    'Rank_Math': 'æ•°å­¦',
                    'Rank_Creative_Writing': 'åˆ›æ„å†™ä½œ',
                    'Rank_Instruction_Following': 'æŒ‡ä»¤éµå¾ª',
                    'Rank_Longer_Query': 'é•¿æŸ¥è¯¢',
                }
                
                rank_options = {}
                for col_key, label in RANK_LABELS.items():
                    if col_key in df_latest_lm.columns and df_latest_lm[col_key].notna().sum() > 0:
                        rank_options[label] = col_key
                
                if not rank_options:
                    st.info("æš‚æ—  Arena Overview æ•°æ®ã€‚")
                else:
                    selected_rank_label = st.selectbox("æ’åºç»´åº¦:", list(rank_options.keys()), index=0, key="arena_ov_cat")
                    selected_rank_col = rank_options[selected_rank_label]
                    
                    ov_df = df_latest_lm.dropna(subset=[selected_rank_col]).copy()
                    ov_df = ov_df.sort_values(selected_rank_col, ascending=True).reset_index(drop=True)
                    
                    if not ov_df.empty:
                        top_n = min(30, len(ov_df))
                        top_df = ov_df.head(top_n).copy()
                        top_df['æ’å'] = top_df[selected_rank_col].astype(int)
                        
                        chart_ov = alt.Chart(top_df).mark_bar(
                            cornerRadiusTopRight=3, cornerRadiusBottomRight=3
                        ).encode(
                            y=alt.Y('Model:N',
                                    sort=alt.EncodingSortField(field='æ’å', order='ascending'),
                                    title=None,
                                    axis=alt.Axis(labelOverlap=False)),
                            x=alt.X('æ’å:Q', title='æ’å (è¶Šå°è¶Šå¥½)', scale=alt.Scale(reverse=True)),
                            color=alt.value('#E45756'),
                            tooltip=['Model', alt.Tooltip('æ’å:Q', title='æ’å')]
                        ).properties(height=max(300, top_n * 25))
                        st.altair_chart(chart_ov, use_container_width=True)
                        
                        # å¤šç»´åº¦æ’åè¡¨
                        st.markdown(f"#### Arena Overview å®Œæ•´æ’å (å…± {len(ov_df)} ä¸ªæ¨¡å‹)")
                        rank_cols_available = [c for c in RANK_LABELS.keys() if c in ov_df.columns]
                        display_ov = ov_df[['Model'] + rank_cols_available].copy()
                        rename_map = {'Model': 'æ¨¡å‹'}
                        rename_map.update({k: RANK_LABELS[k] for k in rank_cols_available})
                        display_ov.rename(columns=rename_map, inplace=True)
                        st.dataframe(display_ov, use_container_width=True, hide_index=True, height=500)
                    else:
                        st.info("è¯¥ç»´åº¦æš‚æ— æ•°æ®ã€‚")
    
    st.markdown("---")
    col_dl1, col_dl2 = st.columns(2)
    if df_bench is not None:
        with col_dl1:
            data, name, mime, label = get_dataset_download(df_bench, "openrouter_benchmark_full")
            st.download_button(label="ä¸‹è½½ AA Benchmark æ•°æ®", data=data, file_name=name, mime=mime)
    if df_lmarena is not None:
        with col_dl2:
            data, name, mime, label = get_dataset_download(df_lmarena, "lmarena_leaderboard_full")
            st.download_button(label="ä¸‹è½½ LMARENA æ•°æ®", data=data, file_name=name, mime=mime)

# ========================================================
# é¡µé¢ 8: å•æ¨¡å‹æ·±åº¦æ¢ç´¢
# ========================================================
elif page == NAV_SINGLE_MODEL:
    st.subheader("å•æ¨¡å‹åˆ†æ")
    st.caption("ç»¼åˆç”¨é‡ã€åŸºå‡†æµ‹è¯•å’Œå®šä»·æ•°æ®ï¼Œè¿½è¸ªå•ä¸€æ¨¡å‹ã€‚")

    # è·å–åŒ…å«è¿‡å»ç°åœ¨æ‰€æœ‰è®°å½•ä¸‹æ¥çš„åå­—é›†åˆï¼Œç»Ÿä¸€æ¶ˆé™¤é‡åå¹²æ‰°é¡¹
    raw_models = set(all_model_names) | set(all_pricing_models) | set(all_benchmark_models)
    normalized_map = {}
    for rm in raw_models:
        norm = normalize_model_name(rm)
        if norm not in normalized_map:
            normalized_map[norm] = []
        normalized_map[norm].append(rm)
        
    all_possible_models = sorted(list(normalized_map.keys()))
    
    if not all_possible_models:
        st.warning("æš‚æœªå‘ç°ä»»ä½•æ¨¡å‹æ•°æ®ã€‚")
    else:
        selected_model_norm = st.selectbox("é€‰æ‹©æ¨¡å‹:", all_possible_models)
        st.markdown("---")
        
        real_names = normalized_map[selected_model_norm]
        
        # 1. ç´¯è®¡ç”¨é‡è¶‹åŠ¿
        st.markdown("### ç´¯è®¡ç”¨é‡è¶‹åŠ¿")
        if df is not None and not df.empty:
            m_df = df[df['Model'].isin(real_names) | df['Display_Name'].isin(real_names)].sort_values('Date').copy()
                
            if not m_df.empty:
                m_df = m_df.groupby('Date', as_index=False)['Total_Tokens'].sum()
                m_df['Cumulative_Tokens'] = m_df['Total_Tokens'].cumsum()
                
                col_m1, col_m2 = st.columns(2)
                recent_7d = m_df.tail(7)['Total_Tokens'].sum()
                col_m1.metric("ç´¯è®¡æ¶ˆè€—", f"{m_df['Cumulative_Tokens'].iloc[-1]:.4f} Billion")
                col_m2.metric("è¿‘ 7 å¤©æ¶ˆè€—", f"{recent_7d:.4f} Billion")
                    
                chart_cum = alt.Chart(m_df).mark_area(
                    opacity=0.6, 
                    color=alt.Gradient(
                        gradient='linear',
                        stops=[alt.GradientStop(color='orange', offset=0), alt.GradientStop(color='white', offset=1)],
                        x1=1, x2=1, y1=1, y2=0
                    )
                ).encode(
                    x=alt.X('Date:T', title='æ—¥æœŸ'),
                    y=alt.Y('Cumulative_Tokens:Q', title='ç´¯è®¡ Tokens (Billion)'),
                    tooltip=['Date', 'Cumulative_Tokens', 'Total_Tokens']
                ).properties(height=350)
                st.altair_chart(chart_cum, use_container_width=True)
            else:
                st.info("è¯¥æ¨¡å‹æš‚æ—  Token æ¶ˆè€—è®°å½•ã€‚")
        else:
            st.info("æœªè¿æ¥åˆ° Token æ•°æ®æºã€‚")

        st.markdown("---")
        
        # 2. åŸºå‡†æµ‹è¯•è·‘åˆ†
        st.markdown(f"### {selected_model_norm} åŸºå‡†æµ‹è¯•è·‘åˆ†")
        st.caption("åŒä¸€æ¨¡å‹å¯èƒ½æœ‰ Reasoning / Non-Reasoning ç­‰å˜ä½“ã€‚")
        if df_bench is not None and not df_bench.empty:
            latest_bench_date = df_bench['Date'].max()
            df_latest_bench = df_bench[(df_bench['Date'] == latest_bench_date) & (df_bench['Metric'].notna())].copy()
            
            bench_model_cols = [col for col in df_latest_bench.columns if col not in ['Date', 'Metric']]
            matched_b_cols = fuzzy_match_model(selected_model_norm, bench_model_cols, threshold=0.55)
            
            if matched_b_cols:
                tabs_b = st.tabs(matched_b_cols)
                
                for i, m_col in enumerate(matched_b_cols):
                    with tabs_b[i]:
                        model_scores = df_latest_bench[['Metric', m_col]].dropna()
                        if not model_scores.empty:
                            rank_data = []
                            for _, row in model_scores.iterrows():
                                metric = row['Metric']
                                score = row[m_col]
                                
                                all_scores_flat = df_latest_bench[df_latest_bench['Metric'] == metric].drop(columns=['Date', 'Metric']).iloc[0].dropna()
                                all_scores_num = pd.to_numeric(all_scores_flat, errors='coerce').dropna()
                                
                                if score in all_scores_num.values:
                                    rank = all_scores_num.rank(method='min', ascending=False)[m_col]
                                    total = len(all_scores_num)
                                    percentile = (total - rank) / total * 100
                                    
                                    rank_data.append({
                                        'æŒ‡æ ‡': metric,
                                        'å¾—åˆ†': f"{score:.3f}",
                                        'æ’å': f"ç¬¬ {int(rank)} / å…± {total}",
                                        'åˆ†ä½æ•°': f"è¶…è¶Š {percentile:.1f}%"
                                    })
                            
                            if rank_data:
                                st.dataframe(pd.DataFrame(rank_data), use_container_width=True, hide_index=True)
                            else:
                                st.info("æš‚æ— å¯ç”¨æµ‹è¯•æ•°æ®ã€‚")
                        else:
                            st.info("æš‚æ— æ•°æ®ã€‚")
            else:
                st.info("è¯¥æ¨¡å‹æœªè¢«æ”¶å½•äº Benchmark æ•°æ®ä¸­ã€‚")
        else:
            st.info("æœªè¿æ¥åˆ°è·‘åˆ†æ•°æ®æºã€‚")

        st.markdown("---")
        
        # 3. å®šä»·åˆ†æï¼ˆåŒå›¾ï¼šæœ‰æ•ˆä»·æ ¼è¶‹åŠ¿ + å„ä¾›åº”å•†ä»·æ ¼æŸ±çŠ¶å›¾ï¼‰
        st.markdown("### å®šä»·åˆ†æ")
        if df_price is not None and not df_price.empty:
            m_price_df = df_price[df_price['Model'].isin(real_names)].copy()
            if not m_price_df.empty:
                latest_pricing_date = m_price_df['Date'].max()
                df_latest_prices = m_price_df[m_price_df['Date'] == latest_pricing_date]
                
                wa_row = df_latest_prices[df_latest_prices['Provider'] == 'Weighted Average']
                if not wa_row.empty:
                    wa = wa_row.iloc[0]
                    st.success(f"æœ‰æ•ˆå‡ä»·: Input **${wa['Input_Price_1M']:.4f}**/1M Â· Output **${wa['Output_Price_1M']:.4f}**/1M")
                
                # å›¾1: æœ‰æ•ˆä»·æ ¼è¶‹åŠ¿
                st.markdown("#### æœ‰æ•ˆä»·æ ¼è¶‹åŠ¿")
                wa_hist = m_price_df[m_price_df['Provider'] == 'Weighted Average'].copy()
                if not wa_hist.empty:
                    wa_hist['Date'] = pd.to_datetime(wa_hist['Date'])
                    wa_long = wa_hist.melt(
                        id_vars=['Date'],
                        value_vars=['Input_Price_1M', 'Output_Price_1M'],
                        var_name='Type', value_name='Price'
                    ).dropna(subset=['Price'])
                    wa_long['Type'] = wa_long['Type'].map({'Input_Price_1M': 'Input', 'Output_Price_1M': 'Output'})
                    
                    chart_wa = alt.Chart(wa_long).mark_line(point=True).encode(
                        x=alt.X('Date:T', title='æ—¶é—´', axis=alt.Axis(format='%m/%d')),
                        y=alt.Y('Price:Q', title='ä»·æ ¼ ($/1M Tokens)'),
                        color=alt.Color('Type:N', title='ç±»å‹'),
                        tooltip=['Date:T', 'Type', alt.Tooltip('Price:Q', format='$.4f')]
                    ).properties(height=250)
                    st.altair_chart(chart_wa, use_container_width=True)
                
                # å›¾2: å„ä¾›åº”å•† Input ä»·æ ¼è¶‹åŠ¿
                st.markdown("#### å„ä¾›åº”å•† Input ä»·æ ¼è¶‹åŠ¿")
                provider_history = m_price_df[m_price_df['Provider'] != 'Weighted Average'].copy()
                if not provider_history.empty:
                    provider_history['Date'] = pd.to_datetime(provider_history['Date'])
                    
                    chart_input = alt.Chart(provider_history).mark_line(point=True).encode(
                        x=alt.X('Date:T', title='æ—¶é—´', axis=alt.Axis(format='%m/%d')),
                        y=alt.Y('Input_Price_1M:Q', title='Input ä»·æ ¼ ($/1M Tokens)'),
                        color=alt.Color('Provider:N', title='ä¾›åº”å•†', scale=alt.Scale(scheme='tableau20')),
                        tooltip=['Date:T', 'Provider', alt.Tooltip('Input_Price_1M:Q', format='$.4f')]
                    ).properties(height=250)
                    st.altair_chart(chart_input, use_container_width=True)
                else:
                    st.info("æš‚æ— ä¾›åº”å•† Input ä»·æ ¼è¶‹åŠ¿æ•°æ®ã€‚")
                
                # å›¾3: å„ä¾›åº”å•† Output ä»·æ ¼è¶‹åŠ¿
                st.markdown("#### å„ä¾›åº”å•† Output ä»·æ ¼è¶‹åŠ¿")
                if not provider_history.empty:
                    chart_output = alt.Chart(provider_history).mark_line(point=True).encode(
                        x=alt.X('Date:T', title='æ—¶é—´', axis=alt.Axis(format='%m/%d')),
                        y=alt.Y('Output_Price_1M:Q', title='Output ä»·æ ¼ ($/1M Tokens)'),
                        color=alt.Color('Provider:N', title='ä¾›åº”å•†', scale=alt.Scale(scheme='tableau20')),
                        tooltip=['Date:T', 'Provider', alt.Tooltip('Output_Price_1M:Q', format='$.4f')]
                    ).properties(height=250)
                    st.altair_chart(chart_output, use_container_width=True)
                else:
                    st.info("æš‚æ— ä¾›åº”å•† Output ä»·æ ¼è¶‹åŠ¿æ•°æ®ã€‚")
                
                # ä¾›åº”å•†è¯¦æƒ…è¡¨æ ¼
                st.markdown("#### ä¾›åº”å•†è¯¦æƒ… (æœ€æ–°)")
                provider_prices = df_latest_prices[df_latest_prices['Provider'] != 'Weighted Average'].sort_values('Input_Price_1M')
                if not provider_prices.empty:
                    st.dataframe(
                        provider_prices[['Provider', 'Input_Price_1M', 'Output_Price_1M', 'Cache_Hit_Rate']].style.format({
                            'Input_Price_1M': '${:.4f}',
                            'Output_Price_1M': '${:.4f}',
                            'Cache_Hit_Rate': '{:.1%}'
                        }),
                        use_container_width=True,
                        hide_index=True
                    )
            else:
                st.info("æš‚æ— è¯¥æ¨¡å‹çš„å®šä»·æ•°æ®ã€‚")
        else:
            st.info("æœªè¿æ¥åˆ°å®šä»·æ•°æ®æºã€‚")

        st.markdown("---")

        # 4. Arena (LMARENA) æ’å
        st.markdown(f"### {selected_model_norm} çš„ Arena æ’å")
        if df_lmarena is not None and not df_lmarena.empty:
            latest_lm_date = df_lmarena['Date'].max()
            df_latest_lm = df_lmarena[df_lmarena['Date'] == latest_lm_date]
            
            lm_all_models = df_latest_lm['Model'].unique().tolist()
            matched_lm = fuzzy_match_model(selected_model_norm, lm_all_models, threshold=0.5)
            
            if matched_lm:
                lm_rows = df_latest_lm[df_latest_lm['Model'].isin(matched_lm)].copy()
                
                score_cols = [c for c in lm_rows.columns if c.startswith('Score_')]
                rank_cols = [c for c in lm_rows.columns if c.startswith('Rank_')]
                SCORE_LABELS = {
                    'Score_text': 'æ–‡æœ¬', 'Score_code': 'ä»£ç ', 'Score_vision': 'è§†è§‰',
                    'Score_text_to_image': 'æ–‡ç”Ÿå›¾', 'Score_image_edit': 'å›¾åƒç¼–è¾‘', 
                    'Score_search': 'æœç´¢', 'Score_text_to_video': 'æ–‡ç”Ÿè§†é¢‘', 
                    'Score_image_to_video': 'å›¾ç”Ÿè§†é¢‘',
                }
                RANK_LABELS = {
                    'Rank_Overall': 'ç»¼åˆ', 'Rank_Expert': 'ä¸“å®¶', 'Rank_Hard_Prompts': 'å›°éš¾æç¤ºè¯',
                    'Rank_Coding': 'ä»£ç ', 'Rank_Math': 'æ•°å­¦', 'Rank_Creative_Writing': 'åˆ›æ„å†™ä½œ',
                    'Rank_Instruction_Following': 'æŒ‡ä»¤éµå¾ª', 'Rank_Longer_Query': 'é•¿æŸ¥è¯¢',
                }
                
                rank_display = []
                for _, row in lm_rows.iterrows():
                    entry = {'æ¨¡å‹': row['Model']}
                    for rc in rank_cols:
                        label = RANK_LABELS.get(rc, rc)
                        if pd.notna(row.get(rc)):
                            entry[f'{label}æ’å'] = int(row[rc])
                    for sc in score_cols:
                        label = SCORE_LABELS.get(sc, sc)
                        if pd.notna(row.get(sc)):
                            entry[f'{label} ELO'] = int(row[sc])
                    rank_display.append(entry)
                
                if rank_display:
                    st.dataframe(pd.DataFrame(rank_display), use_container_width=True, hide_index=True)
                else:
                    st.info("æœªæ‰¾åˆ°è¯¥æ¨¡å‹çš„æ’åæ•°æ®ã€‚")
            else:
                st.info("è¯¥æ¨¡å‹æš‚æœªè¢« Arena æ”¶å½•ã€‚")
        else:
            st.info("æœªè¿æ¥åˆ° Arena æ•°æ®æºã€‚")
