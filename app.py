import streamlit as st
import pandas as pd
import altair as alt
import os
import io
import json

# === 1. åŸºç¡€é…ç½® ===
st.set_page_config(page_title="LLM æ•°æ®çœ‹æ¿", layout="wide")
DATA_FILE = "history_database.csv"
PRICING_FILE = "openrouter_pricing_provider_records.csv"
BENCHMARK_FILE = "openrouter_benchmark_records.csv"
LMARENA_FILE = "lmarena_leaderboard_records.csv"

# é¡µé¢æ ‡é¢˜
st.title("LLM æ•°æ®çœ‹æ¿")

# å®šä¹‰é¡µé¢åç§°å¸¸é‡
NAV_AI_QUERY = "AI æŸ¥è¯¢"
NAV_DAILY_BRIEF = "æ¯æ—¥ç®€æŠ¥"
NAV_TN_DAILY = "T+N æ—¥ç”¨é‡å¯¹æ¯”"
NAV_CUMULATIVE_COMPARE = "ç´¯è®¡ç”¨é‡å¯¹æ¯”"
NAV_DETAIL_DAILY = "å•æ¨¡å‹ç”¨é‡"
NAV_RAW_DATA = "æ•°æ®å¯¼å‡º"
NAV_PRICING = "ä¾›åº”å•†å®šä»·"
NAV_BENCHMARK = "åŸºå‡†æµ‹è¯•"
NAV_SINGLE_MODEL = "å•æ¨¡å‹æ·±åº¦åˆ†æ"

# === 2. å·¥å…·å‡½æ•° ===

def is_reasoning_model(model_name: str) -> bool:
    """åŸºäºæ¨¡å‹å‘½åè§„åˆ™è¿›è¡Œç²—ç•¥åˆ¤æ–­æ˜¯å¦ä¸ºæ·±åº¦æ¨ç†æ¨¡å‹"""
    if not isinstance(model_name, str): return False
    name_lower = model_name.lower()
    reasoning_keywords = ['reasoning', 'o1', 'o3', 'r1', 'qwq']
    for kw in reasoning_keywords:
        if kw in name_lower:
            return True
    return False

import re as _re_global

def _tokenize_model_name(name: str) -> set:
    """å°†æ¨¡å‹åæ‹†ä¸º token é›†åˆï¼Œç”¨äºæ¨¡ç³ŠåŒ¹é…"""
    n = name.lower()
    # å»æ‰å‚å•†å‰ç¼€
    if '/' in n:
        n = n.split('/')[-1]
    # å»æ‰æ‹¬å·å†…ä¿®é¥°è¯ï¼Œå¦‚ (Reasoning), (Oct '24), (Non-reasoning)
    n = _re_global.sub(r'\s*\(.*?\)', '', n)
    # æŒ‰ ç©ºæ ¼ã€æ¨ªçº¿ã€ä¸‹åˆ’çº¿ åˆ†å‰²
    tokens = set(_re_global.split(r'[\s\-_]+', n.strip()))
    tokens.discard('')
    return tokens

def _jaccard_similarity(set_a: set, set_b: set) -> float:
    """è®¡ç®—ä¸¤ä¸ªé›†åˆçš„ Jaccard ç›¸ä¼¼åº¦"""
    if not set_a or not set_b:
        return 0.0
    intersection = set_a & set_b
    union = set_a | set_b
    return len(intersection) / len(union)

def normalize_model_name(name: str) -> str:
    """ç»Ÿä¸€æ¶ˆé™¤å‚å•†å‰ç¼€å’Œæ— ç”¨çš„å¤§å°å†™ï¼Œä½¿ä¸åŒæ•°æ®æºä¸­çš„åŒæ¬¾æ¨¡å‹èƒ½åˆå¹¶"""
    if not isinstance(name, str): return str(name)
    n = name.lower()
    # ç§»é™¤è¯¸å¦‚ 'anthropic/', 'google/' ç­‰å‰ç¼€
    if '/' in n:
        n = n.split('/')[-1]
    
    # ä»…ä¿ç•™æç«¯ç‰¹ä¾‹çš„ç¡¬æ˜ å°„ï¼ˆå®Œå…¨ä¸åŒå‘½åçš„æƒ…å†µï¼‰
    mapping = {
        'deepseek v3': 'deepseek-chat',
        'deepseek-v3': 'deepseek-chat',
    }
    
    for key, val in mapping.items():
        if key in n:
            return val
            
    # å»é™¤å¤šä½™æ‹¬å·å¦‚ (Reasoning) ç­‰å¹²æ‰°è¯ï¼Œä¿ç•™æ ¸å¿ƒ slug
    n = _re_global.sub(r'\s*\(.*?\)', '', n).strip()
    n = n.replace(' ', '-')
    return n

def fuzzy_match_model(target_norm: str, candidate_names: list, threshold: float = 0.55) -> list:
    """åœ¨å€™é€‰æ¨¡å‹ååˆ—è¡¨ä¸­ï¼Œç”¨ Token åŒ– Jaccard åŒ¹é…æ‰¾å‡ºä¸ target_norm ç›¸ä¼¼çš„åå­—"""
    target_tokens = _tokenize_model_name(target_norm)
    matched = []
    for cand in candidate_names:
        cand_tokens = _tokenize_model_name(cand)
        sim = _jaccard_similarity(target_tokens, cand_tokens)
        if sim >= threshold:
            matched.append(cand)
    return matched

@st.cache_data(ttl=600)
def load_data():
    if not os.path.exists(DATA_FILE):
        return None, f"âŒ æ‰¾ä¸åˆ°æ–‡ä»¶ `{DATA_FILE}`ï¼Œè¯·ç­‰å¾…çˆ¬è™«è¿è¡Œã€‚"
    try:
        # Load Token Data
        df = pd.read_csv(DATA_FILE)
        if df.empty: return None, "CSV æ–‡ä»¶ä¸ºç©º"
        df['Date'] = pd.to_datetime(df['Date'])
        
        # åç§°æ¸…æ´—ï¼šå»æ‰ '/' å‰é¢çš„å‚å•†å
        df['Display_Name'] = df['Model'].apply(lambda x: x.split('/')[-1] if '/' in x else x)
        
        return df, None
    except Exception as e:
        return None, str(e)

@st.cache_data(ttl=600)
def load_pricing_data():
    if not os.path.exists(PRICING_FILE):
        return None
    try:
        df_price = pd.read_csv(PRICING_FILE)
        df_price['Date'] = pd.to_datetime(df_price['Date'])
        return df_price
    except Exception:
        return None

@st.cache_data(ttl=600)
def load_benchmark_data():
    if not os.path.exists(BENCHMARK_FILE):
        return None
    try:
        df_bench = pd.read_csv(BENCHMARK_FILE)
        df_bench['Date'] = pd.to_datetime(df_bench['Date'])
        return df_bench
    except Exception:
        return None

@st.cache_data(ttl=600)
def load_lmarena_data():
    if not os.path.exists(LMARENA_FILE): return None
    try:
        df = pd.read_csv(LMARENA_FILE)
        df['Date'] = pd.to_datetime(df['Date'])
        return df
    except Exception:
        return None

# Excel/CSV æ™ºèƒ½å¯¼å‡ºå‡½æ•°
def get_dataset_download(df, filename_prefix):
    try:
        output = io.BytesIO()
        with pd.ExcelWriter(output, engine='openpyxl') as writer:
            df.to_excel(writer, index=False, sheet_name='Sheet1')
        data = output.getvalue()
        file_name = f"{filename_prefix}.xlsx"
        mime = "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
        label = "ğŸ“¥ ä¸‹è½½ Excel æ–‡ä»¶ (.xlsx)"
    except ImportError:
        data = df.to_csv(index=False).encode('utf-8-sig')
        file_name = f"{filename_prefix}.csv"
        mime = "text/csv"
        label = "ğŸ“¥ ä¸‹è½½ CSV æ–‡ä»¶ (Excelå…¼å®¹)"
    
    return data, file_name, mime, label

df, error = load_data()
df_price = load_pricing_data()
df_bench = load_benchmark_data()
df_lmarena = load_lmarena_data()

if error and not (df_price is not None or df_bench is not None):
    st.error(error)
    st.stop()

# === 3. ä¾§è¾¹æ å¯¼èˆª ===
st.sidebar.title("å¯¼èˆª")
page = st.sidebar.radio("é€‰æ‹©åˆ†æè§†å›¾", [
    NAV_AI_QUERY,
    NAV_DAILY_BRIEF,
    NAV_SINGLE_MODEL,
    NAV_TN_DAILY,
    NAV_CUMULATIVE_COMPARE,
    NAV_DETAIL_DAILY,
    NAV_PRICING,
    NAV_BENCHMARK,
    NAV_RAW_DATA
])

all_model_names = df['Display_Name'].unique() if df is not None else []
all_pricing_models = df_price['Model'].unique() if df_price is not None else []
all_benchmark_models = [c for c in df_bench.columns if c not in ['Date', 'Metric']] if df_bench is not None else []

# æ•°æ®æ¦‚è§ˆé¢æ¿
st.sidebar.divider()
st.sidebar.markdown("#### æ•°æ®æ¦‚è§ˆ")
if df is not None:
    st.sidebar.metric("è¿½è¸ªæ¨¡å‹æ•°", len(all_model_names))
    st.sidebar.caption(f"æ•°æ®åŒºé—´: {df['Date'].min().strftime('%Y-%m-%d')} ~ {df['Date'].max().strftime('%Y-%m-%d')}")
if df_price is not None:
    st.sidebar.metric("å®šä»·æ¨¡å‹æ•°", len(all_pricing_models))
    st.sidebar.caption(f"å®šä»·æ›´æ–°è‡³: {df_price['Date'].max().strftime('%Y-%m-%d')}")
if df_bench is not None:
    st.sidebar.metric("è·‘åˆ†æ¨¡å‹æ•°", len(all_benchmark_models))
    st.sidebar.caption(f"è·‘åˆ†æ›´æ–°è‡³: {df_bench['Date'].max().strftime('%Y-%m-%d')}")
if df_lmarena is not None:
    st.sidebar.metric("LMARENA æ¨¡å‹æ•°", df_lmarena['Model'].nunique())
    st.sidebar.caption(f"LMARENA æ›´æ–°è‡³: {df_lmarena['Date'].max().strftime('%Y-%m-%d')}")

# ========================================================
# é¡µé¢ 0: AI æ™ºèƒ½æŸ¥è¯¢
# ========================================================
if page == NAV_AI_QUERY:
    st.subheader("AI æ•°æ®åˆ†æåŠ©æ‰‹")
    
    MODEL_OPTIONS = {
        "DeepSeek V3 (é«˜æ€§ä»·æ¯”)": "deepseek/deepseek-chat",
        "Claude Sonnet 4 (å¼ºæ¨ç†)": "anthropic/claude-sonnet-4",
        "GPT-4.1 (å‡è¡¡)": "openai/gpt-4.1",
        "Gemini 2.5 Flash (å¿«é€Ÿ)": "google/gemini-2.5-flash-preview",
    }
    selected_model_label = st.selectbox("é€‰æ‹© AI æ¨¡å‹:", list(MODEL_OPTIONS.keys()), index=0)
    AI_MODEL = MODEL_OPTIONS[selected_model_label]
    st.caption(f"å½“å‰æ¨¡å‹: `{AI_MODEL}`")
    
    # API Key é…ç½®
    api_key = os.environ.get("OPENROUTER_API_KEY", "") or st.secrets.get("OPENROUTER_API_KEY", "")
    if not api_key:
        api_key = st.text_input("è¯·è¾“å…¥ OpenRouter API Key:", type="password", 
                                help="åœ¨ https://openrouter.ai/keys è·å–ã€‚ä¹Ÿå¯é€šè¿‡ Streamlit Secrets æˆ–ç¯å¢ƒå˜é‡é…ç½®ã€‚")
    
    if not api_key:
        st.warning("è¯·å…ˆé…ç½® OpenRouter API Keyã€‚")
    else:
        # æ„å»ºæ•°æ®åº“ä¸Šä¸‹æ–‡æ‘˜è¦
        @st.cache_data(ttl=600)
        def build_db_context(_df, _df_price, _df_bench, _df_lmarena):
            context_parts = []
            
            if _df is not None and not _df.empty:
                # æä¾›æ‰€æœ‰æ¨¡å‹ååˆ—è¡¨å¸®åŠ© AI åšæ¨¡ç³ŠåŒ¹é…
                all_models = _df['Model'].unique().tolist()
                display_names = _df['Display_Name'].unique().tolist() if 'Display_Name' in _df.columns else []
                context_parts.append(f"""### Token æ¶ˆè€—æ•°æ® (å˜é‡å: df)
- åˆ—: Date, Model, Prompt, Completion, Reasoning, Total_Tokens, Display_Name
- è®°å½•æ•°: {len(_df)}, æ—¥æœŸèŒƒå›´: {_df['Date'].min().strftime('%Y-%m-%d')} ~ {_df['Date'].max().strftime('%Y-%m-%d')}
- Token å•ä½: Billion (10äº¿)
- å…¨éƒ¨æ¨¡å‹åˆ—è¡¨(Modelåˆ—): {', '.join(all_models[:30])}
- æ˜¾ç¤ºååˆ—è¡¨(Display_Nameåˆ—): {', '.join(display_names[:30])}""")

            if _df_price is not None and not _df_price.empty:
                price_models = _df_price['Model'].unique().tolist()
                context_parts.append(f"""### å®šä»·æ•°æ® (å˜é‡å: df_price)
- åˆ—: Date, Model, Provider, Input_Price_1M, Output_Price_1M, Cache_Hit_Rate
- è®°å½•æ•°: {len(_df_price)}, æ—¥æœŸæ•°: {_df_price['Date'].dt.strftime('%Y-%m-%d').nunique()}
- ä»·æ ¼å•ä½: $/1M Tokens
- æ¨¡å‹åˆ—è¡¨(å‰30): {', '.join(price_models[:30])}""")

            if _df_bench is not None and not _df_bench.empty:
                context_parts.append(f"""### Benchmark è·‘åˆ† (å˜é‡å: df_bench)
- ç»“æ„: å®½è¡¨ï¼Œæ¯è¡Œæ˜¯ä¸€ä¸ª Metricï¼Œæ¯åˆ—æ˜¯ä¸€ä¸ªæ¨¡å‹å
- Metric: {', '.join(_df_bench['Metric'].unique()[:8])}
- æ¨¡å‹æ•°: {len([c for c in _df_bench.columns if c not in ['Date','Metric']])}""")

            if _df_lmarena is not None and not _df_lmarena.empty:
                score_cols = [c for c in _df_lmarena.columns if c.startswith('Score_')]
                context_parts.append(f"""### LMARENA ç«æŠ€æ’è¡Œ (å˜é‡å: df_lmarena)
- åˆ—: Date, Model, Overall_Rank, {', '.join(score_cols)}
- ç»´åº¦å« ELO åˆ†æ•°: {', '.join(c.replace('Score_','') for c in score_cols)}
- æ¨¡å‹æ•°: {_df_lmarena['Model'].nunique()}""")
            
            return '\n\n'.join(context_parts)
        
        db_context = build_db_context(df, df_price, df_bench, df_lmarena)
        
        SYSTEM_PROMPT = f"""ä½ æ˜¯ OpenRouter æ•°æ®åˆ†æå¸ˆã€‚ç”¨æˆ·ç”¨è‡ªç„¶è¯­è¨€æé—®ï¼Œä½ åŸºäºæ•°æ®åº“å›ç­”ã€‚

## æ•°æ®åº“

{db_context}

## é‡è¦è§„åˆ™

1. ç”¨ä¸­æ–‡å›ç­”ï¼Œç»“è®ºè¦æœ‰æ•°æ®æ”¯æ’‘
2. ç”¨æˆ·æåˆ°çš„æ¨¡å‹åå¯èƒ½ä¸ç²¾ç¡®ï¼ˆå¦‚ "deepseek" å¯èƒ½æŒ‡ "deepseek/deepseek-chat"ï¼‰ï¼Œä½ éœ€è¦è‡ªåŠ¨æ¨¡ç³ŠåŒ¹é…ã€‚åŒ¹é…ç­–ç•¥ï¼šç”¨ str.contains() åšå­ä¸²åŒ¹é…ï¼Œä¸è¦è¦æ±‚ç²¾ç¡®ç›¸ç­‰
3. å¦‚æœéœ€è¦å¯è§†åŒ–ï¼Œç”Ÿæˆä¸€ä¸ª Python ä»£ç å—(```python```)ï¼Œä»£ç è§„åˆ™:
   - å˜é‡å·²é¢„åŠ è½½: df, df_price, df_bench, df_lmarena, st, alt, pd
   - ç”¨ st.altair_chart(chart, use_container_width=True) å±•ç¤ºå›¾è¡¨
   - ç”¨ st.dataframe() å±•ç¤ºè¡¨æ ¼
   - æ—¥æœŸåˆ—å·²æ˜¯ datetime ç±»å‹
   - æ¨¡å‹ååŒ¹é…ç”¨: df[df['Model'].str.contains('å…³é”®è¯', case=False, na=False)]
4. ä»£ç å—åªå†™ä¸€ä¸ªï¼ŒåŒ…å«å®Œæ•´å¯æ‰§è¡Œä»£ç 
5. å…ˆç»™å‡ºæ–‡å­—åˆ†æç»“è®ºï¼Œå†ç»™ä»£ç å—"""

        # åˆå§‹åŒ–èŠå¤©å†å²
        if "ai_messages" not in st.session_state:
            st.session_state.ai_messages = []
        
        # ç”¨äº exec çš„å‘½åç©ºé—´
        exec_namespace = {
            "df": df, "df_price": df_price, "df_bench": df_bench, "df_lmarena": df_lmarena,
            "st": st, "alt": alt, "pd": pd, "os": os,
        }
        
        # è¾…åŠ©å‡½æ•°ï¼šä» AI å›å¤ä¸­åˆ†ç¦»æ–‡å­—å’Œä»£ç 
        def split_reply(reply):
            import re as _re
            code_blocks = _re.findall(r'```python\s*\n(.*?)```', reply, _re.DOTALL)
            # å»æ‰ä»£ç å—ï¼Œåªç•™æ–‡å­—
            text_only = _re.sub(r'```python\s*\n.*?```', '', reply, flags=_re.DOTALL).strip()
            return text_only, code_blocks[0] if code_blocks else None
        
        # æ˜¾ç¤ºå†å²å¯¹è¯
        for msg in st.session_state.ai_messages:
            with st.chat_message(msg["role"]):
                if msg["role"] == "assistant":
                    text_part, code = split_reply(msg["content"])
                    st.markdown(text_part)
                    if code:
                        try:
                            exec(code, exec_namespace)
                        except Exception:
                            pass
                else:
                    st.markdown(msg["content"])
        
        # ç”¨æˆ·è¾“å…¥
        user_query = st.chat_input("è¾“å…¥ä½ çš„é—®é¢˜ï¼Œä¾‹å¦‚: 'deepseek æœ€è¿‘ä¸€å‘¨çš„ç”¨é‡è¶‹åŠ¿'")
        
        if user_query:
            st.session_state.ai_messages.append({"role": "user", "content": user_query})
            with st.chat_message("user"):
                st.markdown(user_query)
            
            # æ„å»º API è¯·æ±‚
            messages = [{"role": "system", "content": SYSTEM_PROMPT}]
            # åªä¼ æœ€è¿‘ 6 è½®å¯¹è¯ä½œä¸ºä¸Šä¸‹æ–‡
            for msg in st.session_state.ai_messages[-12:]:
                messages.append({"role": msg["role"], "content": msg["content"]})
            
            with st.chat_message("assistant"):
                with st.spinner("AI æ­£åœ¨åˆ†ææ•°æ®..."):
                    try:
                        import requests as _req
                        resp = _req.post(
                            "https://openrouter.ai/api/v1/chat/completions",
                            headers={
                                "Authorization": f"Bearer {api_key}",
                                "Content-Type": "application/json"
                            },
                            json={
                                "model": AI_MODEL,
                                "messages": messages,
                                "max_tokens": 4000,
                                "temperature": 0.3
                            },
                            timeout=60
                        )
                        resp.raise_for_status()
                        result = resp.json()
                        ai_reply = result['choices'][0]['message']['content']
                    except Exception as e:
                        ai_reply = f"æŸ¥è¯¢å¤±è´¥: {str(e)}"
                
                # åˆ†ç¦»æ–‡å­—å’Œä»£ç ï¼Œåªæ˜¾ç¤ºæ–‡å­—ï¼Œä»£ç ç›´æ¥æ‰§è¡Œ
                text_part, chart_code = split_reply(ai_reply)
                st.markdown(text_part)
                
                if chart_code:
                    try:
                        exec(chart_code, exec_namespace)
                    except Exception as e:
                        st.warning(f"å¯è§†åŒ–æ‰§è¡Œå‡ºé”™: {e}")
                        with st.expander("æŸ¥çœ‹ç”Ÿæˆçš„ä»£ç "):
                            st.code(chart_code, language="python")
                
                st.session_state.ai_messages.append({
                    "role": "assistant", 
                    "content": ai_reply,
                })
        
        # æ¸…ç©ºå¯¹è¯æŒ‰é’®
        if st.session_state.ai_messages:
            if st.button("æ¸…ç©ºå¯¹è¯å†å²"):
                st.session_state.ai_messages = []
                st.rerun()

# ========================================================
# é¡µé¢ 1: T+N æ¨ªå‘å¯¹æ¯” (æ¯æ—¥æ¶ˆè€—)
# ========================================================
elif page == NAV_TN_DAILY:
    st.subheader("æ¨¡å‹å¢é•¿æ›²çº¿å¯¹æ¯” (T+N æ¯æ—¥æ¶ˆè€—)")
    st.info("æ¨ªè½´ï¼šä¸Šçº¿å¤©æ•° | çºµè½´ï¼šå½“æ—¥ Token æ¶ˆè€—é‡")

    selected_names = st.multiselect(
        "é€‰æ‹©è¦å¯¹æ¯”çš„æ¨¡å‹:", 
        all_model_names, 
        default=all_model_names[:1] 
    )
    
    if selected_names:
        tn_data = []
        standard_ticks = [0, 1, 2, 3, 4, 5, 6, 7, 10, 14, 30, 60]
        final_tick_values = set(standard_ticks)
        
        max_days_global = 0

        for name in selected_names:
            m_df = df[df['Display_Name'] == name].sort_values('Date')
            if m_df.empty: continue
            
            start_date = m_df.iloc[0]['Date']
            st.caption(f"ğŸ“… **{name}** æ”¶å½•èµ·å§‹æ—¥: {start_date.strftime('%Y-%m-%d')}")
            
            if len(m_df) > 1: m_df = m_df.iloc[:-1]

            latest_date = m_df.iloc[-1]['Date']
            latest_day_diff = (latest_date - start_date).days
            final_tick_values.add(latest_day_diff)
            
            if latest_day_diff > max_days_global:
                max_days_global = latest_day_diff

            for _, row in m_df.iterrows():
                day_diff = (row['Date'] - start_date).days
                if day_diff in standard_ticks or day_diff == latest_day_diff:
                    tn_data.append({
                        'Model': name,
                        'Days_Since_Start': day_diff,
                        'Total_Tokens': row['Total_Tokens'],
                        'Label': f"T+{day_diff}" if day_diff != latest_day_diff else f"Latest (T+{day_diff})",
                        'Real_Date': row['Date'].strftime('%Y-%m-%d')
                    })
        
        if tn_data:
            df_tn = pd.DataFrame(tn_data)
            
            # ã€é…è‰²ä¼˜åŒ–ã€‘ä½¿ç”¨ tableau10 é«˜å¯¹æ¯”é…è‰²
            chart = alt.Chart(df_tn).mark_line(
                point=alt.OverlayMarkDef(size=100, filled=True, color="white", strokeWidth=2)
            ).encode(
                x=alt.X(
                    'Days_Since_Start', 
                    title='ä¸Šçº¿å¤©æ•° (Days)',
                    axis=alt.Axis(values=list(final_tick_values), labelFontSize=20, titleFontSize=24, grid=True),
                    scale=alt.Scale(domain=[0, max_days_global + 1], clamp=True)
                ),
                y=alt.Y(
                    'Total_Tokens', 
                    title='Total Tokens (Billion)',
                    axis=alt.Axis(labelFontSize=20, titleFontSize=24)
                ),
                # å…³é”®ä¿®æ”¹ï¼šscale=alt.Scale(scheme='tableau10')
                color=alt.Color('Model', 
                                scale=alt.Scale(scheme='tableau10'), 
                                legend=alt.Legend(title="æ¨¡å‹åç§°", orient='bottom')),
                tooltip=['Model', 'Label', 'Total_Tokens', 'Real_Date']
            ).properties(height=500)
            
            st.altair_chart(chart, use_container_width=True)
            
            st.markdown("#### ğŸ“‹ æ•°æ®æ˜ç»†")
            df_pivot = df_tn.pivot_table(index='Model', columns='Days_Since_Start', values='Total_Tokens')
            df_pivot.columns = [f"T+{c}" for c in df_pivot.columns]
            st.dataframe(df_pivot.style.format("{:.4f} B"), use_container_width=True)
            
            data, name, mime, label = get_dataset_download(df_pivot.reset_index(), "tn_daily_comparison")
            st.download_button(label=label, data=data, file_name=name, mime=mime)

# ========================================================
# é¡µé¢ 2: å¤šæ¨¡å‹ç´¯è®¡å¢é•¿ (è¶‹åŠ¿å¯¹æ¯”)
# ========================================================
elif page == NAV_CUMULATIVE_COMPARE:
    st.subheader("å¤šæ¨¡å‹ç´¯è®¡å¢é•¿å¯¹æ¯”")
    
    selected_names = st.multiselect(
        "é€‰æ‹©è¦å¯¹æ¯”çš„æ¨¡å‹:", 
        all_model_names, 
        default=all_model_names[:3] if len(all_model_names) >=3 else all_model_names
    )

    if selected_names:
        plot_data = []
        max_day_plot = 0
        
        cols = st.columns(len(selected_names))
        for idx, name in enumerate(selected_names):
            m_df_temp = df[df['Display_Name'] == name].sort_values('Date')
            if not m_df_temp.empty:
                s_date = m_df_temp.iloc[0]['Date'].strftime('%Y-%m-%d')
                cols[idx].caption(f"ğŸ“… **{name}**: {s_date}")

        for name in selected_names:
            m_df = df[df['Display_Name'] == name].sort_values('Date')
            m_df['Cum_Tokens'] = m_df['Total_Tokens'].cumsum()
            if len(m_df) > 1: m_df = m_df.iloc[:-1]
            if m_df.empty: continue

            start_date = m_df.iloc[0]['Date']
            current_max_day = (m_df.iloc[-1]['Date'] - start_date).days
            if current_max_day > max_day_plot:
                max_day_plot = current_max_day

            for _, row in m_df.iterrows():
                day_num = (row['Date'] - start_date).days
                plot_data.append({
                    'Model': name, 'Day': day_num,
                    'Date': row['Date'].strftime('%Y-%m-%d'),
                    'Cumulative_Tokens': row['Cum_Tokens']
                })

        if plot_data:
            df_plot = pd.DataFrame(plot_data)

            # ã€é…è‰²ä¼˜åŒ–ã€‘ä½¿ç”¨ tableau10 é«˜å¯¹æ¯”é…è‰²
            base = alt.Chart(df_plot).encode(
                x=alt.X('Day', title="ä¸Šçº¿å¤©æ•° (Daily)", 
                        scale=alt.Scale(domain=[0, max_day_plot + 2], clamp=True),
                        axis=alt.Axis(labelFontSize=16, titleFontSize=18, grid=True)),
                y=alt.Y('Cumulative_Tokens', title='ç´¯è®¡ Token (Billion)', 
                        axis=alt.Axis(labelFontSize=16, titleFontSize=18)),
                # å…³é”®ä¿®æ”¹ï¼šscale=alt.Scale(scheme='tableau10')
                color=alt.Color('Model', 
                                title='æ¨¡å‹åç§°', 
                                scale=alt.Scale(scheme='tableau10'),
                                legend=alt.Legend(orient='bottom')),
                tooltip=['Model', 'Day', 'Date', 'Cumulative_Tokens']
            )
            chart = (base.mark_line(strokeWidth=3) + base.mark_circle(size=60)).properties(height=600)
            
            st.altair_chart(chart, use_container_width=True)

            st.markdown("### ğŸ“… ç´¯è®¡æ•°å€¼æ˜ç»†")
            df_pivot = df_plot.pivot_table(index='Day', columns='Model', values='Cumulative_Tokens')
            st.dataframe(df_pivot.style.format("{:.4f} B"), use_container_width=True)

            data, name, mime, label = get_dataset_download(df_pivot.reset_index(), "cumulative_growth")
            st.download_button(label=label, data=data, file_name=name, mime=mime)

# ========================================================
# é¡µé¢ 3: å•æ¨¡å‹æ¯æ—¥è¯¦æƒ… (è¶‹åŠ¿åˆ†æ + æ—¥æœŸç­›é€‰)
# ========================================================
elif page == NAV_DETAIL_DAILY:
    st.subheader("å•æ¨¡å‹æ¯æ—¥è¯¦æƒ…è¶‹åŠ¿")
    
    selected_name = st.selectbox("é€‰æ‹©æ¨¡å‹", all_model_names)
    m_df_full = df[df['Display_Name'] == selected_name].sort_values('Date')
    
    if not m_df_full.empty:
        min_date = m_df_full['Date'].min().date()
        max_date = m_df_full['Date'].max().date()
        st.success(f"ğŸ“… **{selected_name}** æ•°æ®æ”¶å½•åŒºé—´: {min_date} è‡³ {max_date}")

        col_filter1, col_filter2 = st.columns([1, 3])
        with col_filter1:
            date_range = st.date_input(
                "ğŸ” ç­›é€‰æ—¶é—´æ®µ",
                value=(min_date, max_date),
                min_value=min_date,
                max_value=max_date
            )
        
        if len(date_range) == 2:
            start_filter, end_filter = date_range
            mask = (m_df_full['Date'].dt.date >= start_filter) & (m_df_full['Date'].dt.date <= end_filter)
            m_df = m_df_full.loc[mask]
        else:
            m_df = m_df_full

        if not m_df.empty:
            latest = m_df.iloc[-1]
            c1, c2, c3 = st.columns(3)
            c1.metric("é€‰æ®µæœ€æ–°æ—¥æœŸ", latest['Date'].strftime('%Y-%m-%d'))
            c2.metric("å½“æ—¥æ¶ˆè€—", f"{latest['Total_Tokens']:.4f} B")
            if latest.get('Reasoning', 0) > 0 and latest.get('Completion', 0) > 0:
                ratio = (latest['Reasoning'] / latest['Completion']) * 100
                c3.metric("Reasoning å æ¯”", f"{ratio:.1f}%")
            else:
                c3.metric("Prompt Tokens", f"{latest['Prompt']:.4f} B")

            chart = alt.Chart(m_df).mark_line(point=True).encode(
                x=alt.X('Date', title='æ—¥æœŸ', axis=alt.Axis(format='%m-%d')),
                y=alt.Y('Total_Tokens', title='Token (Billion)'),
                tooltip=['Date', 'Total_Tokens', 'Prompt', 'Completion']
            )
            
            st.altair_chart(chart, use_container_width=True)
            
            display_cols = ['Date', 'Total_Tokens', 'Prompt', 'Completion', 'Reasoning']
            valid_cols = [c for c in display_cols if c in m_df.columns]
            st.dataframe(m_df[valid_cols].sort_values('Date', ascending=False).style.format({'Total_Tokens':'{:.4f}'}), use_container_width=True)

            data, name, mime, label = get_dataset_download(m_df[valid_cols], f"{selected_name}_daily")
            st.download_button(label=label, data=data, file_name=name, mime=mime)
        else:
            st.warning("âš ï¸ æ‰€é€‰æ—¶é—´æ®µå†…æ— æ•°æ®ã€‚")

# ========================================================
# é¡µé¢ 4: åŸå§‹æ•°æ®æ£€æŸ¥
# ========================================================
elif page == NAV_RAW_DATA:
    st.subheader("æ•°æ®åº“åŸå§‹æ•°æ®")
    
    st.markdown("#### ğŸ’¾ å…¨é‡æ•°æ®ä¸‹è½½")
    data, name, mime, label = get_dataset_download(df, "full_history_database")
    st.download_button(label=label, data=data, file_name=name, mime=mime)
    
    st.divider()
    
    check_name = st.selectbox("é€‰æ‹©è¦æ£€æŸ¥çš„æ¨¡å‹:", all_model_names)
    filtered_df = df[df['Display_Name'] == check_name].sort_values('Date', ascending=False)
    
    st.dataframe(
        filtered_df.style.format({
            'Prompt': '{:.6f} B', 'Completion': '{:.6f} B', 
            'Reasoning': '{:.6f} B', 'Total_Tokens': '{:.6f} B'
        }), use_container_width=True
    )

# ========================================================
# é¡µé¢ 5: æ¯æ—¥é€Ÿè§ˆä¸åˆ†æ
# ========================================================
elif page == NAV_DAILY_BRIEF:
    st.subheader("æ¨¡å‹è¡¨ç°é€Ÿè§ˆä¸åˆ†ææŠ¥å‘Š")
    st.caption("åŸºäºå†å²æ•°æ®çš„å¤šç»´åº¦é‡åŒ–åˆ†æï¼Œæ‰€æœ‰æŒ‡æ ‡å‡ç”±æ•°æ®è‡ªåŠ¨è®¡ç®—ç”Ÿæˆã€‚")

    # --- é¢„è®¡ç®—æ‰€æœ‰æ¨¡å‹çš„æŒ‡æ ‡ ---
    latest_date = df['Date'].max()
    two_weeks_ago = latest_date - pd.Timedelta(days=14)
    seven_days_ago = latest_date - pd.Timedelta(days=7)

    metrics_list = []
    for name in all_model_names:
        m_df = df[df['Display_Name'] == name].sort_values('Date')
        if m_df.empty:
            continue
        # å»æ‰æœ€åä¸€å¤©ï¼ˆå½“å¤©æœªç»“ç®—æ•°æ®ï¼Œå’Œå…¶ä»–é¡µé¢é€»è¾‘ä¿æŒä¸€è‡´ï¼‰
        if len(m_df) > 1:
            m_df = m_df.iloc[:-1]
        if m_df.empty:
            continue

        first_date = m_df.iloc[0]['Date']
        last_date = m_df.iloc[-1]['Date']
        days_online = max((last_date - first_date).days, 1)
        cumulative = m_df['Total_Tokens'].sum()
        daily_avg = cumulative / days_online
        peak = m_df['Total_Tokens'].max()

        # è¿‘ 7 æ—¥å¢é€Ÿ
        recent_df = m_df[m_df['Date'] >= seven_days_ago]
        recent_days = max(len(recent_df), 1)
        recent_avg = recent_df['Total_Tokens'].sum() / recent_days if not recent_df.empty else 0

        # å¢é•¿åŠ¨é‡
        momentum = (recent_avg / daily_avg) if daily_avg > 0 else 0

        metrics_list.append({
            'Model': name,
            'First_Date': first_date,
            'Last_Date': last_date,
            'Days_Online': days_online,
            'Cumulative': round(cumulative, 4),
            'Daily_Avg': round(daily_avg, 4),
            'Recent_7d_Avg': round(recent_avg, 4),
            'Momentum': round(momentum, 2),
            'Peak': round(peak, 4),
        })

    df_metrics = pd.DataFrame(metrics_list)

    if df_metrics.empty:
        st.warning("æš‚æ— å¯åˆ†æçš„æ¨¡å‹æ•°æ®ã€‚")
        st.stop()

    # è®¡ç®—ç™¾åˆ†ä½æ’åï¼ˆä¾›åç»­æ¨¡å—ä½¿ç”¨ï¼‰
    df_metrics['Pct_Rank_DailyAvg'] = df_metrics['Daily_Avg'].rank(pct=True)

    # ============================
    # æ¨¡å— A: è¿‘ä¸¤å‘¨æ–°å¢æ¨¡å‹ä¸€è§ˆ
    # ============================
    st.markdown("---")
    st.markdown("### è¿‘ä¸¤å‘¨æ–°å¢æ¨¡å‹ä¸€è§ˆ")
    st.caption(f"ç»Ÿè®¡åŒºé—´: {two_weeks_ago.strftime('%Y-%m-%d')} ~ {latest_date.strftime('%Y-%m-%d')}")

    new_models_df = df_metrics[df_metrics['First_Date'] >= two_weeks_ago].sort_values('First_Date', ascending=False)

    if new_models_df.empty:
        st.info("è¿‡å»ä¸¤å‘¨å†…æ²¡æœ‰æ–°ä¸Šçº¿çš„æ¨¡å‹ã€‚")
    else:
        st.markdown(f"è¿‡å»ä¸¤å‘¨å…±ä¸Šçº¿ **{len(new_models_df)}** ä¸ªæ–°æ¨¡å‹ã€‚")
        display_new = new_models_df[['Model', 'First_Date', 'Days_Online', 'Cumulative', 'Daily_Avg']].copy()
        display_new.columns = ['æ¨¡å‹åç§°', 'ä¸Šçº¿æ—¥æœŸ', 'ä¸Šçº¿å¤©æ•°', 'ç´¯è®¡æ¶ˆè€— (B)', 'æ—¥å‡æ¶ˆè€— (B)']
        display_new['ä¸Šçº¿æ—¥æœŸ'] = display_new['ä¸Šçº¿æ—¥æœŸ'].dt.strftime('%Y-%m-%d')
        st.dataframe(
            display_new.style.format({'ç´¯è®¡æ¶ˆè€— (B)': '{:.4f}', 'æ—¥å‡æ¶ˆè€— (B)': '{:.4f}'}),
            use_container_width=True, hide_index=True
        )

    # ============================
    # æ¨¡å— B (åŸ D): æ–°æ¨¡å‹ç´¯è®¡å¢é•¿å¯¹æ¯”
    # ============================
    if not new_models_df.empty:
        st.markdown("---")
        st.markdown("### æ–°æ¨¡å‹ç´¯è®¡å¢é•¿å¯¹æ¯”")

        new_model_names = new_models_df['Model'].tolist()
        plot_new = []
        max_day_new = 0

        for name in new_model_names:
            m_df = df[df['Display_Name'] == name].sort_values('Date')
            m_df['Cum_Tokens'] = m_df['Total_Tokens'].cumsum()
            if len(m_df) > 1:
                m_df = m_df.iloc[:-1]
            if m_df.empty:
                continue
            start_date = m_df.iloc[0]['Date']
            current_max = (m_df.iloc[-1]['Date'] - start_date).days
            if current_max > max_day_new:
                max_day_new = current_max
            for _, row in m_df.iterrows():
                day_n = (row['Date'] - start_date).days
                plot_new.append({
                    'Model': name, 'Day': day_n,
                    'Date': row['Date'].strftime('%Y-%m-%d'),
                    'Cumulative_Tokens': row['Cum_Tokens']
                })

        if plot_new:
            df_plot_new = pd.DataFrame(plot_new)
            base_new = alt.Chart(df_plot_new).encode(
                x=alt.X('Day', title='ä¸Šçº¿å¤©æ•°',
                        scale=alt.Scale(domain=[0, max_day_new + 2], clamp=True),
                        # å¼ºåˆ¶ tickMinStep=1 é¿å…æ˜¾ç¤ºå°æ•°åˆ»åº¦
                        axis=alt.Axis(tickMinStep=1, format='d', labelFontSize=14, titleFontSize=16, grid=True)),
                y=alt.Y('Cumulative_Tokens', title='ç´¯è®¡ Token (Billion)',
                        axis=alt.Axis(labelFontSize=14, titleFontSize=16)),
                color=alt.Color('Model', title='æ¨¡å‹',
                                scale=alt.Scale(scheme='tableau10'),
                                legend=alt.Legend(orient='bottom')),
                tooltip=['Model', 'Day', 'Date', 'Cumulative_Tokens']
            )
            chart_new = (base_new.mark_line(strokeWidth=3) + base_new.mark_circle(size=60)).properties(height=500)
            st.altair_chart(chart_new, use_container_width=True)
        else:
            st.info("æ–°æ¨¡å‹æš‚æ— è¶³å¤Ÿæ•°æ®ç»˜åˆ¶è¶‹åŠ¿å›¾ã€‚")

    # ============================
    # æ¨¡å— C: åˆ†ææ‘˜è¦ (è¡¨æ ¼åŒ–)
    # ============================
    st.markdown("---")
    st.markdown("### ç»¼åˆåˆ†ææ‘˜è¦")
    st.caption(f"åˆ†æåŸºå‡†æ—¥: {latest_date.strftime('%Y-%m-%d')}")

    # Top 3 ç´¯è®¡æ¶ˆè€—
    with st.expander("ç´¯è®¡æ¶ˆè€— Top 3", expanded=True):
        top3_cum = df_metrics.nlargest(3, 'Cumulative').copy()
        top3_cum['Rank'] = range(1, len(top3_cum) + 1)
        display_top3 = top3_cum[['Rank', 'Model', 'Cumulative', 'Days_Online', 'Daily_Avg']].copy()
        display_top3.columns = ['æ’å', 'æ¨¡å‹', 'ç´¯è®¡æ¶ˆè€— (B)', 'ä¸Šçº¿å¤©æ•°', 'æ—¥å‡æ¶ˆè€— (B)']
        st.dataframe(
            display_top3.style.format({'ç´¯è®¡æ¶ˆè€— (B)': '{:.4f}', 'æ—¥å‡æ¶ˆè€— (B)': '{:.4f}'}),
            use_container_width=True, hide_index=True
        )

    # è¿‘ 7 æ—¥å¢é€Ÿæœ€å¿«
    with st.expander("è¿‘7æ—¥å¢é€Ÿé¢†å…ˆ (Top 3)", expanded=True):
        top3_recent = df_metrics.nlargest(3, 'Recent_7d_Avg').copy()
        top3_recent['Rank'] = range(1, len(top3_recent) + 1)
        display_recent = top3_recent[['Rank', 'Model', 'Recent_7d_Avg']].copy()
        display_recent.columns = ['æ’å', 'æ¨¡å‹', 'è¿‘7æ—¥æ—¥å‡æ¶ˆè€— (B)']
        st.dataframe(
            display_recent.style.format({'è¿‘7æ—¥æ—¥å‡æ¶ˆè€— (B)': '{:.4f}'}),
            use_container_width=True, hide_index=True
        )

    # åŠ é€Ÿå¢é•¿ä¸­çš„æ¨¡å‹
    with st.expander("æ­£åœ¨åŠ é€Ÿå¢é•¿ (åŠ¨é‡ > 1.2)", expanded=True):
        accel = df_metrics[df_metrics['Momentum'] >= 1.2].sort_values('Momentum', ascending=False)
        if not accel.empty:
            accel['Growth_Pct'] = (accel['Momentum'] - 1) * 100
            display_accel = accel[['Model', 'Momentum', 'Growth_Pct']].head(5).copy()
            display_accel.columns = ['æ¨¡å‹', 'åŠ¨é‡å€¼', 'è¿‘æœŸå¢é€Ÿè¶…å‡ºå‡å€¼ (%)']
            st.dataframe(
                display_accel.style.format({'åŠ¨é‡å€¼': '{:.2f}', 'è¿‘æœŸå¢é€Ÿè¶…å‡ºå‡å€¼ (%)': '+{:.0f}%'}),
                use_container_width=True, hide_index=True
            )
        else:
            st.info("æš‚æ— æ˜æ˜¾åŠ é€Ÿå¢é•¿çš„æ¨¡å‹ã€‚")

    # å¢é€Ÿæ”¾ç¼“çš„æ¨¡å‹
    with st.expander("å¢é€Ÿæ”¾ç¼“å…³æ³¨ (åŠ¨é‡ < 0.8)", expanded=True):
        decel = df_metrics[(df_metrics['Momentum'] <= 0.8) & (df_metrics['Days_Online'] >= 7)].sort_values('Momentum')
        if not decel.empty:
            decel['Slowdown_Pct'] = (1 - decel['Momentum']) * 100
            display_decel = decel[['Model', 'Momentum', 'Slowdown_Pct']].head(5).copy()
            display_decel.columns = ['æ¨¡å‹', 'åŠ¨é‡å€¼', 'è¿‘æœŸå¢é€Ÿä½äºå‡å€¼ (%)']
            st.dataframe(
                display_decel.style.format({'åŠ¨é‡å€¼': '{:.2f}', 'è¿‘æœŸå¢é€Ÿä½äºå‡å€¼ (%)': '-{:.0f}%'}),
                use_container_width=True, hide_index=True
            )
        else:
            st.info("æš‚æ— æ˜æ˜¾å¢é€Ÿæ”¾ç¼“çš„æ¨¡å‹ã€‚")

    # æ–°æ¨¡å‹é€Ÿè¯„ï¼ˆä½¿ç”¨ç™¾åˆ†ä½æ’åäº”çº§åˆ¶ï¼‰
    if not new_models_df.empty:
        with st.expander("æ–°æ¨¡å‹åˆæœŸè¡¨ç°è¯„çº§", expanded=True):
            rating_data = []
            for row in new_models_df.itertuples():
                pct_rank = row.Pct_Rank_DailyAvg
                if pct_rank >= 0.90:
                    tier, desc = "S Â· å¤´éƒ¨æ°´å¹³", f"è¶…è¿‡ {pct_rank*100:.0f}% æ¨¡å‹"
                elif pct_rank >= 0.75:
                    tier, desc = "A Â· è¡¨ç°ä¼˜å¼‚", f"è¶…è¿‡ {pct_rank*100:.0f}% æ¨¡å‹"
                elif pct_rank >= 0.50:
                    tier, desc = "B Â· ä¸­ç­‰æ°´å¹³", "æ—¥å‡ > ä¸­ä½æ•°"
                elif pct_rank >= 0.25:
                    tier, desc = "C Â· ä½äºé¢„æœŸ", f"ä»…è¶… {pct_rank*100:.0f}% æ¨¡å‹"
                else:
                    tier, desc = "D Â· èµ·æ­¥ç¼“æ…¢", f"å {(1-pct_rank)*100:.0f}% åˆ†ä½"
                
                rating_data.append({
                    'æ¨¡å‹': row.Model,
                    'ä¸Šçº¿æ—¥æœŸ': row.First_Date.strftime('%m-%d'),
                    'æ—¥å‡æ¶ˆè€— (B)': row.Daily_Avg,
                    'è¯„çº§': tier,
                    'è¯´æ˜': desc
                })
            
            df_rating = pd.DataFrame(rating_data)
            st.dataframe(
                df_rating.style.format({'æ—¥å‡æ¶ˆè€— (B)': '{:.4f}'}),
                use_container_width=True, hide_index=True
            )

    # ============================
    # æ¨¡å— D (åŸ B): å…¨æ¨¡å‹è¡¨ç°æ’å (ç§»è‡³æœ€å)
    # ============================
    st.markdown("---")
    st.markdown("### å…¨æ¨¡å‹è¡¨ç°æ’å (Top 15)")

    RANK_OPTIONS = {
        'ç´¯è®¡æ€»é‡': 'Cumulative',
        'æ—¥å‡æ¶ˆè€—': 'Daily_Avg',
        'è¿‘7æ—¥å¢é€Ÿ': 'Recent_7d_Avg',
        'å¢é•¿åŠ¨é‡': 'Momentum',
        'å³°å€¼æ¶ˆè€—': 'Peak',
        'ä¸Šçº¿å¤©æ•°': 'Days_Online'
    }
    col_rank1, col_rank2 = st.columns([1, 3])
    with col_rank1:
        rank_label = st.selectbox("é€‰æ‹©æ’åç»´åº¦", list(RANK_OPTIONS.keys()))
    rank_col = RANK_OPTIONS[rank_label]

    df_ranked = df_metrics.sort_values(rank_col, ascending=False).head(15).reset_index(drop=True)
    df_ranked.index = df_ranked.index + 1

    chart_rank = alt.Chart(df_ranked).mark_bar(
        cornerRadiusTopLeft=4, cornerRadiusTopRight=4
    ).encode(
        x=alt.X('Model', sort='-y', title='æ¨¡å‹',
                axis=alt.Axis(labelAngle=-45, labelFontSize=11)),
        y=alt.Y(rank_col, title=rank_label,
                axis=alt.Axis(labelFontSize=14, titleFontSize=16)),
        color=alt.Color('Model', legend=None, scale=alt.Scale(scheme='tableau10')),
        tooltip=['Model', alt.Tooltip(rank_col, title=rank_label, format='.4f')]
    ).properties(height=400)
    st.altair_chart(chart_rank, use_container_width=True)

    display_ranked = df_ranked[['Model', 'Days_Online', 'Cumulative', 'Daily_Avg', 'Recent_7d_Avg', 'Momentum', 'Peak']].copy()
    display_ranked.columns = ['æ¨¡å‹', 'ä¸Šçº¿å¤©æ•°', 'ç´¯è®¡ (B)', 'æ—¥å‡ (B)', 'è¿‘7æ—¥å‡ (B)', 'åŠ¨é‡', 'å³°å€¼ (B)']

    def highlight_momentum(val):
        if isinstance(val, (int, float)):
            if val >= 1.2:
                return 'background-color: #d4edda; color: #155724'
            elif val <= 0.8:
                return 'background-color: #f8d7da; color: #721c24'
        return ''

    st.dataframe(
        display_ranked.style
            .format({'ç´¯è®¡ (B)': '{:.4f}', 'æ—¥å‡ (B)': '{:.4f}', 'è¿‘7æ—¥å‡ (B)': '{:.4f}', 'åŠ¨é‡': '{:.2f}', 'å³°å€¼ (B)': '{:.4f}'})
            .map(highlight_momentum, subset=['åŠ¨é‡']),
        use_container_width=True, hide_index=False
    )
    st.caption("åŠ¨é‡ > 1.2 (ç»¿è‰²èƒŒæ™¯) = åŠ é€Ÿå¢é•¿ Â· åŠ¨é‡ < 0.8 (çº¢è‰²èƒŒæ™¯) = å¢é€Ÿæ”¾ç¼“")

    # ============================
    # æ¨¡å— E: è¿‘ä¸¤å‘¨æ–°æ¨¡å‹åŠ¨æ€ (RSS)
    # ============================
    st.markdown("---")
    st.markdown("### è¿‘ä¸¤å‘¨æ–°æ¨¡å‹åŠ¨æ€")

    if new_models_df.empty:
        st.info("è¿‘ä¸¤å‘¨å†…æ— æ–°ä¸Šçº¿æ¨¡å‹ï¼Œæš‚æ— ç›¸å…³æ–°é—»å¯æ£€ç´¢ã€‚")
    else:
        import re as _re
        import requests as _requests


        # â”€â”€ AI ä¸“ä¸šåª’ä½“ RSS æº â”€â”€
        RSS_FEEDS = [
            ("Reddit LocalLLaMA", "https://www.reddit.com/r/LocalLLaMA/new/.rss"),
            ("Simon Willison",    "https://simonwillison.net/atom/entries/"),
            ("TechCrunch AI",     "https://techcrunch.com/category/artificial-intelligence/feed/"),
            ("The Verge AI",      "https://www.theverge.com/rss/ai-artificial-intelligence/index.xml"),
            ("Ars Technica",      "https://feeds.arstechnica.com/arstechnica/technology-lab"),
            ("Wired AI",          "https://www.wired.com/feed/tag/ai/latest/rss"),
            ("MIT Tech Review",   "https://www.technologyreview.com/feed/"),
            ("InfoQ AI",          "https://feed.infoq.com/"),
            ("OpenAI Blog",       "https://openai.com/blog/rss.xml"),
            ("Hugging Face Blog", "https://huggingface.co/blog/feed.xml"),
            ("Google DeepMind",   "https://deepmind.google/blog/rss.xml"),
            ("Last Week in AI",   "https://lastweekin.ai/feed"),
        ]


        # â”€â”€ ä»æ¨¡å‹å…¨åæå–å“ç‰Œå â”€â”€
        def extract_brand(full_name):
            base = full_name.split('/')[-1]
            return base.split('-')[0].lower()

        # â”€â”€ è‡ªåŠ¨æ‰“æ ‡ç­¾ï¼šè¿”å›åŒ¹é…çš„å“ç‰Œåï¼Œæ— åŒ¹é…è¿”å› None â”€â”€
        def detect_tag(text, brand_label_map):
            text_lower = text.lower()
            for brand, label in brand_label_map.items():
                if brand in text_lower:
                    return label
            return None

        # â”€â”€ æ„å»ºå“ç‰Œåæ ‡ç­¾æ˜ å°„ â”€â”€
        model_names_raw = new_models_df['Model'].tolist()
        brand_label_map = {}
        
        # å¼ºåˆ¶ç½®é¡¶ OpenRouterï¼ˆç¡®ä¿ä¼˜å…ˆåŒ¹é…ï¼‰
        brand_label_map["openrouter"] = "openrouter"
        brand_label_map["open router"] = "openrouter" 
        
        for full_name in model_names_raw:
            brand = extract_brand(full_name)
            if brand and len(brand) >= 3:
                brand_label_map[brand] = brand
        
        # è¡¥å……å‚å•†åˆ«åå’Œå…³è”ï¼ˆå›½å¤–+å›½å†…ä¸»æµæ¨¡å‹ï¼‰
        ALIAS_MAP = {
            # å›½å¤–
            "gpt": "openai", "o1": "openai", "o3": "openai", 
            "claude": "anthropic", "gemini": "google", 
            "llama": "meta", "mistral": "mistralai",
            # å›½å†…
            "kimi": "moonshot", "yi": "01.ai", 
            "doubao": "bytedance", "hunyuan": "tencent",
            "ernie": "baidu", "qwen": "alibaba",
            "chatglm": "zhipu", "glm": "zhipu",
            "minimax": "minimax", "step": "stepfun",
            "deepseek": "deepseek", "baichuan": "baichuan",
            "sensechat": "sensetime", "spark": "iflytek"
        }
        for short, full in ALIAS_MAP.items():
            # åªè¦æ–°æ¨¡å‹é‡Œå‡ºç°äº† short (å¦‚ claude)ï¼Œå°±åŒæ—¶ä¹Ÿå…³æ³¨ full (anthropic)
            if short in brand_label_map:
                brand_label_map[full] = full


        cutoff = latest_date - pd.Timedelta(days=14)
        cutoff_str = cutoff.strftime('%Y-%m-%d')

        # â”€â”€ ç¿»è¯‘å‡½æ•°ï¼ˆç¼“å­˜ 24 å°æ—¶ï¼‰â”€â”€
        @st.cache_data(ttl=86400)
        def translate_zh(text):
            if not text or not text.strip():
                return text
            try:
                from deep_translator import GoogleTranslator
                return GoogleTranslator(source='en', target='zh-CN').translate(text[:500])
            except Exception:
                return text

        # â”€â”€ æŠ“å–å¹¶è§£æ RSSï¼ˆç¼“å­˜ 3 å°æ—¶ï¼Œå¸¦ User-Agent é˜²åçˆ¬ï¼‰â”€â”€
        @st.cache_data(ttl=10800)
        def fetch_rss_articles(cutoff_str):
            import feedparser
            cutoff_dt = pd.Timestamp(cutoff_str, tz='UTC')
            results = []
            headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
            
            for feed_name, feed_url in RSS_FEEDS:
                try:
                    # å…ˆç”¨ requests è·å–å†…å®¹ï¼ˆç»•è¿‡ Reddit ç­‰ç«™ç‚¹çš„ UA æ£€æŸ¥ï¼‰
                    resp = _requests.get(feed_url, headers=headers, timeout=10)
                    if resp.status_code != 200:
                        continue
                        
                    feed = feedparser.parse(resp.content)
                    
                    for entry in feed.entries:
                        title = entry.get('title', '').strip()
                        link  = entry.get('link', '#')
                        
                        # æ‘˜è¦ï¼šä¼˜å…ˆ summaryï¼Œå…¶æ¬¡ content
                        desc_raw = entry.get('summary', '') or ''
                        if not desc_raw and entry.get('content'):
                            desc_raw = entry['content'][0].get('value', '')
                        import re as _re2
                        desc = _re2.sub(r'<[^>]+>', '', desc_raw).strip()[:300]
                        
                        # å‘å¸ƒæ—¶é—´
                        pub_parsed = entry.get('published_parsed') or entry.get('updated_parsed')
                        if pub_parsed:
                            pub_dt = pd.Timestamp(*pub_parsed[:6], tz='UTC')
                        else:
                            pub_dt = pd.Timestamp.now(tz='UTC')
                            
                        if pub_dt < cutoff_dt:
                            continue
                            
                        results.append({
                            'title': title, 'desc': desc, 'link': link,
                            'source': feed_name, 'date': pub_dt.strftime('%Y-%m-%d'),
                        })
                except Exception:
                    continue
            
            results.sort(key=lambda x: x['date'], reverse=True)
            return results


        # æ˜¾ç¤ºåŒ¹é…çš„å“ç‰Œï¼ˆOpenRouter ç½®é¡¶æ˜¾ç¤ºï¼‰
        display_brands = list(brand_label_map.keys())
        if "openrouter" in display_brands:
            display_brands.remove("openrouter")
            display_brands.insert(0, "openrouter")
        brand_display = ', '.join(display_brands[:10])
        
        st.caption(f"æ•°æ®æ¥æº: Reddit / Simon Willison / TechCrunch / The Verge ç­‰ Â· æ¯3å°æ—¶æ›´æ–° Â· é‡ç‚¹å…³æ³¨: {brand_display}")

        all_articles = fetch_rss_articles(cutoff_str)

        # â”€â”€ è¿‡æ»¤å‡ºä¸æ–°æ¨¡å‹ç›¸å…³çš„æ–‡ç«  â”€â”€
        matched = []
        for art in all_articles:
            # æœç´¢åŒ¹é…
            tag = detect_tag(f"{art['title']} {art['desc']}", brand_label_map)
            if tag is not None:
                art['tag'] = tag
                matched.append(art)

        if not matched:
            st.info("è¿‘ä¸¤å‘¨å†… AI åª’ä½“ä¸­æœªæ‰¾åˆ°è¿™äº›æ¨¡å‹çš„ç›¸å…³æŠ¥é“ã€‚")
        else:
            st.markdown(f"å…±æ‰¾åˆ° **{len(matched)}** æ¡ç›¸å…³æŠ¥é“ï¼ˆæ ‡é¢˜å’Œæ‘˜è¦å·²ç¿»è¯‘ä¸ºä¸­æ–‡ï¼‰")
            for art in matched:
                title_zh = translate_zh(art['title']) if art['title'] else "æ— æ ‡é¢˜"
                desc_zh = translate_zh(art['desc']) if art['desc'] else ""
                
                # æ ‡é¢˜åŠ ä¸Šæ ‡ç­¾ï¼Œå¦‚æœæ˜¯ OpenRouter åˆ™é«˜äº®
                tag_str = f"[{art['tag']}]"
                if art['tag'] == "openrouter":
                    tag_str = "ğŸ”¥ [OpenRouter]"
                
                with st.expander(
                    f"{tag_str}  {title_zh}  Â·  {art['source']}  Â·  {art['date']}",
                    expanded=False
                ):
                    if desc_zh:
                        st.markdown(desc_zh)
                    st.caption(f"åŸæ–‡: {art['title']}")
                    st.markdown(f"[é˜…è¯»åŸæ–‡ â†’]({art['link']})")



    # ============================
    # æ¨¡å— F: æŒ‡æ ‡å®šä¹‰ä¸å…¬å¼è¯´æ˜
    # ============================
    st.markdown("---")
    st.markdown("### é™„å½•: æŒ‡æ ‡å®šä¹‰ä¸è®¡ç®—å…¬å¼")
    with st.expander("æŸ¥çœ‹å®Œæ•´æŒ‡æ ‡è¯´æ˜", expanded=False):
        st.markdown("""
| æŒ‡æ ‡ | å®šä¹‰ | è®¡ç®—å…¬å¼ |
|------|------|----------|
| **æ—¥å‡æ¶ˆè€—** | æ¨¡å‹å…¨ç”Ÿå‘½å‘¨æœŸå†…å¹³å‡æ¯å¤©çš„ Token æ¶ˆè€—é‡ | `ç´¯è®¡æ€»é‡ Ã· ä¸Šçº¿å¤©æ•°` |
| **è¿‘7æ—¥å¢é€Ÿ** | æœ€è¿‘ 7 ä¸ªè‡ªç„¶æ—¥å†…çš„æ—¥å¹³å‡ Token æ¶ˆè€—é‡ | `Î£(è¿‘7æ—¥ Total_Tokens) Ã· è¿‘7æ—¥æ•°æ®æ¡æ•°` |
| **å¢é•¿åŠ¨é‡** | è¿‘æœŸæ´»è·ƒåº¦ç›¸å¯¹äºå…¨ç”Ÿå‘½å‘¨æœŸå‡å€¼çš„æ¯”ç‡ | `è¿‘7æ—¥å¢é€Ÿ Ã· æ—¥å‡æ¶ˆè€—` |
| **å³°å€¼æ¶ˆè€—** | å†å²å•æ—¥æœ€é«˜ Token æ¶ˆè€—é‡ | `max(æ¯æ—¥ Total_Tokens)` |
| **ç´¯è®¡æ€»é‡** | æ¨¡å‹ä¸Šçº¿ä»¥æ¥æ‰€æœ‰æ—¥æœŸ Token æ¶ˆè€—ä¹‹å’Œ | `Î£(Total_Tokens)` |
| **ä¸Šçº¿å¤©æ•°** | æ¨¡å‹é¦–æ¬¡å‡ºç°åœ¨æ•°æ®åº“åˆ°æœ€æ–°æ•°æ®çš„å¤©æ•° | `æœ€æ–°æ•°æ®æ—¥æœŸ - é¦–æ¬¡å‡ºç°æ—¥æœŸ` |

**åŠ¨é‡è§£è¯»:**
- åŠ¨é‡ = 1.0 â†’ è¿‘æœŸå¢é€Ÿä¸å…¨æœŸå‡å€¼æŒå¹³
- åŠ¨é‡ > 1.2 â†’ è¿‘æœŸå¤„äºåŠ é€Ÿå¢é•¿é˜¶æ®µ
- åŠ¨é‡ < 0.8 â†’ è¿‘æœŸå¢é€Ÿæ”¾ç¼“ï¼Œå¯èƒ½è¿›å…¥è¡°é€€æœŸ

**æ–°æ¨¡å‹è¯„çº§è¯´æ˜:**

è¯„çº§é‡‡ç”¨**ç™¾åˆ†ä½æ’åæ³• (Percentile Rank)**ï¼Œå°†æ–°æ¨¡å‹çš„æ—¥å‡æ¶ˆè€—æ”¾å…¥å…¨éƒ¨æ¨¡å‹çš„æ—¥å‡æ¶ˆè€—åˆ†å¸ƒä¸­è®¡ç®—æ’åç™¾åˆ†ä½:
- `ç™¾åˆ†ä½ = æ—¥å‡æ¶ˆè€— < è¯¥æ¨¡å‹çš„æ¨¡å‹æ•°é‡ Ã· æ€»æ¨¡å‹æ•°`

| è¯„çº§ | ç™¾åˆ†ä½åŒºé—´ | å«ä¹‰ |
|------|-----------|------|
| **S Â· å¤´éƒ¨æ°´å¹³** | â‰¥ P90 | æ—¥å‡æ¶ˆè€—è¶…è¿‡ 90% çš„æ¨¡å‹ï¼Œå±äºé¡¶çº§è¡¨ç° |
| **A Â· è¡¨ç°ä¼˜å¼‚** | P75 ~ P90 | æ—¥å‡æ¶ˆè€—å¤„äºå‰ 25%ï¼Œå¢é•¿åŠ¿å¤´å¼ºåŠ² |
| **B Â· ä¸­ç­‰æ°´å¹³** | P50 ~ P75 | æ—¥å‡æ¶ˆè€—é«˜äºä¸­ä½æ•°ï¼Œè¡¨ç°ä¸­è§„ä¸­çŸ© |
| **C Â· ä½äºé¢„æœŸ** | P25 ~ P50 | æ—¥å‡æ¶ˆè€—å¤„äºä¸­ä½æ•°ä»¥ä¸‹ï¼Œå…³æ³¨åç»­èµ°åŠ¿ |
| **D Â· èµ·æ­¥ç¼“æ…¢** | < P25     | æ—¥å‡æ¶ˆè€—å¤„äºå€’æ•° 25%ï¼Œå¸‚åœºæ¥å—åº¦è¾ƒä½ |
""")

# ========================================================
# é¡µé¢ 6: ä¾›åº”å•†ä»·æ ¼ä¸æœ‰æ•ˆå®šä»·åˆ†æ
# ========================================================
elif page == NAV_PRICING:
    st.subheader("æ¨¡å‹å®šä»·")
    st.caption("åŸºäº OpenRouter å‰ç«¯ API æŠ“å–çš„æœ€æ–°å®é™…æœ‰æ•ˆä»·æ ¼ã€‚")
    
    if df_price is None or df_price.empty:
        st.warning("æš‚æœªå‘ç°å¯ç”¨çš„å®šä»·æ•°æ®ã€‚")
    else:
        all_models = sorted(df_price['Model'].unique())
        selected_price_model = st.selectbox("é€‰æ‹©æ¨¡å‹:", all_models, index=0)

        # è·å–è¯¥æ¨¡å‹æ‰€æœ‰å†å²æ—¶é—´ç‚¹çš„æ•°æ®
        m_price_df = df_price[df_price['Model'] == selected_price_model].copy()
        
        # å°†æœ€æ–°çš„ç»¼åˆæŠ¥ä»·æ‹†å‡ºæ¥å±•ç¤º KPI
        latest_pricing_date = m_price_df['Date'].max()
        df_latest_prices = m_price_df[m_price_df['Date'] == latest_pricing_date]
        
        weighted_avg = df_latest_prices[df_latest_prices['Provider'] == 'Weighted Average']
        provider_latest = df_latest_prices[df_latest_prices['Provider'] != 'Weighted Average'].sort_values('Input_Price_1M')
        
        if not weighted_avg.empty:
            wa_row = weighted_avg.iloc[0]
            st.markdown("### æœ€æ–°æœ‰æ•ˆä»·æ ¼ (Weighted Average)")
            col1, col2 = st.columns(2)
            col1.metric("Input Price ($/1M)", f"${wa_row['Input_Price_1M']:.4f}")
            col2.metric("Output Price ($/1M)", f"${wa_row['Output_Price_1M']:.4f}")
        
        st.markdown("---")
        
        # === å›¾1: æœ‰æ•ˆä»·æ ¼è¶‹åŠ¿ (Weighted Average çš„ Input+Output å†å²) ===
        st.markdown("### æœ‰æ•ˆä»·æ ¼è¶‹åŠ¿")
        wa_history = m_price_df[m_price_df['Provider'] == 'Weighted Average'].copy()
        if not wa_history.empty:
            wa_history['Date'] = pd.to_datetime(wa_history['Date'])
            wa_long = wa_history.melt(
                id_vars=['Date'],
                value_vars=['Input_Price_1M', 'Output_Price_1M'],
                var_name='Type', value_name='Price'
            ).dropna(subset=['Price'])
            wa_long['Type'] = wa_long['Type'].map({'Input_Price_1M': 'Input', 'Output_Price_1M': 'Output'})
            
            chart_wa = alt.Chart(wa_long).mark_line(point=True).encode(
                x=alt.X('Date:T', title='æ—¶é—´', axis=alt.Axis(format='%m/%d')),
                y=alt.Y('Price:Q', title='ä»·æ ¼ ($/1M Tokens)'),
                color=alt.Color('Type:N', title='ç±»å‹'),
                tooltip=['Date:T', 'Type', alt.Tooltip('Price:Q', format='$.4f')]
            ).properties(height=300)
            st.altair_chart(chart_wa, use_container_width=True)
        else:
            st.info("æš‚æ— æœ‰æ•ˆä»·æ ¼å†å²æ•°æ®ã€‚")
        
        st.markdown("---")
        
        # === å›¾2: å„ä¾›åº”å•†ä»·æ ¼æŸ±çŠ¶å›¾ (æœ€æ–°ä¸€å¤© Input+Output) ===
        st.markdown("### å„ä¾›åº”å•†ä»·æ ¼å¯¹æ¯”")
        if not provider_latest.empty:
            prov_long = provider_latest.melt(
                id_vars=['Provider'],
                value_vars=['Input_Price_1M', 'Output_Price_1M'],
                var_name='Type', value_name='Price'
            ).dropna(subset=['Price'])
            prov_long['Type'] = prov_long['Type'].map({'Input_Price_1M': 'Input', 'Output_Price_1M': 'Output'})
            
            chart_prov = alt.Chart(prov_long).mark_bar().encode(
                x=alt.X('Provider:N', title='ä¾›åº”å•†', axis=alt.Axis(labelAngle=-45)),
                y=alt.Y('Price:Q', title='ä»·æ ¼ ($/1M Tokens)'),
                color=alt.Color('Type:N', title='ç±»å‹'),
                xOffset='Type:N',
                tooltip=['Provider', 'Type', alt.Tooltip('Price:Q', format='$.4f')]
            ).properties(height=350)
            st.altair_chart(chart_prov, use_container_width=True)
            
            # è¯¦ç»†è¡¨æ ¼
            st.markdown("### ä¾›åº”å•†è¯¦æƒ…")
            st.dataframe(
                provider_latest[['Provider', 'Input_Price_1M', 'Output_Price_1M', 'Cache_Hit_Rate']].style.format({
                    'Input_Price_1M': '${:.4f}',
                    'Output_Price_1M': '${:.4f}',
                    'Cache_Hit_Rate': '{:.1%}'
                }),
                use_container_width=True,
                hide_index=True
            )
        else:
            st.info("æš‚æ— ä¾›åº”å•†ä»·æ ¼æ•°æ®ã€‚")
            
        data, name, mime, label = get_dataset_download(df_price, "openrouter_pricing_full")
        st.download_button(label=label, data=data, file_name=name, mime=mime)

# ========================================================
# é¡µé¢ 7: Benchmark è·‘åˆ†æ•°æ®çŸ©é˜µ
# ========================================================
elif page == NAV_BENCHMARK:
    st.subheader("åŸºå‡†æµ‹è¯•ä¸æ’è¡Œæ¦œ")
    st.caption("æ•°æ®æºï¼šArtificial Analysis åŸºå‡†è·‘åˆ† + LMARENA (Chatbot Arena) ç«æŠ€æ’åã€‚")
    
    tab1, tab2, tab3 = st.tabs([
        "å•æŒ‡æ ‡æ’è¡Œ (AA Benchmark)",
        "å¤šæŒ‡æ ‡çŸ©é˜µ (AA Benchmark)",
        "LMARENA ç«æŠ€æ’å"
    ])
    
    # --- Tab 1 & 2: åŸæœ‰ Artificial Analysis Benchmark ---
    if df_bench is None or df_bench.empty:
        with tab1:
            st.warning("æš‚æœªå‘ç°å¯ç”¨çš„ Benchmark æ•°æ®ï¼Œè¯·ç¡®è®¤æ˜¯å¦æˆåŠŸè¿è¡Œ `openrouter_benchmark_scraper.py`ã€‚")
        with tab2:
            st.warning("æš‚æœªå‘ç°å¯ç”¨çš„ Benchmark æ•°æ®ã€‚")
    else:
        latest_bench_date = df_bench['Date'].max()
        df_latest_bench = df_bench[df_bench['Date'] == latest_bench_date].drop(columns=['Date'])
        
        # çŸ©é˜µè½¬ç½®ï¼šè®© Model å˜æˆ indexï¼ŒMetrics å˜æˆ columns
        bench_melted = df_latest_bench.melt(id_vars=['Metric'], var_name='Model', value_name='Score')
        bench_pivot = bench_melted.pivot_table(index='Model', columns='Metric', values='Score')
        
        metrics_available = bench_pivot.columns.tolist()
        
        with tab1:
            st.markdown("### æ ¸å¿ƒåŸºå‡†æµ‹è¯•æ’è¡Œæ¦œ")
            st.info(f"æ•°æ®æ›´æ–°äº: **{latest_bench_date.strftime('%Y-%m-%d')}**")
            primary_metric = st.selectbox("é€‰æ‹©æ’åºæŒ‡æ ‡:", metrics_available, index=0, key="tab1_metric")
            
            if primary_metric:
                bench_sorted = bench_pivot.sort_values(by=primary_metric, ascending=False).reset_index()
                bench_sorted = bench_sorted.dropna(subset=[primary_metric])
                
                top_10_models = bench_sorted['Model'].head(10).tolist()
                
                selected_b_models = st.multiselect(
                    "é€‰æ‹©å¯¹æ¯”æ¨¡å‹ (é»˜è®¤å‰10):", 
                    bench_sorted['Model'].tolist(), 
                    default=top_10_models,
                    key="tab1_models"
                )
                
                if selected_b_models:
                    plot_df = bench_sorted[bench_sorted['Model'].isin(selected_b_models)]
                    
                    chart_vertical = alt.Chart(plot_df).mark_bar(
                        cornerRadiusTopLeft=3, cornerRadiusTopRight=3
                    ).encode(
                        x=alt.X('Model:N', sort='-y', title='æ¨¡å‹åç§°', axis=alt.Axis(labelAngle=-45, labelOverlap=False)),
                        y=alt.Y(f'{primary_metric}:Q', title='å¾—åˆ†æ•°å€¼'),
                        color=alt.Color('Model:N', legend=None, scale=alt.Scale(scheme='tableau20')),
                        tooltip=['Model', alt.Tooltip(f'{primary_metric}:Q', format='.3f')]
                    ).properties(height=500)
                    
                    st.altair_chart(chart_vertical, use_container_width=True)
                else:
                    st.info("è¯·è‡³å°‘é€‰æ‹©ä¸€ä¸ªæ¨¡å‹è¿›è¡Œå¯¹æ¯”ç»˜åˆ¶ã€‚")
                    
        with tab2:
            st.markdown("### å¤šç»´åº¦æŒ‡æ ‡äº¤å‰å¯¹æ¯”")
            st.info(f"æ•°æ®æ›´æ–°äº: **{latest_bench_date.strftime('%Y-%m-%d')}**")
            col_t1, col_t2 = st.columns([1, 2])
            with col_t1:
                t2_metric = st.selectbox("æ’åºæŒ‡æ ‡ä¼˜å…ˆæƒ:", metrics_available, index=0, key="tab2_main_metric")
            with col_t2:
                t2_metrics = st.multiselect("éœ€è¦ä¸€å¹¶åˆ—å‡ºçš„å…¶ä»–æŒ‡æ ‡:", metrics_available, default=metrics_available[:4] if len(metrics_available) >= 4 else metrics_available, key="tab2_metrics")
            
            t2_sorted = bench_pivot.sort_values(by=t2_metric, ascending=False).reset_index()
            t2_models_selected = st.multiselect(
                "éœ€è¦æ”¾å…¥è¡¨æ ¼å¯¹æ¯”çš„æ¨¡å‹ (ç•™ç©ºä»£è¡¨æ˜¾ç¤ºæ‰€æœ‰):",
                t2_sorted['Model'].tolist(),
                default=[]
            )
            
            display_cols = [t2_metric] + [m for m in t2_metrics if m != t2_metric]
            
            if t2_models_selected:
                display_df = bench_pivot.loc[t2_models_selected, display_cols].sort_values(by=t2_metric, ascending=False)
            else:
                display_df = bench_pivot.loc[:, display_cols].sort_values(by=t2_metric, ascending=False)
                
            st.dataframe(display_df.style.format("{:.3f}", na_rep='-'), use_container_width=True)
    
    # --- Tab 3: LMARENA ç«æŠ€æ’å ---
    with tab3:
        st.markdown("### LMARENA æ’è¡Œæ¦œ")
        st.caption("æ•°æ®æº: lmarena-ai.com Â· ç”±çœŸäººç›²æµ‹å¯¹æˆ˜çš„ ELO åˆ†æ•°")
        
        if df_lmarena is None or df_lmarena.empty:
            st.warning("æš‚æœªå‘ç° LMARENA æ’è¡Œæ¦œæ•°æ®ã€‚")
        else:
            latest_lm_date = df_lmarena['Date'].max()
            st.info(f"æ•°æ®æ›´æ–°äº: **{latest_lm_date.strftime('%Y-%m-%d')}**")
            
            df_latest_lm = df_lmarena[df_lmarena['Date'] == latest_lm_date].copy()
            
            # 9 ä¸ªç»´åº¦çš„åˆ—å â†’ ä¸­æ–‡æ ‡ç­¾æ˜ å°„
            SCORE_LABELS = {
                'Score_text': 'æ–‡æœ¬',
                'Score_vision': 'è§†è§‰',
                'Score_webdev': 'ç½‘é¡µå¼€å‘',
                'Score_image_gen': 'æ–‡ç”Ÿå›¾',
                'Score_image_edit': 'å›¾åƒç¼–è¾‘',
                'Score_search': 'æœç´¢',
                'Score_text_video': 'æ–‡ç”Ÿè§†é¢‘',
                'Score_img_video': 'å›¾ç”Ÿè§†é¢‘',
                'Overall_Rank': 'ç»¼åˆæ’å',
            }
            
            # æ„å»ºç»´åº¦é€‰æ‹©ï¼ˆåªæ˜¾ç¤ºæœ‰æ•°æ®çš„ï¼‰
            col_options = {}
            for col_key, label in SCORE_LABELS.items():
                if col_key in df_latest_lm.columns and df_latest_lm[col_key].notna().sum() > 0:
                    col_options[label] = col_key
            
            if not col_options:
                st.info("æš‚æ— æ’è¡Œæ•°æ®ã€‚")
            else:
                selected_label = st.selectbox("é€‰æ‹©æ’è¡Œç»´åº¦:", list(col_options.keys()), index=0, key="lmarena_category")
                selected_col = col_options[selected_label]
                
                # ç­›é€‰æœ‰åˆ†æ•°çš„æ¨¡å‹
                ranked_df = df_latest_lm.dropna(subset=[selected_col]).copy()
                
                is_score = selected_col.startswith('Score_')  # Score åˆ—ç”¨åˆ†æ•°ï¼ˆè¶Šé«˜è¶Šå¥½ï¼‰ï¼ŒOverall_Rank ç”¨æ’å
                
                if is_score:
                    ranked_df = ranked_df.sort_values(selected_col, ascending=False).reset_index(drop=True)
                else:
                    ranked_df = ranked_df.sort_values(selected_col, ascending=True).reset_index(drop=True)
                
                if ranked_df.empty:
                    st.info("è¯¥ç»´åº¦æš‚æ— æ•°æ®ã€‚")
                else:
                    top_n = min(25, len(ranked_df))
                    top_df = ranked_df.head(top_n).copy()
                    top_df['Display_Value'] = top_df[selected_col].astype(int)
                    
                    if is_score:
                        # ELO åˆ†æ•°ï¼šæ°´å¹³æŸ±çŠ¶å›¾ï¼Œåˆ†æ•°ä»å¤§åˆ°å°ï¼ˆY è½´æ’åºï¼‰ï¼ŒX è½´åœ¨åº•éƒ¨
                        chart_rank = alt.Chart(top_df).mark_bar(
                            cornerRadiusTopRight=3, cornerRadiusBottomRight=3
                        ).encode(
                            y=alt.Y('Model:N', 
                                    sort=alt.EncodingSortField(field=selected_col, order='descending'),
                                    title=None, 
                                    axis=alt.Axis(labelOverlap=False)),
                            x=alt.X('Display_Value:Q', title='ELO åˆ†æ•°',
                                    scale=alt.Scale(zero=False)),
                            color=alt.value('#4C78A8'),
                            tooltip=['Model', alt.Tooltip('Display_Value:Q', title='ELO åˆ†æ•°')]
                        ).properties(height=max(300, top_n * 25))
                    else:
                        # ç»¼åˆæ’åï¼šæ°´å¹³æŸ±çŠ¶å›¾ï¼Œæ’åä»å°åˆ°å¤§
                        chart_rank = alt.Chart(top_df).mark_bar(
                            cornerRadiusTopRight=3, cornerRadiusBottomRight=3
                        ).encode(
                            y=alt.Y('Model:N',
                                    sort=alt.EncodingSortField(field=selected_col, order='ascending'),
                                    title=None,
                                    axis=alt.Axis(labelOverlap=False)),
                            x=alt.X('Display_Value:Q', title='æ’å'),
                            color=alt.value('#4C78A8'),
                            tooltip=['Model', alt.Tooltip('Display_Value:Q', title='æ’å')]
                        ).properties(height=max(300, top_n * 25))
                    
                    st.altair_chart(chart_rank, use_container_width=True)
                    
                    # å®Œæ•´æ’åè¡¨æ ¼
                    value_label = 'ELO åˆ†æ•°' if is_score else 'æ’å'
                    st.markdown(f"#### {selected_label} å®Œæ•´æ•°æ® (å…± {len(ranked_df)} ä¸ªæ¨¡å‹)")
                    display_lm = ranked_df[['Model', selected_col]].copy()
                    display_lm[selected_col] = display_lm[selected_col].astype(int)
                    display_lm.columns = ['æ¨¡å‹', value_label]
                    st.dataframe(display_lm, use_container_width=True, hide_index=True, height=400)
    
    st.markdown("---")
    col_dl1, col_dl2 = st.columns(2)
    if df_bench is not None:
        with col_dl1:
            data, name, mime, label = get_dataset_download(df_bench, "openrouter_benchmark_full")
            st.download_button(label="ä¸‹è½½ AA Benchmark æ•°æ®", data=data, file_name=name, mime=mime)
    if df_lmarena is not None:
        with col_dl2:
            data, name, mime, label = get_dataset_download(df_lmarena, "lmarena_leaderboard_full")
            st.download_button(label="ä¸‹è½½ LMARENA æ•°æ®", data=data, file_name=name, mime=mime)

# ========================================================
# é¡µé¢ 8: å•æ¨¡å‹æ·±åº¦æ¢ç´¢
# ========================================================
elif page == NAV_SINGLE_MODEL:
    st.subheader("å•æ¨¡å‹åˆ†æ")
    st.caption("ç»¼åˆç”¨é‡ã€åŸºå‡†æµ‹è¯•å’Œå®šä»·æ•°æ®ï¼Œè¿½è¸ªå•ä¸€æ¨¡å‹ã€‚")

    # è·å–åŒ…å«è¿‡å»ç°åœ¨æ‰€æœ‰è®°å½•ä¸‹æ¥çš„åå­—é›†åˆï¼Œç»Ÿä¸€æ¶ˆé™¤é‡åå¹²æ‰°é¡¹
    raw_models = set(all_model_names) | set(all_pricing_models) | set(all_benchmark_models)
    normalized_map = {}
    for rm in raw_models:
        norm = normalize_model_name(rm)
        if norm not in normalized_map:
            normalized_map[norm] = []
        normalized_map[norm].append(rm)
        
    all_possible_models = sorted(list(normalized_map.keys()))
    
    if not all_possible_models:
        st.warning("æš‚æœªå‘ç°ä»»ä½•æ¨¡å‹æ•°æ®ã€‚")
    else:
        selected_model_norm = st.selectbox("é€‰æ‹©æ¨¡å‹:", all_possible_models)
        st.markdown("---")
        
        real_names = normalized_map[selected_model_norm]
        
        # 1. ç´¯è®¡ç”¨é‡è¶‹åŠ¿
        st.markdown("### ç´¯è®¡ç”¨é‡è¶‹åŠ¿")
        if df is not None and not df.empty:
            m_df = df[df['Model'].isin(real_names) | df['Display_Name'].isin(real_names)].sort_values('Date').copy()
                
            if not m_df.empty:
                m_df = m_df.groupby('Date', as_index=False)['Total_Tokens'].sum()
                m_df['Cumulative_Tokens'] = m_df['Total_Tokens'].cumsum()
                
                col_m1, col_m2 = st.columns(2)
                recent_7d = m_df.tail(7)['Total_Tokens'].sum()
                col_m1.metric("ç´¯è®¡æ¶ˆè€—", f"{m_df['Cumulative_Tokens'].iloc[-1]:.4f} Billion")
                col_m2.metric("è¿‘ 7 å¤©æ¶ˆè€—", f"{recent_7d:.4f} Billion")
                    
                chart_cum = alt.Chart(m_df).mark_area(
                    opacity=0.6, 
                    color=alt.Gradient(
                        gradient='linear',
                        stops=[alt.GradientStop(color='orange', offset=0), alt.GradientStop(color='white', offset=1)],
                        x1=1, x2=1, y1=1, y2=0
                    )
                ).encode(
                    x=alt.X('Date:T', title='æ—¥æœŸ'),
                    y=alt.Y('Cumulative_Tokens:Q', title='ç´¯è®¡ Tokens (Billion)'),
                    tooltip=['Date', 'Cumulative_Tokens', 'Total_Tokens']
                ).properties(height=350)
                st.altair_chart(chart_cum, use_container_width=True)
            else:
                st.info("è¯¥æ¨¡å‹æš‚æ—  Token æ¶ˆè€—è®°å½•ã€‚")
        else:
            st.info("æœªè¿æ¥åˆ° Token æ•°æ®æºã€‚")

        st.markdown("---")
        
        # 2. åŸºå‡†æµ‹è¯•è·‘åˆ†
        st.markdown(f"### {selected_model_norm} åŸºå‡†æµ‹è¯•è·‘åˆ†")
        st.caption("åŒä¸€æ¨¡å‹å¯èƒ½æœ‰ Reasoning / Non-Reasoning ç­‰å˜ä½“ã€‚")
        if df_bench is not None and not df_bench.empty:
            latest_bench_date = df_bench['Date'].max()
            df_latest_bench = df_bench[(df_bench['Date'] == latest_bench_date) & (df_bench['Metric'].notna())].copy()
            
            bench_model_cols = [col for col in df_latest_bench.columns if col not in ['Date', 'Metric']]
            matched_b_cols = fuzzy_match_model(selected_model_norm, bench_model_cols, threshold=0.55)
            
            if matched_b_cols:
                tabs_b = st.tabs(matched_b_cols)
                
                for i, m_col in enumerate(matched_b_cols):
                    with tabs_b[i]:
                        model_scores = df_latest_bench[['Metric', m_col]].dropna()
                        if not model_scores.empty:
                            rank_data = []
                            for _, row in model_scores.iterrows():
                                metric = row['Metric']
                                score = row[m_col]
                                
                                all_scores_flat = df_latest_bench[df_latest_bench['Metric'] == metric].drop(columns=['Date', 'Metric']).iloc[0].dropna()
                                all_scores_num = pd.to_numeric(all_scores_flat, errors='coerce').dropna()
                                
                                if score in all_scores_num.values:
                                    rank = all_scores_num.rank(method='min', ascending=False)[m_col]
                                    total = len(all_scores_num)
                                    percentile = (total - rank) / total * 100
                                    
                                    rank_data.append({
                                        'æŒ‡æ ‡': metric,
                                        'å¾—åˆ†': f"{score:.3f}",
                                        'æ’å': f"ç¬¬ {int(rank)} / å…± {total}",
                                        'åˆ†ä½æ•°': f"è¶…è¶Š {percentile:.1f}%"
                                    })
                            
                            if rank_data:
                                st.dataframe(pd.DataFrame(rank_data), use_container_width=True, hide_index=True)
                            else:
                                st.info("æš‚æ— å¯ç”¨æµ‹è¯•æ•°æ®ã€‚")
                        else:
                            st.info("æš‚æ— æ•°æ®ã€‚")
            else:
                st.info("è¯¥æ¨¡å‹æœªè¢«æ”¶å½•äº Benchmark æ•°æ®ä¸­ã€‚")
        else:
            st.info("æœªè¿æ¥åˆ°è·‘åˆ†æ•°æ®æºã€‚")

        st.markdown("---")
        
        # 3. å®šä»·åˆ†æï¼ˆåŒå›¾ï¼šæœ‰æ•ˆä»·æ ¼è¶‹åŠ¿ + å„ä¾›åº”å•†ä»·æ ¼æŸ±çŠ¶å›¾ï¼‰
        st.markdown("### å®šä»·åˆ†æ")
        if df_price is not None and not df_price.empty:
            m_price_df = df_price[df_price['Model'].isin(real_names)].copy()
            if not m_price_df.empty:
                latest_pricing_date = m_price_df['Date'].max()
                df_latest_prices = m_price_df[m_price_df['Date'] == latest_pricing_date]
                
                wa_row = df_latest_prices[df_latest_prices['Provider'] == 'Weighted Average']
                if not wa_row.empty:
                    wa = wa_row.iloc[0]
                    st.success(f"æœ‰æ•ˆå‡ä»·: Input **${wa['Input_Price_1M']:.4f}**/1M Â· Output **${wa['Output_Price_1M']:.4f}**/1M")
                
                # å›¾1: æœ‰æ•ˆä»·æ ¼è¶‹åŠ¿
                st.markdown("#### æœ‰æ•ˆä»·æ ¼è¶‹åŠ¿")
                wa_hist = m_price_df[m_price_df['Provider'] == 'Weighted Average'].copy()
                if not wa_hist.empty:
                    wa_hist['Date'] = pd.to_datetime(wa_hist['Date'])
                    wa_long = wa_hist.melt(
                        id_vars=['Date'],
                        value_vars=['Input_Price_1M', 'Output_Price_1M'],
                        var_name='Type', value_name='Price'
                    ).dropna(subset=['Price'])
                    wa_long['Type'] = wa_long['Type'].map({'Input_Price_1M': 'Input', 'Output_Price_1M': 'Output'})
                    
                    chart_wa = alt.Chart(wa_long).mark_line(point=True).encode(
                        x=alt.X('Date:T', title='æ—¶é—´', axis=alt.Axis(format='%m/%d')),
                        y=alt.Y('Price:Q', title='ä»·æ ¼ ($/1M Tokens)'),
                        color=alt.Color('Type:N', title='ç±»å‹'),
                        tooltip=['Date:T', 'Type', alt.Tooltip('Price:Q', format='$.4f')]
                    ).properties(height=250)
                    st.altair_chart(chart_wa, use_container_width=True)
                
                # å›¾2: å„ä¾›åº”å•†ä»·æ ¼æŸ±çŠ¶å›¾
                provider_prices = df_latest_prices[df_latest_prices['Provider'] != 'Weighted Average'].sort_values('Input_Price_1M')
                if not provider_prices.empty:
                    st.markdown("#### å„ä¾›åº”å•†ä»·æ ¼å¯¹æ¯”")
                    prov_long = provider_prices.melt(
                        id_vars=['Provider'],
                        value_vars=['Input_Price_1M', 'Output_Price_1M'],
                        var_name='Type', value_name='Price'
                    ).dropna(subset=['Price'])
                    prov_long['Type'] = prov_long['Type'].map({'Input_Price_1M': 'Input', 'Output_Price_1M': 'Output'})
                    
                    chart_prov = alt.Chart(prov_long).mark_bar().encode(
                        x=alt.X('Provider:N', title='ä¾›åº”å•†', axis=alt.Axis(labelAngle=-45)),
                        y=alt.Y('Price:Q', title='ä»·æ ¼ ($/1M Tokens)'),
                        color=alt.Color('Type:N', title='ç±»å‹'),
                        xOffset='Type:N',
                        tooltip=['Provider', 'Type', alt.Tooltip('Price:Q', format='$.4f')]
                    ).properties(height=300)
                    st.altair_chart(chart_prov, use_container_width=True)
                    
                    st.dataframe(
                        provider_prices[['Provider', 'Input_Price_1M', 'Output_Price_1M', 'Cache_Hit_Rate']].style.format({
                            'Input_Price_1M': '${:.4f}',
                            'Output_Price_1M': '${:.4f}',
                            'Cache_Hit_Rate': '{:.1%}'
                        }),
                        use_container_width=True,
                        hide_index=True
                    )
            else:
                st.info("æš‚æ— è¯¥æ¨¡å‹çš„å®šä»·æ•°æ®ã€‚")
        else:
            st.info("æœªè¿æ¥åˆ°å®šä»·æ•°æ®æºã€‚")

        st.markdown("---")

        # 4. LMARENA æ’å
        st.markdown(f"### {selected_model_norm} çš„ LMARENA æ’å")
        if df_lmarena is not None and not df_lmarena.empty:
            latest_lm_date = df_lmarena['Date'].max()
            df_latest_lm = df_lmarena[df_lmarena['Date'] == latest_lm_date]
            
            lm_all_models = df_latest_lm['Model'].unique().tolist()
            matched_lm = fuzzy_match_model(selected_model_norm, lm_all_models, threshold=0.5)
            
            if matched_lm:
                lm_rows = df_latest_lm[df_latest_lm['Model'].isin(matched_lm)].copy()
                
                score_cols = [c for c in lm_rows.columns if c.startswith('Score_')]
                SCORE_LABELS = {
                    'Score_text': 'æ–‡æœ¬', 'Score_vision': 'è§†è§‰', 'Score_webdev': 'ç½‘é¡µå¼€å‘',
                    'Score_image_gen': 'æ–‡ç”Ÿå›¾', 'Score_image_edit': 'å›¾åƒç¼–è¾‘', 'Score_search': 'æœç´¢',
                    'Score_text_video': 'æ–‡ç”Ÿè§†é¢‘', 'Score_img_video': 'å›¾ç”Ÿè§†é¢‘',
                }
                
                rank_display = []
                for _, row in lm_rows.iterrows():
                    entry = {'æ¨¡å‹': row['Model']}
                    if pd.notna(row.get('Overall_Rank')):
                        entry['ç»¼åˆæ’å'] = int(row['Overall_Rank'])
                    for sc in score_cols:
                        label = SCORE_LABELS.get(sc, sc)
                        if pd.notna(row.get(sc)):
                            entry[f'{label} ELO'] = int(row[sc])
                    rank_display.append(entry)
                
                if rank_display:
                    st.dataframe(pd.DataFrame(rank_display), use_container_width=True, hide_index=True)
                else:
                    st.info("æœªæ‰¾åˆ°è¯¥æ¨¡å‹çš„æ’åæ•°æ®ã€‚")
            else:
                st.info("è¯¥æ¨¡å‹æš‚æœªè¢« LMARENA æ”¶å½•ã€‚")
        else:
            st.info("æœªè¿æ¥åˆ° LMARENA æ•°æ®æºã€‚")
